\documentclass[12pt]{article}
\usepackage{style/style_sol}

\begin{document}

\begin{titlepage}
	\centering
	\vspace*{\fill}
	\Huge \textit{\textbf{Solutions MP/MP$^*$\\ Réduction des endomorphismes}}
	\vspace*{\fill}
\end{titlepage}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item On a \function{f^k}{E}{E}{M}{A^{k}M} donc pour tout polynôme $P$, on a $P(f)=P(A)M$ par combinaison linéaire. Si $P(A)=0$, alors $P(f)=0$. Donc si $A$ est diagonalisable, $f$ l'est aussi. Si $P(f)=0$ alors avec $M=I_{n}$, on a $P(A)=0$ et $A$ est diagonalisable si $f$ l'est.
		
		Même résultat avec $g$ et $B$.

		\item Soit $(\lambda_{i,j})_{1\leqslant i,j\leqslant n}$ tel que $\sum_{(i,j)\in\llbracket1,n\rrbracket^{2}}\lambda_{i,j}X_{i}Y_{j}^{\mathsf{T}}=0$. Alors on a 
		\begin{equation}
			\sum_{j=1}^{n}\left(\sum_{i=1}^{n}\lambda_{i,j}X_{i}\right)Y_{j}^{\mathsf{T}}=0
		\end{equation}

		Soit $k\in\llbracket1,n\rrbracket$, la $k$-ième ligne de notre matrice est 
		\begin{equation}
			\sum_{j=1}^{n}\left(\sum_{i=1}^{n}\lambda_{i,j}X_{i,k}\right)Y_{j}^{\mathsf{T}}=0
		\end{equation}
		Puisque $(Y_{j}^{\mathsf{T}})_{1\leqslant j\leqslant n}$ est libre, on a pour tout $j\in\llbracket1,n\rrbracket$,
		\begin{equation}
			\sum_{i=1}^{n}\lambda_{i,j}X_{i,k}=0
		\end{equation}
		Puisque $(X_{i})_{1\leqslant i\leqslant n}$ est libre, pour tout $(i,j)\in\llbracket1,n\rrbracket^{2}$, $\lambda_{i,j}=0$, d'où le résultat.

		\item Puisque $B$ est diagonalisable, $B^{\mathsf{T}}$ l'est aussi. On prend $(X_{i})_{1\leqslant i\leqslant n}$ une base de vecteurs propres de $A$ avec pour tout $i\in\llbracket1,n\rrbracket$, $AX_{i}=\lambda_{i}X_{i}$. Prenons $(Y_{j})_{1\leqslant j\leqslant n}$ une base de vecteurs propres de $B^{\mathsf{T}}$ avec pour tout $j\in\llbracket1,n\rrbracket$, $B^{\mathsf{T}}Y_{j}=\mu_{j}Y_{j}$ et $Y_{j}B^{\mathsf{T}}=\mu_{j}Y_{j}^{\mathsf{T}}$. Ainsi,
		\begin{equation}
			h\left(X_{i}Y_{j}^{\mathsf{T}}\right)=AX_{i}Y_{j}^{\mathsf{T}}B=\mu_{j}AX_{i}Y_{j}^{\mathsf{T}}=\mu_{j}\lambda_{i}X_{i}Y_{j}^{\mathsf{T}}
		\end{equation}
		et les $(X_{i}Y_{j}^{\mathsf{T}})_{1\leqslant i,j\leqslant n}$ forment une base de $E$ d'après ce qui précède. Donc $h$ est diagonalisable.

		Réciproquement, on a le contre-exemple $A=0$ et $B$ non diagonalisable: $h$ est l'endomorphisme nul.
	\end{enumerate}	
\end{proof}

\begin{remark}
	Généralement, soit $A\in\M_{n}(\K)$ et $B\in\M_{p}(\K)$, on définit \function{h_{A,B}}{\M_{n,p}(\K)}{\M_{n,p}(\K)}{M}{AMB}
	La matrice de $h_{A,B}$ dans la base canonique de $\M_{n,p}(\K)$ s'appelle le produit tensoriel de $A$ et $B$ noté 
	\begin{equation}
		A\otimes B=
		\begin{pmatrix}
			a_{1,1}B & \dots & a_{1,n}B\\
			\vdots & &\vdots\\
			a_{n,1}B&\dots & a_{n,n}B
		\end{pmatrix}
	\end{equation}
	On a toujours 
	\begin{equation}
		\Tr(A\otimes B)=\sum_{i=1}^{n}a_{i,i}\Tr(B)=\Tr(A)\Tr(B)
	\end{equation}
	Si $A$ et $B$ sont diagonalisables, $h_{A,B}$ l'est.
\end{remark}

\begin{proof}
	On pose $P=DP_{1}$ et $Q=DQ_{1}$ avec $P_{1}\wedge Q_{1}=1$. Il existe $(U,V)\in\K[X]^{2}$ telles que $UP_{1}+VQ_{1}=1$. On a $MD=PQ$ donc $M=DP_{1}Q_{1}=PQ_{1}=P_{1}Q$.

	\begin{enumerate}
		\item Soit $x\in\ker(D(f))$. On a 
		\begin{equation}
			P(f)(x)=DP_{1}(f)(x)=P_{1}(f)\circ D(f)(x)=0	
		\end{equation}
		De même pour $Q(f)(x)=0$, donc 
		\begin{equation}
			\ker(D(f))\subset\ker(P(f))\cap\ker(Q(f))
		\end{equation}

		Soit $x\in\ker(P(f))\cap\ker(Q(f))$. On a
		\begin{equation}
			DUP_{1}+DVQ_{1}=0
		\end{equation}
		d'où 
		\begin{equation}
			UP+VQ=0
		\end{equation}
		et
		\begin{equation}
			D(f)(x)=UP(f)(x)+VQ(f)(x)=0
		\end{equation}

		Donc 
		\begin{equation}
			\boxed{\ker(D(f))=\ker(P(f))\cap\ker(M(f))}	
		\end{equation}

		\item On a $P\mid M$ donc $\ker(P(f))\subset\ker(M(f))$. De même, $\ker(Q(f))\subset\ker(M(f))$ donc 
		\begin{equation}
			\ker(P(f))+\ker(Q(f))\subset\ker(M(f))
		\end{equation}

		Si $x\in\ker(M(f))$, on a 
		\begin{equation}
			x=\underbrace{UP_{1}(f)(x)}_{\in\ker(Q(f))}+\underbrace{VQ_{1}(f)(x)}_{\in\ker(P(f))}
		\end{equation}
		car $M=P_{1}Q=Q_{1}P$. Donc 
		\begin{equation}
			\boxed{\ker(M(f))=\ker(P(f))+\ker(Q(f))}
		\end{equation}

		\item Si $i\in\im(P(f))$, il existe $x\in E$ tel que $y=P(f)(x)=D(f)\circ P_{1}(f)(x)\in\im(D(f))$. De même pour $\im(Q(f))\subset\im(D(f))$. Donc 
		\begin{equation}
			\im(P(f))+\im(Q(f))\subset\im(D(f))
		\end{equation}

		Soit $y\in\im(D(f))$, alors il existe $x\in E$ tel que 
		\begin{equation}
			y=D(f)(x)=\underbrace{UP(f)(x)}_{\in\im(P(f))}+\underbrace{VQ(f)(x)}_{\in\im(Q(f))}
		\end{equation}
		Donc 
		\begin{equation}
			\boxed{\im(D(f))=\im(P(f))+\im(Q(f))}
		\end{equation}

		\item On a $P\mid M$ d'où $\im(M(f))\subset\im P(f)$ et $\im(M(f))\subset\im Q(f)$. Ainsi,
		\begin{equation}
			\im(M(f))\subset\im(Q(f))\cap\im\im(Q(f))
		\end{equation}

		Si $y\in\im(P(f))\cap\im(Q(f))$ alors il existe $(x,x')\in E^{2}$ tels que 
		\begin{equation}
			y=P(f)(x)=P(f)(x')
		\end{equation}
		Or $M=P_{1}Q=PQ_{1}$ donc 
		\begin{equation}
			y=UP_{1}(f)(y)+VQ_{1}(f)(y)=UP_{1}Q(f)(x')+VQ_{1}P(f)(x)\in\im(M(f))
		\end{equation}
		donc 
		\begin{equation}
			\boxed{\im(M(f))=\im(P(f))\cap\im(Q(f))}
		\end{equation}
	\end{enumerate}
\end{proof}

\begin{proof}
	On a 
	\begin{equation}
		A\left(\frac{-1}{5}A+\frac{4}{5}I_{n}\right)=I_{n}
	\end{equation}
	donc $A$ est inversible.
	\begin{equation}
		X^{2}-4X+5=(X-2+\i)(X-2-\i)
	\end{equation}
	est scindé à racines simples sur $\C$. Donc $A$ est diagonalisable sur $\C$, semblable sur $\C$ à
	\begin{equation}
		\begin{pmatrix}
			\lambda_{1}I_{n_{1}} &0\\
			0 & \lambda_{2}I_{n_{2}}
		\end{pmatrix}
	\end{equation}
	où $\lambda_{1}=2+\i$ et $\lambda_{2}=2-\i$. $A\in\M_{n}(\R)$ donc $\Tr(A)=n_{1}\lambda_{1}+n_{2}\lambda_{2}\in\R$

	Donc 
	\begin{equation}
		\Im(n_{1}\lambda_{1}+n_{2}\lambda_{2})=0=n_{1}-n_{2}
	\end{equation}

	Ainsi $n_{1}=n_{2}$ donc $n$ est pair.

	$A$ est semblable sur $\C$ à 
	\begin{equation}
		\begin{pmatrix}
			\lambda_{1}&0&\dots&\dots&0\\
			0&\overline{\lambda_{1}}&\ddots&&\vdots\\
			\vdots &\ddots & \ddots &\ddots&\vdots\\
			\vdots & &\ddots & \lambda_{1}&0\\
			0 &\dots&\dots&0&\overline{\lambda_{1}}
		\end{pmatrix}
	\end{equation}

	Soit 
	\begin{equation}
		A_{0}=
		\begin{pmatrix}
			0&-5\\
			1&4
		\end{pmatrix}
	\end{equation}
	On a $\chi_{A_{0}}=X^{2}-4X+5$. $A_{0}$ est diagonalisable sur $\C$ et est semblable à 
	\begin{equation}
		\begin{pmatrix}
			\lambda_{1}&0\\
			0&\overline{\lambda_{1}}
		\end{pmatrix}
	\end{equation}
	Donc $A$ est semblable sur $\C$ à 
	\begin{equation}
		\begin{pmatrix}
			A_{0}&&\\
			&\ddots&\\
			&&A_{0}
		\end{pmatrix}
	\end{equation}
	donc $A$ est semblable sur $\R$ à cette même matrice.

	Soit $l\in\N$, on a 
	\begin{equation}
		X^{l}=Q_{p}(X^{2}-4X+5)+\alpha_{l}X+\beta_{l}
	\end{equation}
	par division euclidienne. Donc 
	\begin{equation}
		A^{l}=\alpha_{l}A+\beta_{l}I_{n}
	\end{equation}
	On a notamment 
	\begin{equation}
		\left\lbrace
			\begin{array}[]{l}
				(2+\i)^{l}=\alpha_{l}(2+\i)+\beta_{l}\\
				(2-\i)^{l}=\alpha_{l}(2-\i)+\beta_{l}
			\end{array}
		\right.
	\end{equation}

	On a donc 
	\begin{equation}
		\boxed{
			\left\lbrace
			\begin{array}[]{l}
				\alpha_{l}=\frac{(2+\i)^{l}-(2-\i)^{l}}{2\i}\\
				\beta_{l}=(2+\i)^{l}-\frac{(2+\i)}{2\i}\left[(2+\i)^{l}-(2-\i)^{l}\right]
			\end{array}
		\right.
		}
	\end{equation}
\end{proof}

\begin{remark}
	On a $2+\i=\sqrt{5}\e^{\i\theta}$ avec $\theta=\arccos\left(\frac{2}{\sqrt{5}}\right)\in]0,\pi[$. Donc $\alpha_{l}=\left(\sqrt{5}\right)^{l}\sin(l\theta)$.
\end{remark}

\begin{remark}
	On a 
	\begin{equation}
		I_{n}-4A^{-1}+5A^{-2}=0
	\end{equation}
	De même, $\left(X-\frac{1}{2-\i}\right)\left(X-\frac{1}{2+\i}\right)$ annule $A^{-1}$ et on a pour tout $l\in-\N^{*}$,
	\begin{equation}
		A^{l}=\alpha_{l}A+\beta_{l}I_{n}
	\end{equation}
\end{remark}

\begin{remark}
	$(A-2I_{n})^{2}=-I_{n}$ donc $\det(-I_{n})=(-1)^{n}>0$ donc $n$ est pair.
\end{remark}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item On a 
		\begin{equation}
			A\begin{pmatrix}
				1\\\vdots\\1
			\end{pmatrix}=\begin{pmatrix}
				1\\\vdots\\1
			\end{pmatrix}
		\end{equation}
		et $\begin{pmatrix}
			1&\dots&1
		\end{pmatrix}^{\mathsf{T}}\neq0$ donc 
		\begin{equation}
			\boxed{1\in\Sp_{\R}(A)}
		\end{equation}

		\item Soit $X=\begin{pmatrix}
			x_{1}&\dots&x_{n}
		\end{pmatrix}^{\mathsf{T}}\neq0$ associé à $\lambda$. Pour tout $i\in\llbracket1,n\rrbracket$, on a 
		\begin{equation}
			\lambda x_{i}=\sum_{j=1}^{n}a_{i,j}x_{j}
		\end{equation}
		Soit $i_{0}\in\llbracket1,n\rrbracket$ tel que $\left\lvert x_{i_{0}}\right\rvert=\max\limits_{i\in\llbracket1,n\rrbracket}\left\lvert x_{i}\right\rvert>0$ car $X\neq0$. On a alors 
		\begin{equation}
			\left\lvert\lambda\right\rvert\left\lvert x_{i_{0}}\right\rvert=\left\lvert\sum_{j=1}^{n}a_{i_{0},j}x_{j}\right\rvert\leqslant\sum_{j=1}^{n}a_{i_{0},j}\left\lvert x_{j}\right\rvert\leqslant\left(\sum_{j=1}^{n}a_{i_{0},j}\right)\left\lvert x_{i_{0}}\right\rvert
		\end{equation}
		donc 
		\begin{equation}
			\boxed{\left\lvert\lambda\right\rvert\leqslant1}
		\end{equation}

		\item Soit $J_{i}=\left\lbrace j\in\llbracket1,n\rrbracket\middle| a_{i,j}>0\right\rbrace$. On a 
		\begin{equation}
			\left\lvert\lambda\right\rvert\left\lvert x_{i_{0}}\right\rvert=\left\lvert\sum_{j\in J_{i_{0}}}^{n}a_{i_{0},j}x_{j}\right\rvert\leqslant\sum_{j\in J_{i_{0}}}^{n}a_{i_{0},j}\left\lvert x_{j}\right\rvert\leqslant\left(\sum_{j\in J_{i_{0}}}^{n}a_{i_{0},j}\right)\left\lvert x_{i_{0}}\right\rvert=\left\lvert x_{i_{0}}\right\rvert
		\end{equation}

		On a égalité partout donc pour tout $j\in J_{i_{0}}$, $\left\lvert x_{j}\right\rvert=\left\lvert x_{i_{0}}\right\rvert$ et $x_{j}=\left\lvert x_{i_{0}}\right\rvert\e^{\i \theta}$. En reportant, on a 
		\begin{equation}
			\lambda\left\lvert x_{i_{0}}\right\rvert=\sum_{j\in J_{i_{0}}}a_{i_{0},j}\left\lvert x_{i_{0}}\right\rvert
		\end{equation}
		donc 
		\begin{equation}
			\boxed{\lambda=1}
		\end{equation}

		\item Si $\left\lvert\lambda\right\rvert=1$ et $\lambda\neq1$, on a $i_{0}\notin J_{i_{0}}$ car sinon $\lambda=1$. Donc il existe $i_{1}\in J_{i_{0}}\setminus\lbrace i_{0}\rbrace$ tel que $x_{i_{1}}=\left\lvert x_{i_{0}}\right\rvert\e^{\i\theta}=\lambda x_{i_{0}}$. Ainsi, il existe $i_{2}\neq i_{1}$ tel que $x_{i_{2}}=\lambda x_{i_{1}}$. De proche en proche, il existe $i_{q}\neq i_{q-1}$ tel que $x_{i_{q}}=\lambda x_{i_{q-1}}$ (avec $q\geqslant1$) et $x_{i_{q}}=\lambda^{q}x_{i_{0}}$. Or \function{\varphi}{\N}{\llbracket1,n\rrbracket}{k}{i_{k}} n'est pas injective. Donc il existe $k>l$ tel que $i_{k}=i_{l}$ et $x_{i_{k}}=\lambda^{k-k}x_{i_{k}}$ et $k-l>1$ donc 
		\begin{equation}
			\boxed{
				\lambda\in\U_{k-l}
			}
		\end{equation}

		\item L'identité convient, les matrices de permutation aussi. En effet, si $\sigma\in\Sigma_{n}$, on a $P_{\sigma}^{n!}=I_{n}$ donc les valeurs propres sont racines de $X^{n!}-1$ donc $\Sp_{\C}(P_{\sigma})\subset\U_{n!}$.
		
		Réciproquement, soit $A$ stochastique telle que $\Sp_{\C}(A)\subset(\U)$. Soit $i\in\llbracket1,n\rrbracket$, supposons $\left\lvert J_{i_{0}}\right\rvert\geqslant2$. D'après la décomposition de Dunford, il existe $D$ diagonale et $N$ nilpotente qui commutent telles que $A=D+N$ et $\Sp_{\C}(D)=\Sp_{\C}(A)$. Si $N$ est nilpotente d'indice $r\geqslant2$, on a pour tout $k\in\N^{*}$ avec $k\geqslant r$, on a 
		\begin{equation}
			A^{k}=\sum_{j=1}^{k}\binom{k}{j}N^{j}D^{k-j}=\sum_{j=1}^{r}\binom{k}{j}N^{j}D^{k-j}
		\end{equation}

		Pour tout $j\in\llbracket1,r\rrbracket$, on a 
		\begin{equation}
			\binom{k}{j}=\frac{k(k-1)\dots(k-j+1)}{j!}\underset{k\to+\infty}{\sim}\frac{k^{j}}{j!}
		\end{equation}
		Comme $N^{r-1}\neq0$, on a 
		\begin{equation}
			A^{k}\underset{k\to+\infty}{sim}\frac{k^{r-1}}{(r-1)!}N^{r-1}D^{k-r+1}
		\end{equation}
		et les coefficients de $D^{k-r+1}$ sont bornés car $\Sp(D)\subset\U$.

		Or, notons que si $A$ et $B$ sont stochastiques, $AB$ l'est aussi ($\bm{1}$ est toujours valeur propre). Par récurrence, $A^{k}$ l'est. Donc $A^{k}\in\M_{n}([0,1])$, et l'équivalent est impossible si $r\geqslant2$. Donc $r=1$ donc $N=0$ et $A=D$ est diagonalisable.

		Les valeurs propres de $A$ sont des racines de l'unité, soit $m$ le $\ppcm$ des ordres de ces racines (dans $(\U,\times)$). On a alors 
		\begin{equation}
			A=P\diag(\lambda_{1},\dots,\lambda_{n})P^{-1}
		\end{equation}
		d'où 
		\begin{equation}
			A^{m}=P\diag(\lambda_{1}^{m},\dots,\lambda_{n}^{m})P^{-1}
		\end{equation}

		Notons $M=\max\limits_{j\in J_{i_{0}}}\left\lvert a_{i_{0},j}\right\rvert<1$ (car $\left\lvert J_{i_{0}}\right\rvert\geqslant2$ donc pour tout $j\in J_{i_{0}}$, $a_{i_{0},j}\neq1$). On note $a_{i_{0},i_{0}}^{(m)}$ le coefficient $(i_{0},i_{0})$ de $A^{m}$. On a alors 
		\begin{equation}
			a_{i_{0},i_{0}}^{(m)}=1=\sum_{j\in J_{i_{0}}}a_{i_{0},j}a_{j,i_{0}}^{(m-1)}\leqslant M\sum_{j\in J_{i_{0}}}a_{j,i_{0}}^{(m-1)}\leqslant M\sum_{j=1}^{n}a_{j,i_{0}}^{(m-1)}=M
		\end{equation}
		car $A^{m-1}$ est stochastique. Donc $M=1$ ce qui n'est pas possible (par définition de $M$). Ainsi, pour tout $i\in\llbracket1,n\rrbracket$, on a $\lvert J_{i}\rvert=1$ donc il existe un unique $j_{i}\in\llbracket1,n\rrbracket$ avec $a_{i,j_{i}}=1$ et pour tout $j\neq j_{i}$, $a_{i,j}=0$.

		$i\mapsto j_{i}$ est injective, sinon $\rg(A)\leqslant n-1$ et $0\in\Sp(A)$.
	\end{enumerate}
\end{proof}

\begin{remark}
	On peut avoir $\left\lvert \lambda\right\rvert<1$ pour la question 2, par exemple 
	\begin{equation}
		A=
		\begin{pmatrix}
			\frac{1}{n}&\dots&\frac{1}{n}\\
			\vdots&&\vdots\\
			\frac{1}{n}&\dots&\frac{1}{n}
		\end{pmatrix}
	\end{equation}
	On a $A^{2}=A$ et $\rg(A)=1$, $\Sp(A)=\lbrace0,1\rbrace$.
\end{remark}

\begin{remark}
	Par exemple, pour 4, on a 
	\begin{equation}
		A=
		\begin{pmatrix}
			0&1\\
			1&0
		\end{pmatrix}
	\end{equation}
	On a $\chi_{A}=X^{2}-1$ et $\Sp(A)=\lbrace-1,1\rbrace$.
\end{remark}

\begin{remark}
	Si pour tout $(i,j)\in\llbracket1,n\rrbracket$, $a_{i,j}>0$ (i.e.~pour tout $i\in\llbracket1,n\rrbracket$, $J_{i}=\llbracket1,n\rrbracket$). D'après 3, on a $\Sp_{\C}(A)\cap\U=\lbrace1\rbrace$. De plus, si $X=\begin{pmatrix}
		x_{1}&\dots&x_{n}
	\end{pmatrix}^{\mathsf{T}}\in\M_{n,1}(\C)\setminus\lbrace0\rbrace$ vérifie $AX=X$, d'après ce qui précède, on a $x_{1}=\dots=x_{n}$ et le sous-espace propre associé à 1 est de dimension 1.
\end{remark}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item Soit $(\lambda,\mu)\in\Sp_{\C}(A)\times\Sp_{\C}(B)$. On a $\mu\in\Sp_{\C}(B^{\mathsf{T}})$. Soit $(X,Y)\in\M_{n-1}(\C)\setminus\lbrace0\rbrace$ vecteurs propres associés respectivement à $\lambda$ et à $\mu$. On pose $M=XY^{\mathsf{T}}$. Alors
		\begin{equation}
			\Phi_{A,B}(M)=AXY^{\mathsf{T}}-XY^{\mathsf{T}}B=(\lambda-\mu)XY^{\mathsf{T}}=(\lambda-\mu)M
		\end{equation}
		donc 
		\begin{equation}
			\boxed{\lambda-\mu\in\Sp(\Phi_{A,B})}
		\end{equation}

		Réciproquement, soit $\alpha\in\Sp(\Phi_{A,B})$. Il existe $M\in\M_{n}(\C)\setminus\lbrace0\rbrace$ tel que l'on ait $AM-MB=\alpha M$ d'où $AM=M(\alpha I_{n}+B)$. Par récurrence, $A^{k}M=M(\alpha I_{n}+B)^{k}$ et par combinaison linéaire, pour tout $P\in\C[X]$ on a $P(A)M=MP(\alpha I_{n}+B)$. En particulier, on prend $P=\chi_{A}$. D'après le théorème de Cayley-Hamilton, on a 
		\begin{equation}
			0=M\chi_{A}(\alpha I_{n}+B)
		\end{equation}
		On a $M\neq0$ donc $\chi_{A}(\alpha I_{n}+B)$ n'est pas inversible. On écrit 
		\begin{equation}
			\chi_{A}(X)=\prod_{k=1}^{n}(X-\lambda_{k})
		\end{equation}
		d'où 
		\begin{equation}
			\chi_{A}(\alpha I_{n}+B)=\prod_{k=1}^{n}(B+(\alpha-\lambda_{k})I_{n})
		\end{equation}
		donc il existe $k_{0}\in\llbracket1,n\rrbracket$ tel que $B+(\alpha-\lambda_{k_{0}})I_{n}$ est non inversible. Donc $\lambda_{k_{0}}-\alpha\in\Sp(B)$ et donc $\alpha$ est une différence d'un élément de $\Sp(A)$ et de $\Sp(B)$.

		\item On forme \function{f_A}{\M_n(\C)}{\M_n(\C)}{M}{AM} et \function{g_B}{\M_n(\C)}{\M_n(\C)}{M}{MB}
		Toujours par récurrence et combinaison linéaires, pour tout $P\in\C[X]$,
		\begin{equation}
			P(f_{A})M=P(A)M
		\end{equation}
		Si $P(A)=0$, on a $P(f_{A})=0$. Si $P(f_{A})=0$, pour $M=I_{n}$, on a $P(A)=0$. De même pour $B$. Donc $\Pi_{A}=\Pi_{f_{A}}$ (polynômes minimaux) et $A$ est diagonalisable si et seulement si $f_{A}(M)$ est diagonalisable. $f_{A}$ et $g_{B}$ commutent car 
		\begin{equation}
			(f_{A}\circ g_{B})(M)=AMB=(g_{B}\circ f_{A})(M)
		\end{equation}
		Donc $f_{A}$ et $g_{B}$ sont codiagonalisables et donc $\Phi_{A,B}$ l'est.
	\end{enumerate}
\end{proof}

\begin{remark}
	Si $(X_{1},\dots,X_{n})$ (respectivement $(Y_{1},\dots,Y_{n})$) est une base de vecteurs\\propres de $A$ (respectivement de $B^{\mathsf{T}}$), alors $(X_{i}Y_{j}^{\mathsf{T}})_{1\leqslant i,j\leqslant n}$ est une base de vecteurs propres pour $\Phi_{A,B}$.
\end{remark}

\begin{remark}
	C'est faux sur $\R$, par exemple 
	\begin{equation}
		A=B=
		\begin{pmatrix}
			0 & -1\\
			1 &0	
		\end{pmatrix}
	\end{equation}
	On a $\Sp_{\R}=\emptyset$ et $\Phi_{A,A}(I_{2})=0$ donc $0\in\Sp_{\Phi_{A,A}}$.
\end{remark}

\begin{remark}
	Si $\Phi_{A,B}$ est diagonalisable, soit $(M_{i,j})_{1\leqslant i,j\leqslant n}$ une base de vecteurs propres de $\Phi_{A,B}$. Soit $\lambda\in\Sp_{\C}(B)$ et $X\in\M_{n,1}(\C)\setminus\lbrace0\rbrace$ tel que $BX=\lambda X$. On a 
	\begin{equation}
		AM_{i,j}=M_{i,j}(B+\lambda_{i,j}I_{n})
	\end{equation}
	avec $\Phi_{A,B}(M_{i,j})=\lambda_{i,j}M_{i,j}$. Donc 
	\begin{equation}
		AM_{i,j}X=(\lambda+\lambda_{i,j})M_{i,j}X
	\end{equation}
	Pour tout $X_{0}\in\M_{n,1}(\C)$, il existe $M\in\M_{n}(\C)$ tel que $X_{0}=MX$. $M\in\Vect(M_{i,j})_{1\leqslant i,j\leqslant n}$ donc 
	\begin{equation}
		\Vect(M_{i,j}X)_{1\leqslant i,j\leqslant n}=M_{n,1}(\C)
	\end{equation}
	On peut donc en extraire une base: c'est une base de vecteurs propres de $A$.
\end{remark}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item Par récurrence, pour tout $k\in\N$, on a $A^{k}M=\theta^{k}MA^{k}$, or $F$ est un sous-espace vectoriel donc par combinaisons linéaires, pour tout $P\in\K[X]$, on a $P(A)M=MP(\theta A)$.
		
		\item Soit $X\in\ker(A-\lambda I_{n})$. On a $AMX=\theta MAX=\lambda\theta MX$. On a donc $MX\in\ker(A-\lambda\theta I_{n})$.
		
		Si pour tout $\lambda\in\Sp_{\C}(A)$, on a $\theta\lambda\notin\Sp_{\C}(A)$, alors si $\lambda\in\Sp_{\C}(A)$ et $X\in\ker(A-\lambda I_{n})$, alors $\ker(A-\lambda\theta I_{n})=\lbrace0\rbrace$. Donc $MX=0$. Or les vecteurs propres forment une famille génératrice donc $M=0$ et $F=\lbrace0\rbrace$.

		S'il existe $\lambda_{0}\in\Sp_{\C}(A)$ tel que $\theta\lambda_{0}\in\Sp_{\C}(A)$. Soit $X_{1}$ un vecteur propre de $A$ associé à $\lambda_{0}$. On complète $(X_{1})$ en $\mathcal{B}=(X_{1},\dots,X_{n})$ base de $\C^{n}$ formé de vecteurs propres de $A$. On définit $MX_{1}=Y_{1}\in\ker(A-\lambda_{0}\theta I_{n})\setminus\lbrace0\rbrace$ et pour tout $i\in\llbracket2,n\rrbracket$, on a $MX_{i}=0$. Ainsi, pour tout $i\in\llbracket2,n\rrbracket$, on a 
		\begin{equation}
			AMX_{i}=0=\theta MAX_{i}=\theta\lambda_{i}MX_{i}
		\end{equation}
		et 
		\begin{equation}
			AMX_{1}=AY_{1}=\lambda_{0}\theta Y_{1}=\theta MAX_{1}=\theta\lambda_{0}X_{1}
		\end{equation}
		Donc $M\neq0$ et $M\in F$. Finalement, on a $F=\lbrace0\rbrace$ si et seulement si pour tout $\lambda\in\Sp_{\C}(A),\theta\lambda\notin\Sp_{\C}(A)$.

		\item On écrit $\chi_{A}=\prod_{j=1}^{r}(X-\lambda_{j})^{m_{j}}$ avec $\lambda_{j}$ distincts et $m_{j}\geqslant1$. D'après le théorème de Cayley-Hamilton et le lemme des noyaux, on a 
		\begin{equation}
			\C^{n}=\bigotimes_{j=1}^{r}\ker(A-\lambda_{j}I_{n})^{m_{j}}
		\end{equation}

		Supposons $\theta\neq0$. Si $M\in F$ et si $x\in \ker(A-\lambda_{j}I_{n})^{m_{j}}$. On a 
		\begin{equation}
			\left(\left(\frac{X}{\theta}-\lambda_{j}\right)^{m_{j}}\right)(A)(Mx)=M\left(A-\lambda_{j}I_{n}\right)^{m_{j}}(x)=0
		\end{equation}
		Donc 
		\begin{equation}
			Mx\in\ker\left(\frac{1}{\theta}A-\lambda_{j}I_{n}\right)^{m_{j}}=\ker\left(A-\theta\lambda_{j}I_{n}\right)^{m_{j}}
		\end{equation}
		car $\theta\neq0$.

		De plus, $\ker(A-\theta\lambda_{j} I_{n})^{m_{j}}\neq\lbrace0\rbrace0$ si et seulement si $\ker(A-\theta\lambda_{j}I_{n})\neq\lbrace0\rbrace$ car \begin{equation}
			\det\left[(A-\theta\lambda_{j}I_{n})^{m_{j}}\right]=\det\left[(A-\theta\lambda_{j}I_{n})\right]^{m_{j}}
		\end{equation}

		Si pour tout $\lambda\in\Sp_{\C}(A)$, $\lambda\theta\notin\Sp_{\C}(A)$, soit $x\in\ker(A-\lambda_{j}I_{n})^{m_{j}}$. On a 
		\begin{equation}
			Mx\in\ker(A-\theta\lambda_{j}I_{n})^{m_{j}}=\lbrace0\rbrace
		\end{equation}
		donc $M=0$ car $\C^{n}=\bigotimes_{j=1}^{r}\ker(A-\lambda_{j}I_{n})^{m_{j}}$.

		S'il existe $\lambda_{0}\in\Sp_{\C}(A)$ tel que $\lambda_{0}\theta\in\Sp_{\C}(A)$, soit $x_{1}\in\ker(A-\lambda_{0}I_{n})\neq\lbrace0\rbrace$. On pose 
		\begin{equation}
			Mx_{1}=y_{1}\in\ker(A-\lambda_{0}\theta I_{n})\setminus\lbrace0\rbrace
		\end{equation}
		On complète $(x_{1})$ en $\mathcal{B}=(x_{1},\dots,x_{n})$ base de $\C^{n}$ formée de vecteurs appartenant à 
		\begin{equation}
			\bigcup_{j=1}^{r}\ker(A-\lambda_{j}I_{n})^{m_j}	
		\end{equation}
		On a pour tout $i\in\llbracket2,n\rrbracket$, $Mx_{i}=0$. On a $M\neq0$ et 
		\begin{equation}
			AMx_{1}=Ay_{1}=\theta\lambda_{0}y_{1}=\theta\lambda_{0}Mx_{1}
		\end{equation}
		Pour tout $i\in\llbracket2,n\rrbracket$, on a $AMx_{i}=0$ si $x_{i}\in\ker(A-\lambda_{j_{i}}I_{n})^{m_{j_{i}}}$ et si $\lambda_{j_{i}}\neq\lambda_{0}$. On a $Ax_{i}\in\ker(A-\lambda_{j_{i}}I_{n})^{m_{j_{i}}}$ donc 
		\begin{equation}
			Ax_{i}\in\Vect(x_{2},\dots,x_{n})
		\end{equation}
		et $MAx_{i}=0$ donc $AMx_{i}=\theta MAx_{i}$.

		Si $F\neq\lbrace0\rbrace$, il existe $M\neq0$ tel que $AM=\theta MA$. Pour tout $P\in\C[X]$, on a $P(A)M=MP(\theta A)$. En particulier, pour $P=\chi_{A}$, on a
		\begin{equation}
			M\chi_{A}(\theta A)=0
		\end{equation}
		$M\neq0$ et donc $\chi_{A}(\theta A)$ n'est pas inversible. Si $\chi_{A}=\prod_{k=1}^{n}(X-\lambda_{k})$, il existe $k\in\llbracket1,n\rrbracket$, $(\theta A-\lambda_{k}I_{n})$ est non inversible, d'où 
		\begin{equation}
			\boxed{\lambda_{k}\in\Sp_{\C}(A)\cap\Sp_{\C}(\theta A)}
		\end{equation}
	\end{enumerate}
\end{proof}

\begin{proof}
	On a 
	\begin{align}
		\chi_{A}(\lambda)
		&=
		\begin{vmatrix}
			\lambda-1	&-1			&0			&-1\\
			-1			&\lambda-1	&-1			&0\\
			-1			&0			&\lambda-1	&-1\\
			0			&-1			&-1			&\lambda-1
		\end{vmatrix}\\
		&=(\lambda-3)
		\begin{vmatrix}
			1	&1			&1			&1\\
			-1			&\lambda-1	&-1			&0\\
			-1			&0			&\lambda-1	&-1\\
			0			&-1			&-1			&\lambda-1
		\end{vmatrix}\\
		&=(\lambda-3)
		\begin{vmatrix}
			1	&0			&0			&0\\
			-1			&\lambda-1	&-1			&0\\
			-1			&0			&\lambda-1	&-1\\
			0			&-1			&-1			&\lambda-1
		\end{vmatrix}\\
		&=(\lambda-3)
		\begin{vmatrix}
			\lambda &0 &1\\
			1 &\lambda &0\\
			-1 &-1 &\lambda-1
		\end{vmatrix}\\
		&=(\lambda-3)
		\begin{vmatrix}
			\lambda-1 &0 &1\\
			1-\lambda &\lambda &0\\
			1-\lambda &-1 &\lambda-1
		\end{vmatrix}\\
		&=(\lambda-3)(\lambda-1)
		\begin{vmatrix}
			1 &0 &1\\
			-1 &\lambda &0\\
			-1 &-1 &\lambda-1
		\end{vmatrix}\\
		&=(\lambda-3)(\lambda-1)
		\begin{vmatrix}
			1 &0 &1\\
			0 &\lambda &1\\
			0 &-1 &\lambda
		\end{vmatrix}\\
		&=(\lambda-3)(\lambda-1)(\lambda^{2}+1)
	\end{align}
	où l'on a fait successivement les opérations suivantes: $L_{1}\leftarrow L_{1}+L_{2}+L_{3}+L_{4}$, $C_{i}\leftarrow C_{i}-C_{1}$ pour $i\in\lbrace2,3,4\rbrace$, développement selon la première ligne, $C_{1}\leftarrow C_{1}-C_{2}-C_{3}$, $L_{i}\leftarrow L_{i}+L_{1}$ pour $i\in\lbrace2,3\rbrace$, développement selon la première colonne.

	$\chi_{A}$ est scindé à racines simples sur $\C$ donc $A$ est diagonalisable. On trouve ensuite un vecteur propre dans chaque sous-espace propre (qui sont de dimension un).
\end{proof}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item On a $\lambda\in\Sp_{\R}(A)$ si et seulement s'il existe $X\in\M_{n,1}(\R)\setminus\lbrace0\rbrace$ telle que $AX=\lambda X$ si et seulement si 
		\begin{equation}
			\left\lbrace
				\begin{array}[]{lll}
					\sum_{i\neq 1}a_{i}x_{i}&=&\lambda x_{1}\\
					\vdots\\
					\sum_{i\neq j}a_{i}x_{i}&=&\lambda x_{1}\\
					\vdots\\
					\sum_{i\neq n}a_{i}x_{i}&=&\lambda x_{1}\\
				\end{array}
			\right.
		\end{equation}

		Soit $S=\sum_{i=1}^{n}a_{i}x_{i}$. Ce système équivaut à 
		\begin{equation}
			S=(\lambda+a_{1})x_{1}=\dots=(\lambda+a_{n})x_{n}
		\end{equation}

		Si $S=0$, pour tout $i\in\llbracket1,n\rrbracket$, on a $\lambda=-a_{i}$ ou $x_{i}=0$ (et $X\neq0$). Les $(a_{i})_{1\leqslant i\leqslant n}$, il existe un unique $i_{0}\in\llbracket1,n\rrbracket$ tel que $\lambda=-a_{i_{0}}$ et pour tout $i\neq i_{0}$, on a $x_{i}=0$. En reportant, on a $S=0=\lambda x_{i_{0}}$ donc $\lambda=0$ ce qui est impossible car $0=\lambda=-a_{i_{0}}>0$.

		Donc $S\neq0$ et pour tout $i\in\llbracket1,n\rrbracket$, $\lambda+a_{i}\neq0$ et pour tout $i\in\llbracket1,n\rrbracket$, $x_{i}=\frac{S}{\lambda+a_{i}}$. On a alors 
		\begin{equation}
			S=\sum_{i=1}^{n}a_{i}x_{i}=\sum_{i=1}^{n}\frac{a_{i}S}{\lambda+a_{i}}
		\end{equation}
		donc 
		\begin{equation}
			\boxed{
				\sum_{i=1}^{n}\frac{a_{i}}{\lambda+a_{i}}=1
			}
		\end{equation}

		Réciproquement, on prend $x_{i}=\frac{1}{\lambda+a_{i}}$ et on a bien $AX=\lambda X$.

		\item On définit \function{f}{\R\setminus\lbrace -a_{n},\dots,-a_{1}\rbrace}{\R}{x}{\sum_{i=1}^{n}\frac{a_{i}}{x+a_{i}}}
		\item Posons $-a_{n+1}=-\infty$ et $-a_{0}=+\infty$. Sur $]-a_{k+1},-a_{k}[$, on a 
		\begin{equation}
			f'(x)=\sum_{i=1}^{n}\frac{-a_{i}}{(x+a_{i})^{2}}	
		\end{equation}

		Les $(a_{i})_{1\leqslant i\leqslant n}$ étant positifs, on a $\lim\limits_{x\to-a_{k+1}^{+}}f(x)=+\infty$ et $\lim\limits_{x\to-a_{k}^{-}}f(x)=-\infty$ (si $k\neq n$) (et $\lim\limits_{x\to-\infty}f(x)=\lim\limits_{x\to+\infty}f(x)=0$).

		D'après le théorème des valeurs intermédiaires, pour tout $k\in\llbracket0,n-1\rrbracket$, il existe un unique $\lambda_{k}\in]-a_{k+1},-a_{k}[$ tel que $f(\lambda_{k})=1$. Donc $A$ admet exactement $n$ valeurs propres réelles distinctes. Donc $A$ est diagonalisable sur $\R$.
	\end{enumerate}
\end{proof}

\begin{remark}
	Soit 
	\begin{equation}
		F(X)=-\sum_{k=1}^{n}\frac{a_{k}}{X+a_{k}}+1=\frac{P(X)}{(X+a_{1}\dots(X+a_{n}))}
	\end{equation}
	avec $P=(X+a_{1})\dots(X+a_{n})-\sum_{k=1}^{n}a_{k}P_{k}$ où $P_{k}=\prod_{\substack{i=1\\i\neq k}}(X+a_{i})$ de degré $n-1$. On a $\deg(P)=n$ et son coefficient dominant est 1. De plus, pour tout $\lambda\in\R$, on a $P(\lambda)=0$ si et seulement si $\sum_{k=1}^{n}\frac{a_{k}}{\lambda+a_{k}}=1$ si et seulement si $\lambda\in\Sp(A)$ donc $P=\chi_{A}$.
\end{remark}

\begin{proof}
	On a 
	\begin{equation}
		\begin{pmatrix}
			1 &0 &\dots&\dots&\dots&\dots&0\\
			0&\frac{1}{2}&\ddots&&&&\vdots\\
			\vdots&\ddots & \ddots &\ddots &&\frac{\lambda}{j}&\vdots\\
			\vdots&& \ddots & \ddots &\ddots&&\vdots\\
			\vdots&&&\ddots&\ddots&\ddots&\vdots\\
			\vdots&&&&\ddots&\ddots&0\\
			0&\dots&\dots&\dots&\dots&0&\frac{1}{n}\\
		\end{pmatrix}
		\times\diag(1,2,\dots,n)
		=
		\begin{pmatrix}
			1 &0 &\dots&\dots&\dots&\dots&0\\
			0&1&\ddots&&&&\vdots\\
			\vdots&\ddots & \ddots &\ddots &&\lambda&\vdots\\
			\vdots&& \ddots & \ddots &\ddots&&\vdots\\
			\vdots&&&\ddots&\ddots&\ddots&\vdots\\
			\vdots&&&&\ddots&\ddots&0\\
			0&\dots&\dots&\dots&\dots&0&1\\
		\end{pmatrix}
	\end{equation}
	où le coefficient est à la $i$-ième ligne et la $j$-ième colonne. La matrice à gauche est diagonalisable car son polynôme caractéristique est scindé à racines simples. Donc les matrices de transvections sont dans $G$. De plus, les matrices de dilatations sont aussi dans $G$. Donc $G=GL_{n}(\R)$.
\end{proof}

\begin{proof}
	Supposons $u$ diagonalisable, il existe un base $\mathcal{B}$ telle que 
	\begin{equation}
		\mat_{\mathcal{B}}(u)=A=\diag(0,\dots,0,\lambda_{1},\dots,\lambda_{r})
	\end{equation}
	avec $\lambda_{i}\neq0$. Donc $\mat_{\mathcal{B}}(u^{p})=A^{p})\diag(0,\dots,0,\lambda_{1}^{p},\dots,\lambda_{r}^{p})$ donc $u^{p}$ est diagonalisable. On a toujours $\ker(u)\subset\ker(u^{2})$ et la forme diagonale implique $\ker(u)=\ker(u^{2})$.

	Supposons $u^{p}$ diagonalisable, on écrit $\Pi_{u^{p}}=(X-\lambda_{0})\dots(X-\lambda_{r})=R$ (avec $\lambda_{k}\neq0$ pour tout $k\geqslant k$) qui est scindé à racines simples. On a 
	\begin{equation}
		P(u^{p})=0=(u^{p}-\lambda_{0}id_{E})\circ\dots\circ(u^{p}-\lambda_{r}id_{E})=Q(u)
	\end{equation}
	avec $Q(X)=P(X^{p})$. 

	Si $\lambda_{0}\neq0$, chaque $\lambda_{k}$ admet $p$ racines $p$-ièmes distinctes et si $\mu_{k}$ est l'une de ses racines, on a 
	\begin{equation}
		X^{p}-\lambda_{k}=\prod_{j=1}^{p}\left(X-\mu_{k}\e^{\i\frac{2j\pi}{p}}\right)
	\end{equation}
	De plus, les racines $p$-ièmes des $(\lambda_{k})_{kk\in\llbracket1,r\rrbracket}$ sont deux à deux distinctes. Donc $Q$ est scindé à racines simples, et donc $u$ est diagonalisable.

	Si $\lambda_{0}=0$, on a $Q=X^{p}A(X)$ avec $A$ scindé à racines simples non nulles et $X^{p}\wedge A=1$. D'après le lemme des noyaux, on a 
	\begin{equation}
		\ker(Q(u))=\C^{n}=\ker(u^{p})\otimes\ker(A(u))=\ker(u^{p})\otimes_{i\in I}\ker(u-\mu_{i}id)
	\end{equation}
	car $A$ est scindé à racines simples.
	Montrons que $\ker(u)=\ker(u^{p})$. L'inclusion directe est évidente. Réciproquement, montrons que pour tout $k\in\N$, on a $\ker(u^{k})\subset\ker(u^{k+1})$ et si $\ker(u^{k})=\ker(u^{k+1})$, alors $\ker(u^{k+1})=\ker(u^{k+2})$. L'inclusion est évidente, et si on a l'égalité, si $x\in\ker(u^{k+2})$, on a $u(x)\in\ker(u^{k+1})=\ker(u^{k})$ donc $x\in\ker(u^{k+1})$.
	Comme $\ker(u)=\ker(u^{2})$, d'après ce qui précède, par récurrence, on a $\ker(u)=\ker(u^{p})$, donc $u$ est diagonalisable.
\end{proof}

\begin{proof}
	Soit $(e_{1},\dots,e_{n})$ la base canonique de $\C^{n}$, $u$ canoniquement associée à 
	\begin{equation}
		J_{n}=
		\begin{pmatrix}
			0 & 1 & 0&\dots&0\\
			\vdots & \ddots & \ddots &\ddots& \vdots\\
			\vdots&&\ddots&\ddots&0\\
			0 &&&\ddots&1\\
			1 & 0&\dots &\dots&0
		\end{pmatrix}
	\end{equation}. On a 
	\begin{equation}
		\left\lbrace
			\begin{array}[]{lll}
				u(e_{1})&=&e_{n}\\
				u(e_{2})&=&e_{1}\\
				\vdots\\
				u(e_{n})&=&e_{n-1}
			\end{array}
		\right.
	\end{equation}
	d'où 
	\begin{equation}
		\left\lbrace
			\begin{array}[]{lll}
				u^{k}(e_{1})&=&e_{n+1-k}\\
				\vdots
				u^{k}(e_{k-1})&=&e_{n-1}\\
				\vdots\\
				u^{k}(e_{n})&=&e_{n-k}
			\end{array}
		\right.
	\end{equation}
	et donc 
	\begin{equation}
		J_{n}^{k}=
		\begin{pmatrix}
			0 & \dots & \dots&0 &1 &0&\dots &0\\
			\vdots&\ddots&&&\ddots&\ddots&\ddots&\vdots\\
			\vdots&&\ddots&&&\ddots&\ddots&0\\
			0&&&\ddots&&&\ddots&1\\
			1&\ddots&&&\ddots&&&0\\
			0&\ddots&\ddots&&&\ddots&&\vdots\\
			\vdots&\ddots&\ddots&\ddots&&&\ddots&\vdots\\
			0&\dots&0&1&0&\dots&\dots&0
		\end{pmatrix}
	\end{equation}
	où les $1$ commencent à la $k+1$-ième colonne sur la première ligne et à la $n-k+1$-ième ligne sur la première colonne. Notamment, le 1 sur la dernière colonne est à la $n-k$-ième ligne.

	On a $A(a_{0},\dots,a_{n})=\sum_{k=0}^{n-1}a_{k}J_{n}^{k}$. En développant par rapport à la première ligne, on a 
	\begin{equation}
		\chi_{J_{n}}(X)=X
		\begin{vmatrix}
			X&-1&0&\dots&\dots&0\\
			0&\ddots&\ddots&\ddots&&\vdots\\
			\vdots&\ddots&\ddots&\ddots&\ddots&\vdots\\
			\vdots&&\ddots&\ddots&\ddots&0\\
			\vdots&&&\ddots&\ddots&-1\\
			0&\dots&\dots&\dots&0&X
		\end{vmatrix}+
		\begin{vmatrix}
			0&-1&0&\dots&\dots&0\\
			0&X&\ddots&\ddots&&\vdots\\
			\vdots&\ddots&\ddots&\ddots&\ddots&\vdots\\
			\vdots&&\ddots&\ddots&\ddots&0\\
			0&&&\ddots&\ddots&-1\\
			-1 &0&\dots&\dots&0&X
		\end{vmatrix}
	\end{equation}
	Le premier déterminant vaut $X^{n-1}$ et le deuxième vaut $-(-1)^{n}\times(-1)^{n-2}=-1$ donc $\chi_{J_{n}}(X)=X^{n}-1$.
	Ainsi, $\chi_{J_{n}}$ est scindé à racines simples sur $\C$ donc $J_{n}$ est diagonalisable avec des sous-espaces propres de dimension 1. Soit $\omega=\e^{\frac{2\i\pi}{n}}$, on a $\Sp(J_{n})=\left\lbrace\omega^{k},0\leqslant k\leqslant n-1\right\rbrace$. On a $J_{n}X=\omega^{k}X$ si et seulement si 
	\begin{equation}
		\left\lbrace
			\begin{array}[]{lll}
				x_{2}&=&\omega^{k}x_{1}\\
				\vdots\\
				x_{n}&=&=\omega^{k}x_{n-1}\\
				x_{1}&=&\omega^{k}x_{n}
			\end{array}
		\right.
	\end{equation}
	si et seulement si 
	\begin{equation}
		X=x_{1}\begin{pmatrix}
			1\\
			\omega^{k}\\
			\omega^{2k}\\
			\vdots\\
			(\omega^{k})^{n-1}
		\end{pmatrix}=x_{1}X_{k}
	\end{equation}
	avec $X_{k}$ vecteur propre de $J_{n}$ associé à $\omega^{k}$. Posons 
	\begin{equation}
		P=
		\begin{pmatrix}
			1&1&\dots&1\\
			\vdots&\omega&&\omega^{n-1}\\
			\vdots&\vdots&&\vdots\\
			1&\omega^{n-1}&\dots&(\omega^{n-1})^{n-1}
		\end{pmatrix}
	\end{equation}
	et $P^{-1}J_{n}P=\diag(1,\omega,\dots,\omega^{n-1})$. On a donc 
	\begin{equation*}
		P^{-1}A(a_{0},\dots,a_{n})P=\diag(Q(1),Q(\omega),\dots,Q(\omega^{n-1})),
	\end{equation*} où $Q=\sum_{k=0}^{n-1}a_{k}X^{k}$.
	Donc $A$ est diagonalisable de valeurs propres $Q(1),\dots,Q(\omega^{n-1})$ et donc
	\begin{equation}
		\boxed{\det(A)=\prod_{k=0}^{n-1}Q(\omega^{k})}
	\end{equation}
\end{proof}

\begin{remark}
	On a 
	\begin{equation}
		\begin{vmatrix}
			a&b&c\\
			c&a&b\\
			b&c&a
		\end{vmatrix}=(a+b+c)(a+\j b+\j^{2}c)(a+\j^{2}b+\j c)=(a+b+c)(a^{2}+b^{2}+c^{2}-ab-bc-ac)
	\end{equation}

	Si $a,b,c\in\R_{+}$ vérifient $a+b+c=1$, on a 
	\begin{equation}
		\left\lvert a+\j b+\j^{2}c\right\rvert=\left\lvert a+\j^{2}b+\j c\right\rvert\leqslant a+b+c=1
	\end{equation}
	si et seulement si $a,\j b,\j^{2}c$ ont même argument si et seulement si $\lbrace a,b,c\rbrace=\lbrace1,0,0\rbrace$.
\end{remark}

\begin{proof}
	On sait que que $f^{n}=0$ d'après le théorème de Cayley-Hamilton et que pour tout $k\in\N$, $\ker(f^{k})\subset\ker(f^{k+1})$ et si $\ker(f^{k})=\ker(f^{k+1})$, alors $\ker(f^{k})=\ker(f^{m})$ pour tout $m\geqslant k$.

	Soit $k\in\llbracket0,n-1\rrbracket$ et \function{u}{\ker(f^{+1})}{\ker(f^{k})}{x}{u(x)} est bien définie car si $x\in\ker(f^{k+1}),f(x)\in\ker(f^{k})$. Comme $\ker(f)\subset\ker(f^{k+1})$, $\ker(u)=\ker(f)$ et $\dim(\ker(u))=1$. D'après le théorème du rang, on a $\dim(\ker(f^{k+1}))=\rg(u)+1\leqslant\dim(\ker(f^{k}))+1$. Par récurrence, on a pour tout $k\in\N$, $\dim(\ker(f^{k}))\leqslant k$ (car on ne peut croître au lus de 1 à chaque itération).

	Si $f^{n-1}=0$, on a $\dim(\ker(f^{n-1}))=n\leqslant n-1$ ce qui est absurde. Donc 
	\begin{equation}
		\boxed{f^{n-1}\neq0}
	\end{equation}

	Soit $x\notin\ker(f^{n-1})$. Soit $(\alpha_{0},\dots,\alpha_{n-1})\in\K^{n}$. Si $\alpha_{0}x+\dots+\alpha_{n-1}f^{n-1}(x)=0$, en appliquant $f^{n-1}$, on a $\alpha_{0}f^{n-1}(x)=0$ donc $\alpha_{0}=0$. Puis on applique $f^{n-2}$, etc. De proche en proche, $\alpha_{0}=\alpha_{1}=\dots=\alpha_{n-1}=0$. Ainsi, $\mathcal{B}=(x,f(x),\dots,f^{n-1}(x))$ est libre en dimension $n$, c'est donc une base et on a 
	\begin{equation}
		\mat_{\mathcal{B}}(f)=
		\begin{pmatrix}
			0&\dots&\dots&\dots&0\\
			1&\ddots&&&\vdots\\
			0&\ddots&\ddots&&\vdots\\
			\vdots&\ddots&\ddots&\ddots&\vdots\\
			0&\dots&0&1&0
		\end{pmatrix}
	\end{equation}
	qui est une matrice nilpotente d'indice $n$. Matriciellement, on a 
	\begin{equation*}
		\ker(f^{k})=\Vect(e_{n-k+1},\dots,e_{n}).
	\end{equation*}
\end{proof}

\begin{proof}
	Supposons qu'il existe $x\in V$, $(x,u(x),\dots,u^{n-1}(x))$ soit une base de $V$. Notons $u^{n}(x)=a_{0}x+\dots+a_{n-1}u^{n-1}(x)$. Soit $y\in V$ tel que $u(y)=\lambda y$. Pour $y=\sum_{i=0}^{n-1}y_{i}u^{i}(x)$. On a donc 
	\begin{equation}
		u(y)=\sum_{i=0}^{n-1}y_{i}u^{i+1}(x)=\sum_{i=0}^{n-1}\lambda y_{i}u^{i}(x)=\sum_{i=1}^{n-1}y_{i-1}u^{i}(x)+y_{n-1}\sum_{i=0}^{n-1}a_{i}u^{i}(x)
	\end{equation}
	Donc $u(y)=\sum_{i=1}^{n-1}u^{i}(x)(y_{i-1}+y_{n-1}a_{i})+y_{n-1}a_{0}x$ donc 
	\begin{equation}
		\left\lbrace
			\begin{array}[]{lll}
				\lambda y_{0} &= &y_{n-1}a_{0}\\
				\lambda y_{1} &= &y_{0}+a_{1}y_{n-1}\\
				\vdots\\
				\lambda y_{n-2} &= &y_{n-3}+a_{n-2}y_{n-1}\\
				\lambda y_{n-1} &= &y_{n-2}+a_{n-1}y_{n-1}
			\end{array}
		\right.
	\end{equation}
	donc par récurrence 
	\begin{equation}
		\left\lbrace
			\begin{array}[]{lll}
				\lambda y_{n-2} &= &(\lambda -a_{n-1})y_{n-1}\\
				\lambda y_{n-3} &= &(\lambda(\lambda-a_{n-1})-a_{n-2})y_{n-1}\\
				\vdots\\
				\lambda y_{0} &= &(\lambda^{n-1}-a_{n-1}\lambda^{n-2}-\dots-a_{1})y_{n-1}
			\end{array}
		\right.
	\end{equation}
	Donc les sous-espaces propres sont de dimension 1.

	Supposons que les sous-espaces propres de $u$ sont de dimension 1. On écrit $\chi_{u}=\prod_{i=1}^{r}(X-\lambda_{i})^{n_{i}}$. D'après le théorème de Cayley-Hamilton et le lemme des noyaux, on a 
	\begin{equation}
		V=\bigotimes_{i=1}^{r}\underbrace{\ker(u-\lambda_{i}id_{V})^{n_{i}}}_{F_{i}}
	\end{equation}
	et les sous-espaces caractéristiques $F_{i}$ sont stables par $u$. Soit $v_{i}=u_{\mid F_{i}}-\lambda_{i}id_{F_{i}}$. On a $\chi_{u}=\prod_{i=1}^{r}\chi_{u_{\mid F_{i}}}$ (matrice diagonale par blocs dans un base adaptée). $(X-\lambda_{i})^{n}$ annule $u_{\mid F_{i}}$ et $\Sp_{F_{i}}(u_{\mid F_{i}})=\lbrace\lambda_{i}\rbrace$. Alors $\chi_{u_{\mid F_{i}}}=(X-\lambda_{i})^{\dim(F_{i})}$. En reportant, on a $\dim(F_{i})=n_{i}$. De plus, $V_{i}^{n_i}=0$ donc $v_{i}$ est nilpotent. On a donc $\dim(\ker(v_{i}))=\dim(\ker(u-\lambda_{i}id_{E}))=1$. Donc il existe $x_{i}\in F_{i}$ tel que $(x_{i},v_{i}(x_{i}),\dots,v_{i}^{n_{i}-1}(x_{i}))$ soit une base de $F_{i}$.

	On forme $x=\sum_{i=1}^{r}x_{i}$. Soit $(\alpha_{0},\dots,\alpha_{r-1})$ tel que 
	\begin{equation*}
		\sum_{j=0}^{n-1}\alpha_{j}u^{j}(x)=0=\sum_{i=1}^{r}\left(\sum_{j=0}^{n-1}\alpha_{j}u^{j}(x_{i})\right).	
	\end{equation*}
	Les $F_{i}$ sont en somme directe donc 
	\begin{equation}
		\sum_{j=0}^{n-1}\alpha_{j}u^{j}(x_{i})=0
	\end{equation}

	Soit $P(X)=\sum_{j=0}^{n-1}\alpha_{j}X^{j}$. $I_{x_{i}}=\left\lbrace A\in\C[X]\middle| A(u)(x_{i})=0\right\rbrace$ est un idéal de $\C[X]$ donc est principal et il existe $\Pi_{i}\in I_{x_{i}}$ minimal et 
	\begin{equation}
		\Pi_{i}\mid P
	\end{equation}
	On a $(X-\lambda_{i})^{n_{i}}(u)(x_{i})=0$ et $(x_{i},u(x_{i}),\dots,u^{n_{i}-1}(x))$ est libre, donc si $P\in I_{x_{i}}$, $\deg(P)\geqslant n_{i}$ donc $\deg(\Pi_{i})=n_{i}$ et $\Pi_{i}=(X-\lambda_{i})^{n_{i}}$. Ainsi, pour tout $i\in\llbracket1,r\rrbracket$, $\Pi_{i}\mid P$ et donc 
	\begin{equation}
		\prod_{i=1}^{r}(X-\lambda_{i})^{n_{i}}\mid P
	\end{equation}
	Mais $P$ est de degré $\leqslant n-1$, nécessairement $P=0$ et $(x,u(x),\dots,u^{n-1}(x))$ est libre.
\end{proof}

\begin{remark}
	Autre méthode pour le sens direct: on a 
	\begin{equation}
		\mat_{(x,u(x),\dots,u^{n-1}(x))}(u)=
		\begin{pmatrix}
			0&\dots&\dots&0&a_{0}\\
			1&\ddots&&\vdots&\vdots\\
			0&\ddots&\ddots&\vdots&\vdots\\
			\vdots&\ddots&\ddots&0&\vdots\\
			0&\dots&0&1&a_{n-1}
		\end{pmatrix}=A
	\end{equation}

	Si $\lambda\in\Sp(u)$, on a 
	\begin{equation}
		A-\lambda I_{n}=
		\mat_{(x,u(x),\dots,u^{n-1}(x))}(u)=
		\begin{pmatrix}
			-\lambda&\dots&\dots&0&a_{0}\\
			1&\ddots&&\vdots&\vdots\\
			0&\ddots&\ddots&\vdots&\vdots\\
			\vdots&\ddots&\ddots&-\lambda&\vdots\\
			0&\dots&0&1&a_{n-1}-\lambda
		\end{pmatrix}
	\end{equation}
	qui est non inversible, mais donc les $(n-1)$ première colonnes sont libres, donc est de rang $n-1$.
\end{remark}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item On utilise le fait que pour tout $k\in\N$ tel que $\im(f^{k+1})\subset\im(f^{k})$. S'il existe $k\in\N$, $\im(f^{k+1})=\im(f^{k})$ alors pour tout $l\geqslant k$, $\im(f^{k})=\im(f^{l})$. 

		En effet, si $x=f^{k+1}(x')\in\im(f^{k+1})$,, on a $x=f^{k}(f(x))\in\im(f^{k})$. Si on a égalité des espaces, soit $x=f^{k+1}(x')=f(f^{k}(x'))\in\im(f^{k+1})$. Alors $f^{k}(x')\in\im(f^{k})=\im(f^{k+1})$ donc il existe $x''$ tel que $f^{k}(x')=f^{k+1}(x'')$, mais alors $x=f^{k+2}(x'')\in\im(f^{k+2})$. On a donc le résultat en itérant.

		Ainsi, pour tout $n\geqslant d$, on a $\rg(f^{n})=\rg(f^{d})$ donc $(\rg(f^{n}))_{n\in\N}$ est stationnaire au moins à partir de $d$ et $r(f)=\rg(f^{d})$.

		\item Comme $f$ et $g$ commutent, on a 
		\begin{equation}
			(f+g)^{2d}=\sum_{k=0}^{2d}\binom{2d}{k}f^{k}g^{2d-k}
		\end{equation}
		Pour tot $k\in\llbracket0,2d\rrbracket$, on a $k\geqslant d$ ou $2d-k\geqslant d$ donc 
		\begin{equation}
			\left\lbrace
				\begin{array}[]{l}
					\im(f^{k}g^{2d-k})\subset\im(f^{d})\\
					\text{ou}\\
					\im(f^{k}g^{2d-k})\subset\im(g^{d})
				\end{array}
			\right.
		\end{equation}
		et donc $\im(f^{k}g^{2d-k})\subset\im(f^{d})+\im(g^{d})$. Finalement, $\im(f+g)^{2d}\subset\im(f^{d})+\im(g^{d})$. On a donc 
		\begin{align}
			r(f+g)
			&=\dim(\im(f+g)^{2d})\\
			&\leqslant \dim(\im(f^{d})+\im(g^{d}))\\
			&\leqslant \dim(\im(f^{d}))+\im(g^{d})\\
			&\leqslant r(f)+r(g)
		\end{align}

		Pour un contre-exemple, on utilise $A=\begin{pmatrix}
			0&0\\1&0
		\end{pmatrix}$ et $B=A^{\mathsf{T}}$. On a $A^{2}=B^{2}$ donc $r(A^{2})=r(B^{2})=0$ et $A+B$ inversible donc $r(A+B)=2>r(A)+r(B)$.

		\item On a $\chi_{f}=X^{m_{0}}Q$ avec $\deg(Q)=d-m_{0}$ et $Q(0)=0$. D'après le lemme des noyaux, on a 
		\begin{equation}
			V=\ker(f^{m_{0}})\otimes\ker(Q(f))
		\end{equation}
		Dans une base adaptée $\mathcal{B}$, on a $\mat_{\mathcal{B}}(f)=\begin{pmatrix}
			A&0\\0&B
		\end{pmatrix}$ avec $A^{m_{0}}=0$ et $B$ inversible. Alors pour tout $k\geqslant m_{0}$, $\mat_{\mathcal{B}}(f^{k})=\begin{pmatrix}
			0&0\\0&B^{k}
		\end{pmatrix}$ et $\rg(f^{k})=\rg(B^{k})=d-m_{0}=r(f)$.
	\end{enumerate}
\end{proof}


\begin{proof}
	On munit $\M_{n}(\C)$ de la norme $\vertiii{A}=\sup\limits_{\left\lVert X\right\rVert_{\infty}=1}\left\lVert AX\right\rVert_{\infty}$. Notons que si $\lambda\in\Sp_{\C}(A)$, alors $\left\lvert\lambda\right\rvert\leqslant\vertiii{A}$. En effet, si $X$ est un vecteur propre associé à $\lambda$, on a 
	\begin{equation}
		\left\lVert AX\right\rVert_{\infty}=\left\lvert\lambda\right\rvert\left\lVert X\right\rVert_{\infty}\leqslant\vertiii{A}\left\lVert X\right\rVert_{\infty}
	\end{equation}
	et $X\neq0$ donc $\left\lVert X\right\rVert_{\infty}\neq0$.

	Soit $A=\e^{\frac{2\i k\pi}{q}}I_{n}$, soit $B\in \mathcal{G}_{q}$ telle que $\vertiii{B-A}\leqslant\sin\left(\frac{\pi}{q}\right)$. Soit $\mu\in\Sp_{\C}(B)$, on a $\mu\in\U_{q}$ car $B^{q}=I_{n}$. Donc $\mu-\e^{\frac{2\i k\pi}{q}}\in\Sp_{\C}(B-A)$ et 
	\begin{equation}
		\left\lvert\mu-\e^{\frac{2\i k\pi}{q}}\right\rvert\leqslant\sin\left(\frac{\pi}{q}\right)
	\end{equation}
	Si $\mu=\e^{\frac{2\i l\pi}{q}}$, on a 
	\begin{equation}
		\left\lvert\mu-\e^{\frac{2\i k\pi}{q}}\right\rvert=2\left\lvert\sin\left(\frac{\left(l-k\right)\pi}{q}\right)\right\rvert>\sin\left(\frac{\pi}{q}\right)
	\end{equation}
	si $l\neq k$. Nécessairement, on a $\mu=\e^{\frac{2\i k\pi}{q}}$, donc $B=A$ car $B$ est diagonalisable et $\Sp_{\C}(B)=\left\lbrace\e^{\frac{2\i k\pi}{q}}\right\rbrace$. Donc $A$ est un point isolé de $\mathcal{G}_{q}$.

	Soit maintenant $A\in\mathcal{G}_{q}$, on suppose que $A$ n'est pas une matrice scalaire, donc $\left\lvert\Sp_{\C}(A)\right\rvert\geqslant2$. Soit $\lambda\in\left(\Sp_{\C}(A)\right)$.
	Il existe $P\in GL_{n}(\C)$ telle que 
	\begin{equation*}
		P^{-1}AP=\diag(\lambda,\dots,\lambda,\mu_{1},\dots,\mu_{r}),
	\end{equation*}avec $\mu_{1},\dots,\mu_{r}\neq\lambda$. Soit $\varepsilon>0$, posons 
	\begin{equation}
		A_{\varepsilon}=P
		\begin{pmatrix}
			\lambda &0 &\dots&\dots &\dots&\dots&\dots&\dots&0\\
			0 & \ddots & \ddots &&&&&&\vdots\\
			\vdots &\ddots&\ddots&0&\dots&\dots&\dots&\dots&\vdots\\
			\vdots&&\ddots&\lambda & \varepsilon&0&\dots&\dots&0\\
			\vdots&&& \ddots & \mu_{1}&0&\dots&\dots&0\\
			\vdots&&&&\ddots&\ddots&\ddots&&\vdots\\
			\vdots&&&&&\ddots&\ddots&\ddots&\vdots\\
			\vdots&&&&&&\ddots&\ddots&0\\
			0&\dots&\dots&\dots&\dots&\dots&\dots&0&\mu_{r}
		\end{pmatrix}P^{-1}
	\end{equation}

	On a $A_{\varepsilon}\xrightarrow[\varepsilon\to0]{}A$ et $A_{\varepsilon}\neq A$. Montrons que $A_{\varepsilon}\in \mathcal{G}_{q}$. On a $\chi_{A_{\varepsilon}}=\chi_{A}$, $\rg\left(A_{\varepsilon}-\lambda I_{n}\right)=\rg(A-\lambda I_{n})$ (observer les colonnes) et pour $\mu_{l}\in\Sp(A)$, $\mu_{l}\neq \lambda$, on a $\rg\left(A_{\varepsilon}-\mu_{l}I_{n}\right)=\rg\left(A-\mu_{l}I_{n}\right)$ (observer les lignes). La dimension des sous-espaces propres de $A$ et $A_{\varepsilon}$ sont les mêmes donc $A_{\varepsilon}$ est diagonalisable. De plus, $\Sp(A_{\varepsilon})\subset\Sp(A)\subset\U_{q}$ donc $A_{\varepsilon}\in\mathcal{G}_{q}$. Ainsi, $A$ n'est pas isolé dans $\mathcal{G}_{q}$.
\end{proof}

\begin{proof}
	On a 
	\begin{equation}
		\chi_{M}(\lambda)=
		\begin{vmatrix}
			\lambda-1 &1&0\\
			1&\lambda-2&-1\\
			-1&0&\lambda-1
		\end{vmatrix}=(\lambda-1)(\lambda-2)(\lambda-1)-\left((\lambda-1)-1\right)=\lambda(\lambda-2)^{2}
	\end{equation}

	Soit $X=\begin{pmatrix}
		x&y&z
	\end{pmatrix}^{\mathsf{T}}$. On a $MX=0$ si et seulement si $y=x$ et $z=-x$ donc $E_{0}=\Vect\begin{pmatrix}
		1&1&-1
	\end{pmatrix}=\Vect(\varepsilon_{1})$.

	On a $(M-2I_{3})X=0$ si et seulement si $y=z=-x$ donc $E_{2}=\Vect\begin{pmatrix}
		1&-1&-1
	\end{pmatrix}=\Vect(\varepsilon_{2})$.

	$M$ n'est pas diagonalisable sur $\R$ mais trigonalisable. D'après le théorème de Cayley-Hamilton et le lemme des noyaux, on a 
	\begin{equation}
		\R^{3}=\ker(u)\otimes\ker(u-2 id)^{2}
	\end{equation}
	Soit $P\in GL_{n}(\C)$ tel que 
	\begin{equation}
		P^{-1}MP=
		\begin{pmatrix}
			0&0&0\\
			0&2&\star\\
			0&0&2
		\end{pmatrix}
	\end{equation}
	avec $\varepsilon_{3}\in\ker(u-2id)^{2}$ et $\varepsilon_{3}\notin\Vect(\varepsilon_{2})$. On a 
	\begin{equation}
		(M-2I_{3})^{2}=
		\begin{pmatrix}
			2&1&-1\\
			2&1&-1\\
			-2&-1&1
		\end{pmatrix}
	\end{equation}
	donc $(M-2I_{3})^{2}X=0$ si et seulement si $2x-y+z=0$. On pose $\varepsilon_{3}=\begin{pmatrix}
		0\\1\\1
	\end{pmatrix}$. On a $M\varepsilon_{3}=-\varepsilon_{2}+2\varepsilon_{3}$ donc si 
	\begin{equation}
		P=
		\begin{pmatrix}
			1&1&0\\
			1&-1&1\\
			-1&1&1
		\end{pmatrix}
	\end{equation}
	on a 
	\begin{equation}
		P^{-1}MP=
		\begin{pmatrix}
			0&0&0\\
			0&2&-1\\
			0&0&2
		\end{pmatrix}
	\end{equation}

	Les sous-espaces stables de dimension $0$: $\lbrace0\rbrace$. Les sous-espaces stables de dimension 1: ils sont engendrés par les vecteurs propres, ce sont donc $\Vect(\varepsilon_{1})$ et $\Vect(\varepsilon_{2})$. Si maintenant $F$ est un sous-espace stable de dimension $2$, montrons que l'on a 
	\begin{equation}
		F=\left(F\cap\ker(u)\right)\otimes\left(F\cap\ker(u-2id)^{2}\right)=F_{0}\otimes F_{2}
	\end{equation}
	En effet, on a $F_{0}\otimes F_{2}\subset F$. Si maintenant $x\in F$, a priori on a $x=x_{0}+x_{2}$ avec $x_{0}\in\ker(u)$ et $x_{2}\in\ker(u-2id)^{2}$. On a $u(x)=u(x_{2})\in F$ par stabilité, $u^{2}(x)=u^{2}(x_{2})\in F$, et $(u-2id)^{2}(x_{2})=0$ donc $x_{2}=\frac{1}{4}\left(-u^{2}(x)+4u(x_{2})\right)\in F$ et $x_{0}=x-x_{2}\in F$.

	On a $F_{0}=\lbrace0\rbrace$ ou $\ker(u)$. Si $F_{0}=\lbrace0\rbrace$, on a $F=F_{2}$. Si $F_{0}=\ker(u)$, on a $\dim(F_{2})=1$ donc $F_{2}=\Vect(\varepsilon_{2})$.

	Donc les sous-espaces stables de dimension 2 sont $\ker(u-2id)^{2}$ et $\Vect(\varepsilon_{1},\varepsilon_{2})$. 

	Enfin, les sous-espaces stables de dimension 3: $\R^{3}$.
\end{proof}

\begin{remark}
	Plus généralement, si $\chi_{u}=\prod_{i=1}^{r}(X-\lambda_{i})^{m_{i}}$. On écrit 
	\begin{equation}
		E=\bigotimes_{i=1}^{r}\ker(u-\lambda_{i}id_{E})^{m_{i}}=\bigotimes_{i=1}^{r}F_{i}
	\end{equation}
	Si $F$ est stable, on note $\Pi_{i}$ le projecteur sur $F_{i}$ parallèlement à $\bigotimes_{\substack{j=1\\j\neq i}}^{r}F_{i}\in\K[u]$. On a pour tout $x\in F$, $\Pi_{i}(x)\in F$ par stabilité, il s'ensuit que 
	\begin{equation}
		\boxed{F=\bigotimes_{i=1}^{r}\left(F\cap F_{i}\right)}
	\end{equation}
\end{remark}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item Si $(a_{1},\dots,a_{n})\neq(0,\dots,0)$, on a 
		\begin{equation}
			A-I_{n+1}=
			\left(
			\begin{array}{@{}c|c@{}}
			0_{n} &
			\begin{matrix}
				a_{1}\\
				\vdots\\
				a_{n}
				\end{matrix}
				\\
			\hline
			\begin{matrix}
				a_{1} &
				\dots
				& a_{n}
				\end{matrix}
				& 0
			\end{array}
			\right)
		\end{equation}
		donc $\rg(A-I_{n+1})=2$ et $\chi_{A-I_{n+1}}=X^{n-1}(X-\lambda)(X-\mu)$ (sur $\C$). On a $\Tr(A-I_{n+1})=0=\mu+\lambda$ et $\Tr(A-I_{n+1})^{2}=2\sum_{i=1}^{n}a_{i}^{2}=\lambda^{2}+\mu^{2}$ donc $\lbrace\lambda,\mu\rbrace\in\left\lbrace\pm\sqrt{\sum_{i=1}^{n}a_{i}^{2}}\right\rbrace$ et $A-I_{n+1}$ est semblable à 
		\begin{equation}
			\begin{pmatrix}
				0&\star&\dots&\dots&\star\\
				0&\ddots&\ddots&&\vdots\\
				\vdots&\ddots&0&\ddots&\vdots\\
				\vdots&&\ddots&\lambda&\star\\
				0&\dots&\dots&0&\mu
			\end{pmatrix}
		\end{equation}

		\item On note $\lambda=\sqrt{\sum_{i=1}^{n}a_{i}^{2}}$. Soit $X$ tel que $A'X=\pm\lambda$ où $A'=A-I_{n+1}$. Alors en écrivant le système, on vérifie que l'on peut prendre 
		\begin{equation}
			f_{\pm}=\begin{pmatrix}
				a_{1}\\
				\vdots\\
				a_{n}\\
				\lambda
			\end{pmatrix}	
		\end{equation}
		et si $X$ est tel que $A'X=0$, si $i_{0}$ est tel que $a_{i_{0}}\neq0$, on récupère une bas de $\ker(A')$ avec 
		\begin{equation}
			f_{i}=\begin{pmatrix}
				0\\
				\vdots\\
				0\\
				1\\
				0\\
				\dots\\
				0\\
				-\frac{a_{i}}{a_{i_{0}}}\\
				0\\
				\vdots\\
				0
			\end{pmatrix}
		\end{equation}
		où $i\in\llbracket1,n\rrbracket\setminus\lbrace i_{0}\rbrace$. Le 1 est à la $i$-ième ligne, $-\frac{a_{i}}{a_{i_{0}}}$ est à la ligne $i_{0}$.
	\end{enumerate}
\end{proof}


\begin{proof}
	On pose $\vertiii{A}=\sup\limits_{\left\lVert X\right\rVert_{\infty}=1}\left\lVert AX\right\rVert_{\infty}$. On montre d'abord que si $A\in\M_{n}(\C)$, alors pour tout $\lambda\in\Sp_{\C}(A)$ $\left\lvert\lambda\right\rvert\leqslant\vertiii{A}$. En effet, si $X$ non nul est tel que $AX=\lambda X$, on a $\left\lVert AX\right\rVert_{\infty}\leqslant\vertiii{A}\left\lVert X\right\rVert_{\infty}$ donc $\left\lvert\lambda\right\rvert\left\lVert X\right\rVert_{\infty}\leqslant\vertiii{A}\left\lVert X\right\rVert_{\infty}$, d'où le résultat.

	Soit alors $m=\sup\left\lbrace\vertiii{M}\middle| M\in G\right\rbrace$. Soit $M\in G$ et $\lambda\in\Sp_{\C}(M)$. On a $\left\lvert\lambda\right\rvert\leqslant m$. Comme $M^{k}\in G$ pour tout $k\in\Z$, on a aussi $\left\lvert\lambda\right\rvert^{k}\leqslant m$ donc $\left\lvert\lambda\right\rvert=1$ (faire tendre $k$ vers $-\infty$ et $+\infty$).

	Grâce à la décomposition de Dunford, on a $M=D+N$ avec $D$ diagonalisable, $N$ nilpotente et $D$ et $N$ qui commutent. Soit $r\in\N^{*}$ telle que $N^{r-1}\neq0$ et $N^{r}=0$. On a $\Sp_{\C}(D)=\Sp_{\C}(M)\subset\U$ donc $G\in GL_{n}(\C)$ et $MD^{-1}=I_{n}+ND^{-1}$ avec $ND^{-1}$ est nilpotente d'indice $r$. On a pour tout $k\geqslant r$, on a 
	\begin{align}
		(MD^{-1})^{k}
		&=M^{k}(D^{-1})^{k}\\
		&=\sum_{i=0}^{k}\binom{k}{i}(ND^{-1})^{i}\\
		&=\sum_{i=0}^{r-1}\binom{k}{i}(ND^{-1})^{i}\\
		&\underset{k\to+\infty}{\sim}\frac{k^{r-1}}{(r-1)!}N^{r-1}D^{k-r+1}
	\end{align}

	Notons que pour tout $(A,B)\in\M_{n}(\C)$, pour tout $X\in\M_{n,1}(\C)$ si $\left\lVert X\right\rVert_{\infty}=1$, on a 
	\begin{equation}
		\left\lVert (AB)X\right\rVert_{\infty}\leqslant\vertiii{A}\left\lVert BX\right\rVert_{\infty}\leqslant\vertiii{A}\vertiii{B}
	\end{equation}
	donc $\vertiii{AB}\leqslant\vertiii{A}\vertiii{B}$.

	La suite $(MD^{-1})^{k}$ est bornée, donc $r=1$ et $N=0$, donc $M$ est diagonalisable.

	Prenons ensuite $\alpha=\sqrt{3}$. Soit $\lambda\in\Sp_{\C}(M)$, on a $\lambda-1\in\Sp_{\C}(M-I_{n})$. Si $\lambda=\e^{\i\theta}$ avec $\theta\in]-\pi,\pi[$, on a $\left\lvert\e^{\i\theta}-1\right\rvert<\sqrt{3}$ si et seulement si $2\sin\left(\frac{\theta}{2}\right)<\sqrt{3}$ si et seulement si $\theta\in\left]-\frac{2\pi}{3},\frac{2\pi}{3}\right[$.

	Pour tout $k\in\R$, on a $\left(\e^{\i\theta}\right)^{k}\in\Sp(M^{k})$. Donc on a aussi $\left\lvert\sin\left(\frac{k\theta}{2}\right)\right\rvert<\frac{\sqrt{3}}{2}$. Quitte à changer $\theta$ en $-\theta$, on peut supposer $\theta\in\left[0,\frac{2\pi}{3}\right[$. Si $\theta\geqslant0$, posons l'unique $k\in\N$ tel que $k\theta\geqslant\frac{2\pi}{3}$ et $(k-1)\theta\in\left[0,\frac{2\pi}{3}\right[$. On a alors 
	\begin{equation}
		k\theta=(k-1)\theta+\theta<\frac{4\pi}{3}
	\end{equation}
	ce qui est absurde si et seulement si $\left\lvert\sin\left(\frac{k\theta}{2}\right)\right\rvert\geqslant\frac{\sqrt{3}}{2}$.

	Ainsi, $\theta=0$ et $\Sp(M)=\lbrace1\rbrace$, et puisque $M$ est diagonalisable, $M=I_{n}$ et $G=\lbrace I_{n}\rbrace$.
\end{proof}

\begin{remark}
	Soit $\alpha>\sqrt{3}$ et $G=\left\lbrace I_{n},\j I_{n},\j^{2} I_{n}\right\rbrace$. Pour tout $M\in G$, $\vertiii{M-I_{n}}<\alpha$ et $G\neq\lbrace I_{n}\rbrace$.
\end{remark}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item On vérifie que $\chi_{A}(\lambda)=\lambda^{3}$. On a $AX=0$ si et seulement si $x_{1}=x_{3}$ et $x_{2}=-2x_{1}$. On prend $\varepsilon_{1}=\begin{pmatrix}
			1\\-1\\1
		\end{pmatrix}$. $u$ est nilpotente et $\dim(\ker(u))=1$. On a $u^{3}=0$ et on a
		\begin{equation}
			A^{2}=\begin{pmatrix}
				1&1&1\\
				-2&-2&-2\\
				1&1&1
			\end{pmatrix}\neq0
		\end{equation}
		Soit $e_{1}=\begin{pmatrix}
			1\\0\\0
		\end{pmatrix}$, on a $u^{2}(e_{1})\neq0$ donc $(u^{2}(e_{1}),u(e_{1}),e_{1})$ est une base de $\R^{3}$. On a
		\begin{equation}
			\mat_{\mathcal{B}}(u)=\begin{pmatrix}
				0&1&0\\
				0&0&1\\
				0&0&0
			\end{pmatrix}
		\end{equation}
		
		$\dim(\ker(u^{k}))=k$ pour $k\in\lbrace0,1,2,3\rbrace$, notamment car $\rg(u^{2})=1$ pour justifier que $\dim(\ker(u^{2}))=2$.

		Soit $F$ stable par $u$ de dimension $i\in\lbrace0,1,2,3\rbrace$. $u_{\mid F}$ est nimpotente et $u_{\mid F}^{i}=0$. Donc $F\subset\ker(u^{i})$ qui est de dimension $i$. Donc $F=\ker(u^{i})$.

		\item Si $B^{2}=A$, $B^{6}=0$ donc $B^{3}=0$. Alors $B^{4}=0=A^{2}$ ce qui n'est pas vrai. Donc il n'y a pas de $B$ tel que $B^{2}=A$.
	\end{enumerate}
\end{proof}

\begin{proof}
	Soit $u\in\L(\R^{3})$ canoniquement associé à $A$. On a $X^{3}+X^{2}+X+1=(X+1)(X^{2}+1)$. D'après le lemme des noyaux, on a 
	\begin{equation}
		\R^{3}=\ker(u+id)\otimes\underbrace{\ker(u^{2}+id)}_{F}
	\end{equation}
	On a $F\neq\lbrace0\rbrace$ car $u\neq-id$. On note $v=u_{\mid F}$. On a $v^{2}=-id_{F}$ et $\det(v^{2})=(\det(v))^{2}=(-1)^{\dim(F)}>0$ donc $\dim(F)$ est pair. Nécessairement, on a $\dim(F)=2$ et $\dim(\ker(u+id))=1$. Soit $\varepsilon_{3}$ vecteur propre associé à -1. Soit $x\in F\setminus\lbrace0\rbrace$. Si $(x,u(x))$ est lié, $x$ est vecteur propre de $v$ et $v^{2}+id_{F}=0$ ce qui est impossible car il n'y a pas de valeur propre réelle. Donc on pose $\mathcal{B}=(x,u(x),\varepsilon_{3})$ base de $\R^{3}$. On a $u^{2}(x)=-x$, donc 
	\begin{equation}
		\boxed{
			\mat_{\mathcal{B}}(u)=
			\begin{pmatrix}
				0&-1&0\\
				1&0&0\\
				0&0&-1
			\end{pmatrix}
		}
	\end{equation}
\end{proof}

\begin{remark}
	Sur $\C$, on peut prendre 
	\begin{equation}
		A=
		\begin{pmatrix}
			\i&0&0\\
			0&-1&0\\
			0&0&-1
		\end{pmatrix}
	\end{equation}
	ou $\i I_{3}$.
\end{remark}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item $I_{x}=\left\lbrace A\in\K[X]\middle| A(f)(x)=0\right\rbrace$ est un idéal de $\K[X]$ non vide car $\mu_{f}\in I_{x}$. Donc il existe un unique $P_{x}$ unitaire tel que $I_{x}=P_{x}\K[X]$.
		\item Soit $A\in\K[X]$, on a $A(f)=0$ si et seulement si pour tout $x\in E$, $P_{x}\mid A$ si et seulement si $\vee_{x\in E}P_{x}\mid A$ donc $\mu_{f}=\vee_{x\in E}P_{x}$.
		\item On a 
		\begin{align}
			P_{x}P_{y}(f)(x+y)
			&=P_{x}P_{y}(f)(x)+P_{x}P_{y}(f)(y)\\
			&=P_{f}(f)\left(P_{x}(f)(x)\right)+P_{x}(f)\left(P_{y}(f)(y)\right)\\
			&=0
		\end{align}
		Donc $P_{x+y}\mid P_{x}P_{y}$.

		Soit $A\in\K[X]$. Supposons $A(f)(x+y)=0$. On a $P_{x}(f)\left(A(f)(x+y)\right)=0$, $P_{x}(f)\left(A(f)(x)\right)=A(f)\left(P_{x}(f)(x)\right)=0=-AP_{x}(f)(y)$. Donc $P_{y}\mid AP_{x}$. D'après le théorème de Gauss, $P_{y}\mid A$. De même, $P_{x}\mid A$. On prend $A=P_{x+y}$. Comme $P_{x}\wedge P_{y}=1$ et $P_{x}P_{y}\mid P_{x+y}$, on a 
		\begin{equation}
			\boxed{P_{x}P_{y}=P_{x+y}}
		\end{equation}

		\item On décompose $\mu_{f}=\prod_{i=1}^{r}A_{i}^{\alpha_{i}}$ avec pour tout $i\in\llbracket1,r\rrbracket$, $A_{i}$ irréductible sur $\K[X]$ et $\alpha_{i}\geqslant1$.
		Comme $\mu_{f}=\vee_{x\in E}P_{x}$, pour tout $i\in\llbracket1,r\rrbracket$, il existe $y_{i}\in E$ tel que $P_{y_{i}}=A_{i}^{\alpha_{i}}Q_{i}$. On pose $x_{i}=Q_{i}(f)(y_{i})$. Pour tout $A\in\K[X]$, on a $A(f)(x_{i})=0$ si et seulement si $AQ_{i}(f)(y_{i})=0$ si et seulement si $A_{i}^{\alpha_{i}}Q_{i}\mid AQ_{i}$ si et seulement si $A_{i}^{\alpha_{i}}\mid A$. Ainsi, $P_{x_{i}}=A_{i}^{\alpha_{i}}$. En utilisant le point précédent par récurrence, on a $\mu_{f}=P_{\sum_{i=1}^{r}x_{i}}$ et on pose donc $x=\sum_{i=1}^{r}x_{i}$.

		\item Supposons que ce $v$ existe. D'après le théorème de Cayley-Hamilton, $\deg(\mu_{f})\leqslant n$. Soit $(\alpha_{0},\dots,\alpha_{n-1})\in\K^{n}$. Si $\alpha_{0}id+\alpha_{1}f+\dots+\alpha_{n-1}f^{n-1}=0$. En appliquant en $v$, comme la famille est libre, on a de proche en proche $\alpha_{0}=\dots=\alpha_{n-1}=0$. Donc pour tout $A\in\K_{n-1}[X]$, si $A(f)=0$, alors $A=0$. Donc $\deg(\mu_{f})\geqslant n$, donc $\deg(\mu_{f})=n$.
		
		Réciproquement, soit $v\in E$ tel que $P_{v}=\mu_{f}$ qui existe d'après le point précédent. Soit $(\alpha_{0},\dots,\alpha_{n-1})\in\K^{n}$ tel que $\alpha_{0}v+\dots+\alpha_{n-1}f^{n-1}(v)=0$. On forme $A=\alpha_{0}+\dots+\alpha_{n-1}X^{n-1}$. On a $A(f)(v)=0$, donc $P_{v}\mid A$ mais $P_{v}$ est de degré $n$ donc $A=0$. Donc la famille est libre et de cardinal $n$: c'est une base.
	\end{enumerate}
\end{proof}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item $\varphi$ est linéaire, soit $(s,t)\in S^{2}$. On a $\varphi(s)=t$ si et seulement si $\frac{1}{n}\sum_{k=1}^{n}s_{k}=t_{n}$ pour tout $n\geqslant1$ si et seulement si 
		\begin{equation}
			\left\lbrace
				\begin{array}[]{lll}
					s_{1}&=&t_{1}\\
					s_{1}+s_{2}&=&2t_{2}\\
					\vdots\\
					s_{1}+\dots+s_{n-1}&=&(n-1)t_{n-1}\\
					s_{1}+\dots+s_{n}&=&nt_{n}\\
					\vdots
				\end{array}
			\right.
		\end{equation}
		si et seulement si 
		\begin{equation}
			\left\lbrace
				\begin{array}[]{lll}
					s_{1}&=&t_{1}\\
					s_{2}&=&2t_{2}-t_{1}\\
					\vdots\\
					s_{n}=nt_{n}-(n-1)t_{n-1}\\
					\vdots
				\end{array}
			\right.
		\end{equation}
		donc $\varphi$ est bijective.

		\item Soit $\lambda\in\R$, il existe $s\in S\setminus\lbrace0\rbrace$ tel que $\varphi(s)=\lambda s$ si et seulement si $\frac{1}{n}\sum_{k=1}^{n}s_{k}=\lambda s_{n}$ pour tout $n\geqslant1$ si et seulement si $s\in S\setminus\lbrace0\rbrace$ tel que $s=\lambda\varphi^{-1}(s)$ si et seulement si $s\in S\setminus\lbrace0\rbrace$ tel que $s_{1}=\lambda s_{1}$ et pour tout $n\geqslant2$, $s_{n}=\lambda(ns_{n}-(n-1)s_{n-1})$ i.e.~$(\lambda n-1)s_{n}=\lambda(n-1)s_{n-1}$.
		
		Si c'est le cas, si $s_{1}\neq0$, on a $\lambda=1$ et pour tout $n\geqslant2$, $s_{n}=s_{n-1}$ donc $s$ est constante.

		Sinon, soit $n_{0}=\min\left\lbrace n\in\N^{*}\middle| s_{n_{0}}\neq0\right\rbrace$. On a $(\lambda n_{0}-1)s_{n_{0}}=0$ donc $\lambda=\frac{1}{n_{0}}$ et pour tout $n> n_{0}$, on a $s_{n}=\frac{\frac{1}{n_{0}}(n-1)}{\frac{n}{n_{0}}-1}s_{n-1}=\frac{n-1}{n-n_{0}}s_{n-1}$. Ainsi, 
		\begin{equation}
			s_{n}=\frac{(n-1)!}{(n_{0}-1)!(n-n_{0})!}s_{n_{0}}=\binom{n-1}{n_{0}-1}s_{n_{0}}
		\end{equation}

		Réciproquement, en posant $s_{n_{0}}=1$ et en définissant pour tout $n>n_{0}$, $s_{n}=\binom{n-1}{n_{0}-1}$ et pour tout $n\leqslant n_{0}-1$, $s_{n}=0$, alors on a $\varphi(s)=\frac{1}{n_{0}}s$. Ainsi, 
		\begin{equation}
			\boxed{
				\Sp(\varphi)=\left\lbrace\frac{1}{n}\middle| n\in\N^{*}\right\rbrace
			}
		\end{equation}
		et les sous-espaces propres sont de dimension 1.
	\end{enumerate}
\end{proof}

\begin{remark}
	Si on se limite à $\R^{p}$, en définissant 
	\begin{equation*}
		\varphi_{p}(s_{1},\dots,s_{p})=(s_{1},\frac{s_{1}+s_{2}}{2},\dots,\frac{s_{1}+\dots+s_{p}}{p}),	
	\end{equation*}
	alors en écrivant la matrice de $\varphi_{p}$ dans la base canonique, on a 
	\begin{equation*}
		\chi_{\varphi_{p}}=(X-1)(X-\frac{1}{2})\dots(X-\frac{1}{p}).
	\end{equation*}
\end{remark}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item Soit $\lambda\in\Sp(A)$. Supposons que pour tout $i\in\llbracket1,n\rrbracket$, $\left\lvert\lambda-a_{i,i}\right\rvert >L_{i}$. $\lambda I_{n}-A$ est une matrice à diagonale strictement dominante donc inversible: absurde. Donc il existe $i\in\llbracket1,n\rrbracket$ tel que $\lambda\in D_{i}$. Comme $\lambda\in\Sp(A^{\mathsf{T}})$, il existe $i\in\llbracket1,n\rrbracket$ tel que $\lambda\in S_{i}$. D'où le résultat.
		\item Soit $X$ non nul (dans $\C^{n}$) tel que $AX=\lambda X$. Soit $i_{1}\in\llbracket1,n\rrbracket$ tel que $\left\lvert x_{i}\right\rvert=\left\lVert X\right\rVert_{\infty}>0$. On a, pour tout $i\in\llbracket1,n\rrbracket$, $(\lambda-a_{i,i})x_{i}=\sum_{j\neq i}a_{i,j}x_{j}$. 
		
		Soit $i_{2}\in\llbracket1,n\rrbracket$ tel que $\left\lvert x_{i_{2}}\right\rvert=\max_{i\neq i_{1}}\left\lvert x_{i}\right\rvert$. 
		
		Si $x_{i_{2}}=0$, on a $\lambda=a_{i_{1},i_{1}}$ et $\left\lvert \lambda-a_{i_{1},a_{1}}\right\rvert=0$ et $\left\lvert\lambda-a_{i_{1},a_{1}}\right\rvert\left\lvert\lambda-a_{i_{2},i_{2}}\right\rvert=0\leqslant L_{i_{1}}L_{i_{2}}$.

		Sinon, on a $\left\lvert\lambda-a_{i_{1},i_{1}}\right\rvert\left\lvert x_{i_{1}}\right\rvert\leqslant\left\lvert x_{i_{2}}\right\rvert L_{i_{1}}$ et de même $\left\lvert\lambda-a_{i_{2},i_{2}}\right\rvert\left\lvert x_{i_{2}}\right\rvert\leqslant\left\lvert x_{i_{1}}\right\rvert L_{i_{2}}$ d'où le résultat.
	\end{enumerate}
\end{proof}

\begin{remark}
	On peut avoir égalité, par exemple avec la matrice nulle.
\end{remark}

\begin{proof}
	$A$ est symétrique réelle, donc diagonalisable. Si pour tout $i\in\llbracket1,n\rrbracket$, $a_{i}=0$ alors $A=0$.

	Sinon, $\rg(A)=2$. Soit $u\in\L(\R^{n+1})$ canoniquement associée à $A$. On a $\dim(\ker(u))=n-1$ et $\chi_{A}=X^{n-1}(X-\lambda)(X-\mu)$ sur $\C$. On a $\Tr(A)=\lambda+\mu=0$ donc $\mu=-\lambda$ et $\Tr(A^{2})=\lambda^{2}+\mu^{2}=2\sum_{i=1}^{n}a_{i}^{2}$ donc 
	\begin{equation}
		\left\lbrace\lambda,\mu\right\rbrace=\left\lbrace\pm\sqrt{\sum_{i=1}^{n}a_{i}^{2}}\right\rbrace
	\end{equation}
	Les deux valeurs propres sont de multiplicité 1 dans $\chi_{A}$: les sous-espaces propres sont de dimension 1.

	$A$ est diagonalisable sur $\R$, semblable à $\diag(0,\dots,0,\sqrt{\sum_{i=1}^{n}a_{i}^{2}},-\sqrt{\sum_{i=1}^{n}a_{i}^{2}})$.

	On a $AX=0$ si et seulement si 
	\begin{equation}
		\left\lbrace
			\begin{array}[]{lll}
				a_{1}x_{n+1}&=&0\\
				\vdots\\
				a_{n}x_{n+1}&=&0\\
				a_{1}x_{1}+\dots+a_{n}x_{n}&=&0
			\end{array}
		\right.
	\end{equation}
	si et seulement si 
	\begin{equation}
		\left\lbrace
			\begin{array}[]{lll}
				x_{n+1}&=&0\\
				x_{i_{0}}&=&\sum_{\substack{i=1\\ i\neq i_{0}}}a_{i}x_{i}
			\end{array}
		\right.
	\end{equation}
	avec $i_{0}$ indice tel que $a_{i_{0}}\neq0$. Une base de $\ker(u)$ est donc 
	\begin{equation}
		f_{i}=\begin{pmatrix}
			0\\
			\vdots\\
			0\\
			1\\
			0\\
			\dots\\
			0\\
			-\frac{a_{i}}{a_{i_{0}}}\\
			0\\
			\vdots\\
			0
		\end{pmatrix}
	\end{equation}
	où $i\in\llbracket1,n\rrbracket\setminus\lbrace i_{0}\rbrace$. Le 1 est à la $i$-ième ligne, $-\frac{a_{i}}{a_{i_{0}}}$ est à la ligne $i_{0}$.

	On a $AX=\sqrt{\sum_{i=1}^{n}a_{i}^{2}}X=\alpha X$ si et seulement si 
	\begin{equation}
		\left\lbrace
			\begin{array}[]{lll}
				a_{1}x_{n+1}&=&\alpha x_{1}\\
				\vdots\\
				a_{n}x_{n+1}&=&\alpha x_{n}\\
				\sum_{i=1}^{n}a_{i}x_{i}&=&\alpha x_{n+1}
			\end{array}
		\right.
	\end{equation}
	si et seulement si
	\begin{equation}
		\left\lbrace
			\begin{array}[]{lll}
				x_{1}&=&\frac{a_{1}}{\alpha}x_{n+1}\\
				\vdots\\
				x_{n}&=&\frac{a_{n}}{\alpha}x_{n+1}\\
			\end{array}
		\right.
	\end{equation}
	Une base de $ker(u-\alpha id)$ est donc 
	\begin{equation}
		\begin{pmatrix}
			\frac{a_{1}}{\alpha}\\
			\dots\\
			\frac{a_{n}}{\alpha}\\
			1
		\end{pmatrix}
	\end{equation}

	De même pour $-\alpha$, on vérifie qu'une base de $\ker(u+\alpha id)$ est 
	\begin{equation}
		\begin{pmatrix}
			-\frac{a_{1}}{\alpha}\\
			\dots\\
			-\frac{a_{n}}{\alpha}\\
			1
		\end{pmatrix}
	\end{equation}
\end{proof}

\begin{proof}
	Pour le sens direct, $f$ et $g$ ont les mêmes espaces propres distincts. Pour tout $i\in\llbracket1,r\rrbracket$, $E_{i}=\ker(f-\lambda_{i}id)=\ker(g-\mu_{i}id)$ avec $\lambda_{1},\dots,\lambda_{r}$ valeurs propres distinctes deux à deux de $f$ et $\mu_{1},\dots,\mu_{r}$ pour $g$.

	On pose 
	\begin{equation}
		\begin{array}[]{lll}
			Q&=&\sum_{i=1}^{r}\mu_{i}\prod_{\substack{j=1\\ j\neq i}}^{r}\frac{X-\lambda_{j}}{\lambda_{i}-\lambda_{j}}\\
			P&=&\sum_{i=1}^{r}\lambda_{i}\prod_{\substack{j=1\\ j\neq i}}^{r}\frac{X-\mu_{j}}{\mu_{i}-\mu_{j}}
		\end{array}
	\end{equation}

	Soit $i\in\llbracket1,r\rrbracket$ et $x\in E_{i}$. On a $Q(f)(x)=Q(\lambda_{i})x=\mu_{i}x=g(x)$. $Q(f)$ et $g$ coïncident sur les vecteurs d'une base, donc ils sont égaux, donc $Q(f)=g$. De même, $f=P(g)$.

	Réciproquement, s'il existe $(P,Q)\in\K_{n-1}[X]^{2}$ tel que $f=P(g)$ et $g=Q(f)$. On prend $\lambda\in\Sp(f)$, soit $x\in\ker(f-\lambda id)$. On a $g(x)=Q(f)(x)=Q(\lambda)x$ donc $x\in\ker(g-Q(\lambda)id)$. On a 
	\begin{equation}
		\ker(f-\lambda id)\subset\ker(g-Q(\lambda)id)\subset\ker(f-P(Q(\lambda))id)
	\end{equation}
	Or $P(Q(\lambda))=\lambda$ car pour $x\in\ker(f-\lambda id)\setminus\lbrace0\rbrace$, on a $\lambda x=P(Q(\lambda))x$. Donc $\ker(f-\lambda id)=\ker(g-Q(\lambda)id)$ donc $f$ et $g$ ont les mêmes sous-espaces propres.
\end{proof}

\begin{remark}
	C'est faux si $f$ et $g$ ne sont pas diagonalisables, par exemple 
	\begin{equation}
		A=\begin{pmatrix}
			0&1&0&0\\
			0&0&0&0\\
			0&0&0&0\\
			0&0&1&0
		\end{pmatrix},~~~B=\begin{pmatrix}
			0&1&0&0\\
			0&0&1&0\\
			0&0&0&0\\
			0&0&0&0
		\end{pmatrix}
	\end{equation}
	$A$ et $B$ ont les mêmes sous-espaces propres  (un seul: $\Vect(e_{1},e_{4})$). On a $A^{2}=0$ donc pour tout $P\in\C[X]$, il existe $(\alpha,\beta)\in\C^{2}$,
	\begin{equation}
		P(A)=\alpha I_{4}+\beta A=\begin{pmatrix}
			\alpha&\beta&0&0\\
			0&\alpha&0&0\\
			0&0&\alpha&0\\
			0&0&\beta&\alpha
		\end{pmatrix}\neq B
	\end{equation}
\end{remark}

\begin{proof}
	Soit $m=\left\lvert G\right\rvert$. On a pour tout $M\in G$, on a $M^{m}=I_{2}$ donc $M$ est diagonalisable sur $\C$ et $\Sp_{\C}(M)\subset\U_{m}$. Notons que $G$ étant abélien, toutes les matrices sont co-diagonalisables.

	Si $\Sp_{\C}(M)=\lbrace1\rbrace$, alors $M=I_{2}$. Si $\Sp_{\C}(M)=\lbrace-1\rbrace$, alors $M=-I_{2}$. Dans ces deux cas, on a $\det(M)=1$ et $\Tr(M)=\pm2$. On note ce cas 1.
	
	Si $\Sp_{\C}(M)=\lbrace-1,1\rbrace$, $M$ est semblable à $\begin{pmatrix}
		-1&0\\
		0&1
	\end{pmatrix}$ et $M^{2}=I_{2}$. Dans ce cas, on a $\det(M)=-1$ et $\Tr(M)=0$. On note ce cas 2.

	Notons que l'on a $\chi_{M}=X^{2}-\Tr(M)X+\det(M)$ et $\Delta=(\Tr(M))^{2}-4\det(M)$. Comme $\chi_{M}$ est un polynôme réel, si $\delta<0$, on écrit $\chi_{M}(X-\e^{\i\theta})(X-\e^{-\i\theta})$. Comme $\Tr(M)=2\cos(\theta)\in\Z$, on a $\theta\in\left\lbrace\frac{2\pi}{3},\frac{\pi}{2},\frac{\pi}{3}\right\rbrace$.

	Si $\theta=\frac{2\pi}{3}$, $M$ est semblable à 
	\begin{equation}
		\begin{pmatrix}
			\e^{\i\frac{2\pi}{3}}&0\\
			0&\e^{-\i\frac{2\pi}{3}}
		\end{pmatrix}
	\end{equation}
	et $M^{3}=I_{2}$. On a $\det(M)=1$ et $\Tr(M)=-1$. On note ce cas 3.

	Si $\theta=\frac{\pi}{2}$, $M$ est semblable à 
	\begin{equation}
		\begin{pmatrix}
			\e^{\i\frac{\pi}{2}}&0\\
			0&\e^{-\i\frac{\pi}{2}}
		\end{pmatrix}
	\end{equation}
	et $M^{4}=I_{2}$. On a $\det(M)=1$ et $\Tr(M)=0$. On note ce cas 4.

	Si $\theta=\frac{\pi}{3}$, $M$ est semblable à 
	\begin{equation}
		\begin{pmatrix}
			\e^{\i\frac{\pi}{3}}&0\\
			0&\e^{-\i\frac{\pi}{3}}
		\end{pmatrix}
	\end{equation}
	et $M^{6}=I_{2}$. On a $\det(M)=1$ et $\Tr(M)=1$. On note ce cas 5.

	Par ailleurs, il existe $P\in GL_{2}(\C)$ telle que pour tout $M\in G$, $P^{-1}MP=I_{2}$.

	S'il existe $M\in G$ du type 2, alors les types 3 et 5 sont exclus car on obtiendrait par produit $\begin{pmatrix}
		\e^{\i\theta}&0\\0&-\e^{\i\theta}
	\end{pmatrix}$ avec $\Tr(M)=0$ car $\chi_{m}$ est un polynôme réel, et $\theta\in\left\lbrace\frac{\pi}{3},\frac{2\pi}{3}\right\rbrace$, ce qui n'est pas.

	Ainsi, 
	\begin{equation}
		P^{-1}GP\subset\left\lbrace I_{2},-I_{2},\begin{pmatrix}
			-1&0\\0&1
		\end{pmatrix},\begin{pmatrix}
			1&0\\0&-1
		\end{pmatrix},\begin{pmatrix}
			\i&0\\0&-\i
		\end{pmatrix},\begin{pmatrix}
			-\i&0\\0&\i
		\end{pmatrix}\right\rbrace
	\end{equation}

	Ainsi, 
	\begin{equation}
		G=
		\left\lbrace
			\begin{array}[]{ll}
				\left\lbrace I_{2}\right\rbrace\\
				\left\lbrace -I_{2},I_{2}\right\rbrace\\
				\left\lbrace I_{2},B\right\rbrace& B\text{ matrice de type 2}\\
				\left\lbrace I_{2}, A,A^{2},A^{3}\right\rbrace\cong\Z/4\Z& A\text{ matrice de type 4}\\
				\left\lbrace I_{2},A,B,A^{2},A^{3},-B\right\rbrace\cong \Z/3\Z\times\Z/2\Z& A,B\text{ matrices de type 4, 2}\\
				\left\lbrace I_{2},B,-B,-I_{2}\right\rbrace\cong\left(\Z/2\Z\right)^{2} & B\text{ matrice de type 2}
			\end{array}
		\right.
	\end{equation}

	S'il n'y a pas de matrice de type 2, on a 
	\begin{equation}
		P^{-1}GP\subset\left\lbrace I_{2},-I_{2},\begin{pmatrix}
			\j&0\\0&\j^{2}
		\end{pmatrix},\begin{pmatrix}
			\i&0\\0&-\i
		\end{pmatrix},\begin{pmatrix}
			-\i&0\\0&\i
		\end{pmatrix},\begin{pmatrix}
			-\j&0\\0&-\j^{2}
		\end{pmatrix},\begin{pmatrix}
			-\j^{2}&0\\0&-\j
		\end{pmatrix}\right\rbrace
	\end{equation}

	On ne peut pas avoir une matrice de type 3 ou 5 car $\pm\i\j$ et $\pm\i\j^{2}$ ne sont pas des valeurs propres possibles. Donc 
	\begin{equation}
		G=
		\left\lbrace
		\begin{array}[]{ll}
			\left\lbrace I_{2},C,C^{2}\right\rbrace\cong\Z/3\Z & C\text{ matrice de type 3}\\
			\left\lbrace I_{2},D,D^{2},D^{3},D^{4},D^{5}\right\rbrace\cong\Z/6\Z& D\text{ matrice de type 5}
		\end{array}
		\right.
	\end{equation}
	Notons que dans le deuxième cas, $D^{2}$ est de type 3.

	On a donc bien $\left\lvert G\right\rvert\in\left\lbrace1,2,3,4,6\right\rbrace$.
\end{proof}

\begin{proof}
	Si $u$ est diagonalisable, la famille des vecteurs propres est génératrice. En prenant un sous-espace de $E$ de base $\mathcal{B}$, on complète avec des vecteurs propres, ce qui forme un sous-espace stable par u.

	Réciproquement, soit 
	\begin{equation}
		F=\bigotimes_{\lambda\in\Sp(u)}\ker(u-\lambda id)
	\end{equation}
	stable par $u$. $F$ admet un supplémentaire stable qu'on nommera $G$. Si $G\neq\lbrace0\rbrace$, $u_{\mid G}$ admet nécessairement un vecteur propre, or les vecteurs propres sont dans $F\setminus\lbrace0\rbrace$; absurde. Donc $G=\left\lbrace0\right\rbrace$ et $E=F$ donc $u$ est diagonalisable.
\end{proof}

\begin{proof}
	Plus généralement, soit $A=(a_{i,j})\in\M_{n}(\K)$ et $B=(b_{k,l})\in\M_{p}(\K)$. On définit 
	\begin{equation}
		M=B\otimes A=
		\begin{pmatrix}
			b_{1,1}A&\dots&b_{1,p}A\\
			\vdots&&\vdots\\
			b_{p,1}A&\dots&b_{p,p}A
		\end{pmatrix}
	\end{equation}

	Si $B$ est diagonalisable, il existe $Q\in GL_{p}(\K)$ tel que $Q^{-1}BQ=\diag(\mu_{1},dots,\mu_{p})$. On note $Q=(q_{i,j})$ et $Q^{-1}=(q'_{i,j})$. Par produits par blocs, on a 
	\begin{equation}
		\underbrace{\begin{pmatrix}
			q'_{1,1}I_{n}&\dots&q'_{1,p}I_{n}\\
			\vdots&&\vdots\\
			q'_{p,1}I_{n}&\dots&q'_{p,p}I_{n}
		\end{pmatrix}}_{=Q^{-1}\otimes I_{n}=(Q\otimes I_{n})^{-1}}M
		\underbrace{\begin{pmatrix}
			q_{1,1}I_{n}&\dots&q_{1,p}I_{n}\\
			\vdots&&\vdots\\
			q_{p,1}I_{n}&\dots&q_{p,p}I_{n}
		\end{pmatrix}}_{Q\otimes I_{n}}
		=\diag(\mu_{1}A,\dots,\mu_{p}A)=M_{1}
	\end{equation}

	Si de plus $A$ est diagonalisable, il existe $P\in GL_{n}(\K)$ tel que $P^{-1}AP=\diag(\lambda_{1},\dots,\lambda_{n})$. Alors 
	\begin{equation}
		\diag(P^{-1},\dots,P^{-1})M_{1}\diag(P,\dots,P)=\diag(\mu_{1},\lambda_{1},\dots,\mu_{1},\lambda_{n},\dots,\mu_{p}\lambda_{1},\dots,\mu_{p}\lambda_{n})
	\end{equation}

	Donc $B\otimes A$ est diagonalisable et $\Sp(B\otimes A)=\left\lbrace \lambda_{i}\mu_{j}\middle| 1\leqslant i\leqslant n,1\leqslant j\leqslant p\right\rbrace$.
\end{proof}

\begin{remark}
	On a $\Tr(B\otimes A)=\Tr(B)\times\Tr(A)$ et $\det(B\otimes A)=\det(B)^{n}\det(A)^{p}$.
\end{remark}
\begin{remark}
	Si $B$ est diagonalisable et non nulle, si $B\otimes A$ est diagonalisable, il existe $i\in\llbracket1,p\rrbracket$ tel que $\mu_{i}\neq0$ et $\mu_{i}A$ est diagonalisable (restriction à un sous-espace stable d'un endomorphisme diagonalisable) donc $A$ est diagonalisable.
\end{remark}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item On note $(e_{1},\dots,e_{n})$ la base canonique de $\C^{n}$ et $u\in\L(\C^{n})$ canoniquement associé à $A$. 
		
		Si $n=2m$ avec $m\in\N^{*}$, on pose pour $k\in\llbracket1,m\rrbracket$, $F_{k}=\Vect(e_{k},\dots,e_{n-k+1})$. On a $u(e_{k})=x_{k}e_{n-k+1}$ et $u(e_{n-k+1})=x_{n-k+1}e_{k}$ donc $F_{k}$ est stable par $u$. Ainsi, $\mat_{(e_{k},e_{n-k+1})}(u_{\mid F_{k}})=\begin{pmatrix}
			0&x_{n-k+1}\\ x_{k}&0
		\end{pmatrix}$. 

		Étudions, pour tout $(a,b)\in\C^{2}$, $M=\begin{pmatrix}
			0&a\\b&0
		\end{pmatrix}$. On a $\chi_{M}=X^{2}-ab$. 
		
		Si $ab\neq0$, on note $\lambda$ une racine carrée de $ab$ sur $\C$. On a $\chi_{M}(X-\lambda)(X+\lambda)$ qui est scindé à racines simples donc $M$ est diagonalisable et semblable à $\diag(\lambda,-\lambda)$.

		Si $ab=0$: si $a=b=0$ alors $M=0$. Si $a$ ou $b\neq0$, alors $M$ n'est pas diagonalisable puisque si elle l'était, comme sa seule valeur propre est 0, elle serait semblable donc égale à 0 ce qui n'est pas. Donc $A$ est diagonalisable si et seulement si pour tout $k\in\llbracket1,m\rrbracket$, $x_{k}$ et $x_{n-k+1}$ non nuls ou $x_{k}=0=x_{n-k+1}$.

		Si $n=2m+1$, on fait le même raisonnement avec $u(e_{m})=x_{m}e_{m}$.

		\item $(A^{p})_{p\in\N}$ converge si et seulement si pour tout $k\in\llbracket1,m\rrbracket$, $\left(u_{\mid F_{k}}^{p}\right)_{p\in\N}$ converge. 
		
		Soit $k\in\llbracket1,m\rrbracket$. Si $x_{k}x_{n+1-k}\neq0$, soit $\lambda\in\C$ tel que $\lambda^{2}=x_{k}x_{n+1-k}$. Dans une base de vecteurs propres $\mathcal{B}$, on a 
		\begin{equation}
			\mat_{\mathcal{B}}(u_{\mid F_{k}}^{p})=\begin{pmatrix}
				\lambda^{p}&0\\0&(-\lambda)^{p}
			\end{pmatrix}
		\end{equation}
		Cela converge si et seulement si $\lvert\lambda\rvert<1$ si et seulement si $\left\lvert x_{k}x_{n+1-k}\right\rvert<1$.

		Si $x_{k}=x_{n+1-k}=0$, alors pour tout $p\geqslant2$, $u_{\mid F_{k}}^{p}=0$ donc on a convergence.
	\end{enumerate}
\end{proof}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item On a 
		\begin{equation}
			\mat_{(1,X,\dots,X^{n})}(\varphi)=\diag(-n,1-n,\dots,0)
		\end{equation}
		donc les valeurs propres sont $(k-n)_{k\in\llbracket0,n\rrbracket}$. On a $n+1$ valeurs propres distinctes donc $\varphi$ est diagonalisable et les sous-espaces propres sont de dimension 1. Les vecteurs propres sont les $(X^{k})$ pour $k\in\llbracket1,n\rrbracket$.

		\item Si $\varphi(P)=kP$, alors $\deg(P)=k$. Si $P=\sum_{i=0}^{k}a_{i}X^{i}$ donc 
		\begin{equation}
			XP'-nP''-kP=0=\sum_{i=0}^{k-2}\left(ia_{i}-n(i+1)(i+2)-ka_{i}\right)X^{i}-a_{k-1}X^{k-1}
		\end{equation}

		Ainsi, par récurrence, pour tout $p\in\left\lbrace0,\dot,\left\lfloor\frac{k-1}{2}\right\rfloor\right\rbrace$, $a_{k-(2p+1)}=0$ et pour tout $p\in\left\lbrace0,\dots,\left\lfloor\frac{k}{2}\right\rfloor\right\rbrace$,
		\begin{equation}
			a_{k-2p}=\frac{n^{p}(k-2p+1)\dots(k-1)k}{(-2p)\dots(-4)(-2)}a_{k}=\frac{n^{p}(-1)^{p}}{2^{p}p!}\times \frac{k!}{(k-2p)!}a_{k}
		\end{equation}
		donc les vecteurs propres correspondent aux polynômes ayant ces coefficients.
	\end{enumerate}
\end{proof}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item On a 
		\begin{equation}
			A=aI_{3}+c\underbrace{\begin{pmatrix}
				0&0&1\\1&0&0\\0&1&0
			\end{pmatrix}}_{C}+b\underbrace{\begin{pmatrix}
				0&1&0\\0&0&1\\1&0&0
			\end{pmatrix}}_{B}
		\end{equation}
		avec $B=C^{2}$ et $C$ est la matrice compagnon du polynôme $X^{3}-1$, donc $\chi_{C}=X^{3}-1$. Ainsi, $C$ est diagonalisable sur $\C$ et $\Sp_{\C}(C)=\left\lbrace1,\j,\j^{2}\right\rbrace$. On a 
		\begin{equation}
			\begin{array}[]{l}
				C\begin{pmatrix}
					1\\1\\1
				\end{pmatrix}=\begin{pmatrix}
					1\\1\\1
				\end{pmatrix}\\
				C\begin{pmatrix}
					1\\\j^{2}\\\j
				\end{pmatrix}=\j\begin{pmatrix}
					1\\\j^{2}\\\j
				\end{pmatrix}\\
				C\begin{pmatrix}
					1\\\j\\\j^{2}
				\end{pmatrix}=\j^{2}\begin{pmatrix}
					1\\\j\\\j^{2}
				\end{pmatrix}\\
			\end{array}
		\end{equation}
		donc si 
		\begin{equation}
			P=\begin{pmatrix}
				1&1&1\\ 1&\j^{2}&\j\\1&\j&\j^{2}
			\end{pmatrix}
		\end{equation}
		alors on a 
		\begin{equation}
			P^{-1}AP=\begin{pmatrix}
				1&0&0\\
				0&a+b\j^{2}+c\j&0\\
				0&0&a+b\j+c\j^{2}
			\end{pmatrix}
		\end{equation}
		et 
		\begin{equation}
			\boxed{
				\Sp_{\C}(A)=\left\lbrace1,a+c\j+b\j^{2},a+b\j+c\j^{2}\right\rbrace
			}
		\end{equation}

		\item On a 
		\begin{equation}
			A^{n}=P\begin{pmatrix}
				1&0&0\\
				0&(a+b\j^{2}+c\j)^{n}&0\\
				0&0&(a+b\j+c\j^{2})^{n}
			\end{pmatrix}P^{-1}
		\end{equation}

		Tout d'abord, on a $\left\lvert a+c\j+b\j^{2}\right\rvert\leqslant\left\lvert a\right\rvert+\left\lvert b\right\rvert+\left\lvert c\right\rvert=1$ et on a égalité si et seulement si $a,c\j$ et $b\j^{2}$ ont le même argument si et seulement si 
		\begin{equation*}
			\left\lbrace a,b,c\right\rbrace\in\left\lbrace\left\lbrace1,0,0\right\rbrace,\left\lbrace0,1,0\right\rbrace,\left\lbrace0,0,1\right\rbrace\right\rbrace.
		\end{equation*}
		Si $a=1$ et $b=c=0$, la suite $(A^{n})_{n\in\N}$ converge.
		Si $(b=1$ et $a=c=0$) ou ($c=1$ et $a=b=0$), $(A^{n})_{n\in\N}$ diverge.
		Sinon,
		\begin{equation}
			\boxed{
			A^{n}\xrightarrow[n\to+\infty]{}P\begin{pmatrix}
				1&0&0\\0&0&0\\0&0&0
			\end{pmatrix}P^{-1}}
		\end{equation}
	\end{enumerate}
\end{proof}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item On vérifie en calculant les premiers termes puis par récurrence que 
		\begin{equation}
			\boxed{A^{k}=\begin{pmatrix}
				B^{k}&\sum_{i=1}^{k}B^{i-1}CD^{k-i}\\0&D^{k}
			\end{pmatrix}}
		\end{equation}

		\item On a 
		\begin{equation}
			\mu_{A}(A)=0=\begin{pmatrix}
				\mu_{A}(B)&\star\\0&\mu_{A}(D)
			\end{pmatrix}
		\end{equation}
		donc $\mu_{A}(B)=\mu_{A}(D)=0$. Ainsi, $\mu_{B}\mid \mu_{A}$ et $\mu_{D}\mid\mu_{A}$ donc $\mu_{B}\vee\mu_{D}\mid \mu_{A}$.

		Si $B$ et $D$ sont de tailles 1, $\chi_{A}(X)=(X-b)(X-d)$ d'où $\mu_{B}=X-b$ et $\mu_{D}=X-d$.

		Si $b\neq d$, $X-b$ et $X-d$ divise $\mu_{A}$ donc $\mu_{A}\mid \chi_{A}$ d'après le théorème de Cayley-Hamilton.

		Si $b=d$, si $c=0$ on a $\mu_{A}=X-b$ donc $\mu_{A}\mid\mu_{B}\mu_{D}=(X-b)^{2}$. Si $c\neq0$, $\mu_{A}=X-b$ ou $\mu_{A}=(X-b)^{2}$ et $A-bI_{n}\neq0$ donc $\mu_{A}=(X-b)^{2}$.

		\item Si $C=0$, on a $\mu_{A}=\mu_{B}\vee\mu_{D}$.
		\item Si $B=D$ et $C=I_{n_{1}}$, on a $A^{0}=I_{n_{1}+n_{2}}$ et pour $k\geqslant1$,
		\begin{equation}
			A^{k}=\begin{pmatrix}
				B^{k}&kB^{k-1}\\
				0&B^{k}
			\end{pmatrix}
		\end{equation}
		Ainsi
		\begin{equation}
			P(A)=\begin{pmatrix}
				P(B)&P'(B)\\
				0&P(B)
			\end{pmatrix}=0
		\end{equation}
		si et seulement si $P(B)=P'(B)=0$ donc $\mu_{B}\mid P$ et $\mu_{B}\mid P'$ donc $\mu_{B}\mid P\vee P'$.

		On a 
		\begin{equation}
			\mu_{B}^{2}(A)=\begin{pmatrix}
				0&2\mu_{B}\mu_{B}'(B)\\
				0&0
			\end{pmatrix}=0
		\end{equation}
		donc $\mu_{A}\mid\mu_{B}^{2}$.

		On décompose $\mu_{B}=(X-\lambda_{1})^{m_{1}}\times\dots\times(X-\lambda_{r})^{m_{r}}$. On a $P(A)=0$ si et seulement si pour tout $i\in\left\lbrace1,\dots,r\right\rbrace$, $\lambda_{i}$ est racine de $P$ d'ordre plus grand que $m_{i}+1$.

		\item On prend 
		\begin{equation}
			A=\begin{pmatrix}
				0&1&0&0\\
				0&0&0&1\\
				0&0&0&1\\
				0&0&0&0
			\end{pmatrix}
		\end{equation}
		On a 
		\begin{equation}
			B=D=\begin{pmatrix}
				0&1\\0&0
			\end{pmatrix}
		\end{equation}
		$\mu_{B}=\mu_{D}=X^{2}$, et $\mu_{A}\mid X^{3}$. Or $u^{2}(e_{4})\neq0$ donc $\mu_{A}=X^{3}\neq X^{2}=\mu_{B}\vee\mu_{D}\neq X^{4}=\mu_{B}\mu_{D}$.
	\end{enumerate}
\end{proof}

\begin{proof}
	On décompose sur $\C$: $P(X)-\lambda=\alpha\prod_{i=1}^{m}(X-\mu_{i})$ avec $\alpha\neq0$. On a 
	\begin{equation}
		\underbrace{g-\lambda id}_{\substack{\text{non inversible}\\\text{(respectivement non injectif)}}}=\alpha\prod_{i=1}^{m}(f-\mu_{i}id)
	\end{equation}
	donc il existe $i\in\llbracket1,m\rrbracket$, $f-\mu_{i}id$ est non inversible (respectivement non injectif), d'où le résultat.
\end{proof}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item Soit $(f_{1},\dots,f_{r})$ une base de $V$ et $x\in E\setminus\lbrace0\rbrace$. Soit $(\alpha_{1},\dots,\alpha_{r})\in\K^{r}$ tels que $\alpha_{1}f_{1}(x)+\dots+\alpha_{r}f_{r}(x)=0$. Or $V$ est un sous-espace, donc $\alpha_{1}f_{1}+\dots+\alpha_{r}f_{r}\in V$ et $\alpha_{1}f_{1}+\dots+\alpha_{r}f_{r}\notin GL(E)$ car $x\neq0$. D'où $\alpha_{1}f_{1}+\dots+\alpha_{r}f_{r}=0$ et donc $\alpha_{1}=\dots=\alpha_{r}=0$ car $(f_{1},\dots,f_{r})$ est une base de $V$.
		
		Ainsi $(f_{1}(x),\dots,f_{r}(x))$ est libre donc $r=\dim(V)\leqslant\dim(E)$.

		\item Si $\dim(V)=1$, alors $V=\C f$ avec $f\in GL(E)$. Si $\dim(V)\geqslant2$, soient $(f,g)\in V^{2}$ tels que $(f,g)$ soit libre alors pour tout $\alpha\in\C$, $f+\alpha g\neq0$ et $f+\alpha g\in V\setminus\lbrace0\rbrace$. Or $f+\alpha g=g(g^{-1}\circ f+\alpha id)$. Pour $\alpha\in \Sp(-g^{-1}\circ f)$ (existe car on est dans $\C$), on obtient une contradiction. Donc de même, $V=\C f$ avec $f\in GL(E)$.
		
		\item Comme $\dim(E)=2$, on a $\dim(V)\leqslant2$. Si $\dim(V)=1$, comme précédemment, on a $V=\R f$ avec $f\in GL(E)$. Si $\dim(V)=2$, soit $(f,g)$ une base de $V$. D'après ce qui précède, on a $\Sp_{\R}(g^{-1}\circ f)=\emptyset$. Soit $\mathcal{B}$ une base de $E$ et $A=\mat_{\mathcal{B}}(f)$ et $B=\mat_{\mathcal{B}}(g)$. On écrit $\chi_{A^{-1}B}=(X-\lambda)(X-\overline{\lambda})$ avec $\lambda=\alpha+\i \beta$, $\beta>0$. $A^{-1}B$ est diagonalisable sur $\C$ et semblable à $\diag(\lambda,\overline{\lambda})$ et $\frac{A^{-1}B-\alpha I_{2}}{\beta}$ est semblable sur $\C$ à $\diag(\i,-\i)$ semblable sur $\R$ à $\begin{pmatrix}
			0&-1\\1&0
		\end{pmatrix}$ donc $\frac{A^{-1}B-\alpha I_{2}}{\beta}$ est semblable sur $\R$ à $\begin{pmatrix}
			0&-1\\1&0
		\end{pmatrix}$. Il existe $P\in GL_{2}(\R)$ tel que 
		\begin{equation}
			A^{-1}B=P\begin{pmatrix}
				\alpha&-\beta\\\beta&\alpha
			\end{pmatrix}P^{-1}
		\end{equation}

		Pour tout $(\lambda,\mu)\in\R^{2}$, 
		\begin{equation}
			\lambda A+\mu B=A(\lambda I_{2}+\mu A^{-1}B)=\underbrace{AP}_{\in GL_{2}(\R)}\begin{pmatrix}
				\lambda+\alpha\mu&-\beta\\\beta&\lambda+\alpha\mu
			\end{pmatrix}\underbrace{P^{-1}}_{\in GL_{2}(\R)}
		\end{equation}
		avec $\beta>0$,$\alpha\in\R$.

		\item Avec les notations précédentes, si $(A,B)\in V^{2}$ est libre, on a 
		\begin{equation}
			\i\in\Sp\left(\frac{A^{-1}B-\alpha I_{2}}{\beta}\right)=\Sp\left((BA)^{-1}(B-\alpha A)\right)
		\end{equation}
		et on pose $A'=\beta A\in V$ et $B'=B-\alpha A\in V$.
	\end{enumerate}
\end{proof}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item En notant $c_{i,j}$ les cofacteurs d'indice $(j,i)$, on a 
		\begin{equation}
			\left[\com(\lambda I_{n}-A)^{\mathsf{T}}\right]_{i,j}=(-1)^{i+j}c_{j,i}(\lambda-I_{n})
		\end{equation}

		En développant, on obtient des polynômes en $\lambda$ de degré plus petit que $n-1$. En regroupant selon les puissances de $\lambda$, on a 
		\begin{equation}
			\boxed{\com(\lambda I_{n}-A)^{\mathsf{T}}=M_{0}+M_{1}\lambda+\dots+\lambda^{n-1}M_{n-1}}
		\end{equation}

		\item On a $(\lambda I_{n}-A)\com(\lambda I_{n}-A)^{\mathsf{T}}=\det(\lambda I_{n}-A)I_{n}=\chi_{A}(\lambda)I_{n}$
		
		En identifiant les coefficients, on a 
		\begin{equation}
			\left\lbrace
				\begin{array}[]{lll}
					M_{n-1}&=&I_{n}\\
					M_{n-2}&=&A+a_{n-1}I_{n}\\
					M_{n-3}&=&A^{2}+a_{n-1}A+a_{n-2}I_{n}\\
					\vdots\\
					M_{n-k}&=&A^{k-1}+a_{n-1}A^{k-2}+\dots+a_{n-k+1}I_{n}\\
					\vdots\\
					M_{0}&=&A^{n-1}+a_{n-1}A^{n-2}+\dots+a_{1}I_{n}
				\end{array}
			\right.
		\end{equation}
		et $-AM_{0}=a_{0}I_{n}$. En reportant, on a bien $\chi_{A}(A)=O_{\M_{n}(\K)}$: on a une preuve du théorème de Cayley-Hamilton.

		\item Soit $\lambda\in\K$. On forme $\lambda I_{n}-A=(c_{1}(\lambda),\dots,c_{n}(\lambda))$ avec 
		\begin{equation}
			c_{j}(\lambda)=\begin{pmatrix}
				-a_{1,j}\\\dots\\-a_{j-1,j}\\\lambda-a_{j,j}\\-a_{j+1,j}\\\dots\\-a_{n,j}
			\end{pmatrix}
		\end{equation}

		On a $\chi_{A}(\lambda)=\det(c_{1}(\lambda),\dots,c_{n}(\lambda))$. $\det$ étant une forme $n$-linéaire, on a 
		\begin{equation}
			\chi_{A}'(\lambda)=\sum_{k=1}^{n}\det(c_{1}(\lambda),\dots,c_{k-1}(\lambda),c_{k}'(\lambda),c_{k+1}(\lambda),\dots,c_{n}(\lambda))
		\end{equation}
		En développant le terme $k$ par rapport à la $k$-ième colonne, on trouve qu'il vaut $_{k,k}(\lambda I_{n}-A)$. Ainsi,
		\begin{equation}
			\boxed{\chi_{A}'(\lambda)=\Tr(\com(\lambda I_{n}-A)^{\mathsf{T}})}
		\end{equation}

		\item On a donc $a_{1}+2a_{2}\lambda+\dots+(n-1)a_{n-1}\lambda^{n-2}+n\lambda^{n-1}=\sum_{k=0}^{n-1}\lambda^{k}\Tr(M_{k})$ pour tout $\lambda\in\K$ (par linéarité de $\Tr$). Donc pour tout $k\in\llbracket0,n-2\rrbracket$, $\Tr(M_{k})$, $\Tr(M_{k})=(k+1)a_{k+1}$ (et $\Tr(M_{n-1})=\Tr(I_{n})=n$). On a $\Tr(M_{n-2})=(n-1)a_{n-1}=\Tr(A)+na_{n-1}$ donc $a_{n-1}=\Tr(A)$. Puis $\Tr(M_{n-3})=(n-2)a_{n-2}=\Tr(A^{2})+a_{n-1}\Tr(A)+a_{n-2}n$ donc $a_{n-2}=-\frac{\Tr(A^{2})}{2}+\frac{\Tr(A)^{2}}{2}$. De proche en proche, on a $a_{n-k}=f_{k}(\Tr(A),\dots,\Tr(A^{k}))$ avec $f_{k}$ indépendante de $A$.
		
		\item D'après ce qui précède, pour tout $k\in\llbracket0,n-1\rrbracket$, $a_{k}=b_{k}$ car $f$ est indépendante de $A$. Donc $\chi_{A}=\chi_{B}$.
	\end{enumerate}
\end{proof}

\begin{remark}
	Si $\Tr(A)=\Tr(A^{2})=\dots=\Tr(A^{n})=0$, alors $\chi_{A}=\chi_{0}=X^{n}$ et $A$ est nilpotente. On peut le vérifier à la main sur $\C$: si $(\lambda_{1},\dots,\lambda_{r})$ sont les valeurs propres non nulles distinctes de $A$ et $m_{i}$ la multiplicité de $\lambda_{i}$ dans $\chi_{A}$, alors on a le système
	\begin{equation}
		\left\lbrace
			\begin{array}[]{lllll}
				\Tr(A)&=&m_{1}\lambda_{1}+\dots+m_{r}\lambda_{r}&=&0\\
				\Tr(A^{2})&=&m_{1}\lambda_{1}^{2}+\dots+m_{r}\lambda_{r}^{2}&=&0\\
				\vdots\\
				\Tr(A^{r})&=&m_{1}\lambda_{1}^{r}+\dots+m_{r}\lambda_{r}^{r}&=&0
			\end{array}
		\right.
	\end{equation}
	donc 
	\begin{equation}
		\begin{pmatrix}
			\lambda_{1}&\dots&\lambda_{r}\\
			\vdots&&\vdots\\
			\lambda_{1}^{r}&\dots&\lambda_{r}^{r}
		\end{pmatrix}\begin{pmatrix}
			m_{1}\\\vdots\\m_{r}
		\end{pmatrix}=0
	\end{equation}
	et la matrice est inversible car les $\lambda_{i}$ sont distincts non nuls. Donc $m_{1}=\dots=m_{r}=0$ et $\Sp_{\C}(A)=\lbrace0\rbrace$ et $\chi_{A}=X^{n}$.
\end{remark}

\begin{proof}
	On définit, pour $A=(a_{i,j})\in\M_{n}(\Z)$, $\overline{A}=(\overline{a_{i,j}})\in\M_{n}\left(\Z/p\Z\right)$. Comme $p$ est premier, $\Z/p\Z$ est un corps. On a $\chi_{A}\in\Z/p\Z[X]$ donc il existe $\mathbb{L}$ un sur-corps de $\Z/p\Z$ où $\chi_{A}$ est scindé sur $\mathbb{L}$. On écrit $\chi_{A}=(X-\lambda_{1})\dots(X-\lambda_{n})$ avec $\lambda_{1},\dots,\lambda_{n}\in\mathbb{L}$. On peut trigonaliser $\overline{A}$ sur $\mathbb{L}$ et on a $\Tr(\overline{A}^{p})=\sum_{i=1}^{n}\lambda_{i}^{p}$. Or la caractéristique de $\mathbb{L}$ vaut $p$ donc on a $(x+y)^{p}=x^{p}+y^{p}$ (binôme de Newton et utiliser le fait que $p\mid\binom{p}{k}$ pour $k\in\llbracket1,p-1\rrbracket$). Ainsi,
	\begin{equation}
		\Tr(\overline{A}^{p})=\left(\sum_{i=1}^{n}\lambda_{i}\right)^{p}=\Tr(\overline{A})^{p}
	\end{equation}
	et on peut appliquer le petit théorème de Fermat: on a bien $\Tr(\overline{A}^{p})=\Tr(\overline{A})$ et en remontant dans $\Z$, 
	\begin{equation}
		\boxed{\Tr(A^{p})\equiv\Tr(A)[p]}
	\end{equation}
\end{proof}

\begin{proof}
	Si on a (i), soit $x$ un vecteur propre associé à $\rho(u)=\rho e^{\mathrm{i}\theta}$. On a $\Vert u(x)\Vert=\Vert\rho(u) x\Vert=\rho(u)\Vert x\Vert$ et comme $x\neq0$, on a $\rho(u)\leqslant\vertiii{\rho(u)}<1$ d'où (ii).

	Si (ii), on utilise la décomposition de Dunford $u=n+d$ avec $n$ nilpotent, $d$ diagonalisable et $dn=nd$. Soit $m=\dim(E)$. Pour tout $p\geqslant m$, on a 
	\begin{equation}u^{p}=\sum_{k=0}^{p}\binom{p}{k}n^{k}d^{p-k}=\sum_{k=0}^{m-1}\binom{p}{k}n^{k}\underbrace{d^{p-k}}_{\xrightarrow[p\to+\infty]{}0}\end{equation}
	En effet, on a $k\geqslant m-1$ fixé, il existe une base $\mathcal{B}$ de $E$ telle que 
	\begin{equation}\binom{p}{k}\mat\limits_{\mathcal{B}}(d^{p})=\binom{p}{k}\diag(\lambda_{1}^{p},\dots,\lambda_{m}^{p})\xrightarrow[p\to+\infty]{}0\end{equation}
	car $\vert\lambda_{i}\vert<1$ pour tout $i\in\{1,\dots,m\}$ et 
	\begin{equation}\binom{p}{k}\underset{p\to+\infty}{\sim}\frac{p^{k}}{k!}=\underset{p\to+\infty}{o}\Bigl(\frac{1}{\rho(u)^{p}}\Bigr)\end{equation}
	donc on a (iii).

	Si (iii), soit $x$ un vecteur propré associé à $\lambda\in\C$, on a $u^{p}\xrightarrow[p\to+\infty]{}0$ donc en particulier, $u^{p}(x)=\lambda^{p}\xrightarrow[p\to+\infty]{}0$, donc $\rho(u)^{p}\xrightarrow[p\to+\infty]{}0$ et $\rho(u)\geqslant0$ donc $\rho(u)<1$. Posons encore $u=d+n$ la décomposition de Dunford de $u$. Soit $\varepsilon>0$, il existe $\mathcal{B}_{0}=(e_{1},\dots,e_{n})$ base de $E$ dans laquelle les coefficients de $\mat\limits_{\mathcal{B}_{0}}(n)$ sont en module $\leqslant\varepsilon$. Définissons sur $E$ 
	\begin{equation}\Biggl\Vert\sum_{i=1}^{m}x_{i}e_{i}\Biggr\Vert_{\infty}=\max\limits_{1\leqslant i\leqslant m}\vert x_{i}\vert\end{equation}
	Soit $M=\mat\limits_{\mathcal{B}_{0}}(u)=(m_{i,j})_{1\leqslant i,j\leqslant m}$ triangulaire supérieure avec $m_{ii}=\lambda_{i}$ et pour tout $j\neq i$, $\vert m_{i,j}\vert<\varepsilon$. Soit donc $x=\sum_{i=1}^{m}x_{i}e_{i}\in\C^{m}$, on a 
	\begin{equation}
	\Vert Mx\Vert_{\infty}=\max\limits_{1\leqslant i\leqslant n}\underbrace{\Biggl\vert\sum_{j=1}^{m}m_{i,j}x_{j}\Biggr\vert}_{(\vert\lambda_{i}\vert+(m-1)\varepsilon)\Vert x\Vert_{\infty}}
	\end{equation}
	donc 
	\begin{equation}\vertiii{u}\leqslant\underbrace{\rho(u)}_{<1}+(m-1)\varepsilon\end{equation}
	et on choisit
	\begin{equation}\varepsilon<\frac{1-\rho(u)}{\underbrace{m-1}_{>0}}\end{equation}
	d'où $\vertiii{u}<1$ et donc on a (i) et finalement on a bien l'équivalence.
\end{proof}

\begin{remark}
	$u\mapsto\rho(u)$ n'est pas une norme car pour $u$ nilpotente non nulle, $\rho(u)=0$.
\end{remark}

\begin{proof}
	Supposons (i), soit $Y$ un vecteur propre de $A$ avec $AY=\lambda Y$ pour $\lambda\in\C$. Pour tout $k\in\N,BA^{k}Y=\lambda^{k}BY$ et il existe $k_{0}\in\N$ tel que $\lambda^{k_{0}}BY\neq0$ et $BY\neq0$ donc on a (ii).

	Si (ii), supposons qu'il existe $Y\in\C^{n}\setminus\{0\}$ tel que $\varphi=0$. On note 
	\begin{equation}\chi_{A}=\prod_{i=1}^{r}(X-\lambda_{i})^{m_{i}}\end{equation} avec les $\lambda_{i}$ distincts. Alors $Y=\sum_{i=1}^{r}Y_{i}$ où $Y_{i}\in\ker(A-\lambda_{i}I_{n})$. Il existe $i_{0}\in\{1,\dots,n\}$ tel que $Y_{i_{0}}\neq0$ car $Y\neq0$. On a alors, pour $t\in\R$,
	\begin{equation}B\exp(tA)Y=\sum_{i=1}^{r}B\exp(t\lambda_{i})Y_{i}=0\end{equation}
	Pour tout $k\in\{0,\dots,r-1\}$, on a $\varphi^{(k)}(t)=\sum_{i=1}^{r}B\lambda_{i}^{k}\exp(t\lambda_{i})Y_{i}=0$. Pour $t=0$ on a $\sum_{i=1}^{r}\lambda_{i}^{k}BY_{i}=0$ ce qui, pour $t=0$, donne le système 
	\begin{equation}
	\left\{
		\begin{array}[]{lll}
			BY_{1}+\dots+BY_{r} &= &0\\
			\lambda_{1}BY_{1}+\dots+\lambda_{r}BY_{r} &=& 0\\
			&\vdots&\\
			\lambda_{1}^{r-1}BY_{1}+\dots+\lambda_{r}^{r-1}BY_{r} &= &0
		\end{array}
	\right.
	\end{equation}
	Pour tout $P\in\C_{r-1}[X]$, on a donc $\sum_{i=1}^{r}P(\lambda_{i})BY_{i}=0$. Pour $i\in\{0,\dots,r-1\}$ et $P=\prod_{i\neq j}\frac{(X-\lambda_{j})}{\lambda_{i}-\lambda_{j}}$, on obtient pour tout $i\in\{1,\dots, r\}, BY_{i}=0$. En particulier, $BY_{i_{0}}=0$ et $Y_{i_{0}}$ est un vecteur propre de $A$ car non nul. C'est une contradiction. On a donc (iii).

	\item Soit $Y\in\C^{n}\setminus\{0\}$, supposons que pour tout $k\in\{0,\dots,n-1\}$, $BA^{k}Y=0$. Soit $k\geqslant n$, il existe $(Q_{k},R_{k})\in\C[X]\times\C_{n-1}[X]$ tel que 
	\begin{equation}X^{k}=Q_{k}\chi_{A}+R_{k}\end{equation}
	et le théorème de Cayley-Hamilton donne donc $A^{k}=R_{k}(A)$ d'où $BA^{k}Y=BR_{k}(A)Y=0$. Alors pour tout $t\in\R$,
	\begin{align}
		B\exp(tA)Y
		&=B\sum_{k=0}^{+\infty}\frac{t^{k}A^{k}}{k!}Y\\
		&=\sum_{k=0}^{+\infty}\frac{t^{k}(BA^{k}Y)}{k!}\\
		&=0
	\end{align}
	Par contraposée, on a bien ce qu'il faut, d'où l'équivalence.
\end{proof}

\begin{proof}
	Noter d'abord que le résultat ne dépend pas de la norme (équivalente).
	Si $A=\lambda I_n$ on vérifie que $\left\lVert A^{p}\right\rVert^{\frac{1}{p}}\xrightarrow[p\to+\infty]{}\left\lvert\lambda\right\rvert$. On choisit $\left\lVert A\right\rVert=\sup_{\left\lVert X\right\rVert_{\infty}=1}\left\lVert AX\right\rVert_{\infty}$ et on vérifie que si $A$ est diagonalisable avec $\left\lvert \lambda_{1}\right\rvert\leqslant\left\lvert\lambda_{2}\right\rvert$ ses valeurs propres (complexes), alors le résultat est $\left\lvert \lambda_{2}\right\rvert$. Si $A$ est juste trigonalisable (son spectre contient juste $\lambda\in\C$), on écrit $A$ comme somme d'une matrice scalaire et d'une matrice nilpotente, et on calcule explicitement $\left\lVert A^p\right\rVert$. Le résultat est alors $\left\lvert\lambda\right\rvert$, et de manière générale, il s'agit du rayon spectral. En dimension quelconque, on utilise la décomposition de Dunford.
\end{proof}

\begin{proof}\phantom{}
	\begin{enumerate}
		\item $M$ est la matrice compagnon de $P$, donc $\chi_{M}=P$. Si $\lambda\in\Sp_{\K}(M)$, $\rg(M-\lambda I_n)=n-1$ car les $n-1$ premières lignes sont indépendantes. Ainsi, $\dim(E_{\lambda})=1$. Pour la condition nécessaire et suffisante, $M$ est diagonalisable et si et seulement si $P$ est scindé à racines simples.
		\item On pose $Y=\begin{pmatrix}
			y\\y'\\y''
		\end{pmatrix}$ et 
		\begin{equation*}
			A=\begin{pmatrix}
				0&1&0\\
				0&0&1\\
				-1&1&1
			\end{pmatrix}.
		\end{equation*}

		La solution générale est alors $Y(t)=\e^{t A}Y_{0}$. $\chi_{A}=(X-1)^{2}(X+1)$, donc $A$ n'est pas diagonalisable sur $\R$ mais trigonalisable. On trigonalise, par exemple
		\begin{equation*}
			\begin{pmatrix}
				1&1&-1\\-1&1&0\\1&1&1
			\end{pmatrix},
		\end{equation*}
		et on a $P^{-1}AP=A_1=\begin{pmatrix}
			-1&0&0\\0&1&1\\0&0&1
		\end{pmatrix}$. On peut alors calculer $\exp(tA_{1})$.
	\end{enumerate}
\end{proof}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item On utilise le fait que $\mathcal{B}$ est une base.
		\item On calcule le polynôme caractéristique qui vaut $P$. Comme $\mathcal{B}$ est une base, $P$ est nécessairement le polynôme minimal de $f$. Comme $X^{n}-1$ annule $f$, $P\mid X^{n}-1$. Comme $X^{n}-1$ est scindé à racines simples sur $\C$, $f$ est diagonalisable sur $\C$, et les valeurs propres réelles de $f$ sont dans $\left\lbrace-1,1\right\rbrace$. Le polynôme minimal de $f$ est de degré 3, donc il n'est pas scindé à racines simples sur $\R$. Donc $f$ n'est pas diagonalisable sur $\R$.
		\item $f^{n}=\mathrm{id}_{\R^{3}}$ donc $f$ est inversible. Comme $\chi_{f}=P=X^{3}-cX^{2}-bX-a$, $\det(f)=a\neq0$ donc $f^{-1}=\frac{1}{a}(f^{2}-cf-b\mathrm{id}_{\R^{3}})$.
		\item $\deg(\chi_{f})=3$ donc d'après le théorème des valeurs intermédiaires, il existe $\lambda\in\R$ telle que $\chi_{f}(\lambda)=0$. S'il existe $\mu\in\R$ avec $\mu\neq\lambda$ tel que $\chi_{f}(\mu)=0$, alors la troisième racine est réelle (car si elle était complexe, on aurait également son conjugué). Mais Le spectre de $f$ sur $\R$ est contenu dans $\left\lbrace-1,1\right\rbrace$, c'est absurde. Donc $\Sp_{\R}(f)=\left\lbrace\lambda\right\rbrace$.
		\item Les racines de $\chi_{f}$ sur $\C$ sont $\left\lbrace\lambda,\e^{\i\theta},\e^{-\i\theta}\right\rbrace$ avec $\theta\in]0,\pi[$ et $\e^{\i\theta}\in\U_{n}$ (racine de $X^{n}-1$). Donc $\det(f)=\lambda>0$ (respectivement $<0$). Donc $\lambda=1$ (respectivement -1). Si $\det(f)>0$, on a 
		\begin{equation*}
			\chi_{f}=(X-1)(X^{2}-2\cos\theta X+1)=X^{3}-cX^{2}-bX-a,
		\end{equation*}
		d'où $a=1$, $b=-(1+2\cos\theta)$, $c=(1+2\cos\theta)$.
	\end{enumerate}
\end{proof}

\begin{remark}
	Pour $n\geqslant3$, $r=\begin{pmatrix}
		1&\\&R_{\frac{2\pi}{n}}
	\end{pmatrix}$ vérifie l'hypothèse.
\end{remark}

\begin{proof}
	\phantom{}
	\begin{enumerate}
		\item $J$ est symétrique réelle donc diagonalisable sur $\R$ et les sous-espaces propres sont deux à deux orthogonaux. $\rg(J)=1$ donc $\dim(\ker(J))=n-1$. Ainsi, $0$ est valeurs propre d'ordre $n-1$, puis $\Tr(J)=n$ donc $\Sp_{R}(J)=\left\lbrace0,n\right\rbrace$. On a $\ker(J-n\mathrm{I}_n)=\Vect(1,\dots,1)^{\mathsf{T}}$.
		\item $MJ=dJ=JM$ et $M^{2}=M+d\mathrm{I}_n$. Comme $\im J=\Vect(1,\dots,1)^{\mathsf{T}}$ et $M\begin{pmatrix}
			1\\\vdots\\1
		\end{pmatrix}=d\begin{pmatrix}
			1\\\vdots\\1
		\end{pmatrix}$, on a $\im J\subset\ker(M-d\mathrm{I}_n)$. Alors $\ker(M-d\mathrm{I}_n)\neq0$ et $d\in\Sp(M)$. $X^{2}-x-d$ annule $M$ donc $d^{2}=2d$. Ainsi, $d=0$ ou $d=2$.
	\end{enumerate}
\end{proof}
\end{document}
\documentclass[12pt]{article}
\usepackage{style/style_sol}

\begin{document}

\begin{titlepage}
	\centering
	\vspace*{\fill}
	\Huge \textit{\textbf{Solutions MP/MP$^*$\\ Calcul matriciel}}
	\vspace*{\fill}
\end{titlepage}

\begin{proof}
    Soit $(k,m)\in\left(\N^{*}\right)^{2}$, on a 
    \begin{align}
        \left[M\overline{M}\right]_{k,m}
        &=\sum_{j=1}^{n}\omega^{(k-1)(j-1)}\overline{\omega}^{(j-1)(m-1)}\\
        &=\sum_{j=0}^{n-1}\left[\omega^{k-1}\overline{\omega}^{m-1}\right]^{j}\\
        &=\sum_{j=0}^{n-1}\left[\omega^{k-m}\right]^{j}
    \end{align}

    Or $\omega^{k-m}=1$ si et seulement si $n\mid k-m$ si et seulement si $k=m$ car $\left\lvert k-m\right\rvert\in\llbracket0,n-1\rrbracket$. Si $k=m$, on a $[M\overline{M}]_{k,m}=n$ et si $k\neq m$, on a 
    \begin{equation}
        [M\overline{M}]_{k,m}=\frac{1-\left(\omega^{k-m}\right)^{n}}{1-\omega^{k-m}}=0
    \end{equation}
    Donc $M\overline{M}=nI_{n}$. Ainsi, $M\in GL_{n}(\C)$ et 
    \begin{equation}
        \boxed{M^{-1}=\frac{1}{n}\overline{M}}
    \end{equation}

    On a $\det(M\overline{M})=\det(M)\det(\overline{M})=n^{n}=\det(M)\overline{\det(M)}=\left\lvert\det(M)\right\rvert^{2}$ donc $\left\lvert\det(M)\right\rvert=n^{\frac{n}{2}}$.

    On calcul $M^{2}$. On a 
    \begin{equation}
        [M^{2}]_{k,m}=\sum_{j=1}^{n}\omega^{(k-1)(j-1)+(j-1)(m-1)}=\sum_{j=0}^{n-1}\left[\omega^{k+m-2}\right]^{j}
    \end{equation}

    On a $k+m-2\in\llbracket0,2n-2\rrbracket$ donc $n\mid k+m-2$ si et seulement si $k+m=n+2$ ou $k+m=2$ si et seulement si $m=n+2-k$ ou $k=m=1$. Donc 
    \begin{equation}
        M^{2}=
        \begin{pmatrix}
            n       & 0         & \dots     & \dots & 0\\
            0       &           &           &       & n\\
            \vdots  &           &           & n     & 0\\
            \vdots  &           & \iddots   &       & \vdots\\
            0       & n         &\dots      &\dots  &0
        \end{pmatrix}
    \end{equation}

    En développant par rapport à la première ligne (ou colonne), on a 
    \begin{equation}
        \det(M^{2})=n^{n}(-1)^{\frac{n(n+1)}{2}}
    \end{equation}

    donc 
    \begin{equation}
        \boxed{\det(M)=
        \left\lbrace
            \begin{array}[]{ll}
                \pm n^{\frac{n}{2}} &\text{si }\frac{n(n+1)}{2}\text{ est pair i.e. }
                \left\lbrace
                \begin{array}[]{l}
                    n\equiv 0[4]\\
                    \text{ou}\\
                    n\equiv 3[4]
                \end{array}
                \right.\\
                \pm \i n^{\frac{n}{2}} &\text{si }\frac{n(n+1)}{2}\text{ est impair i.e. }
                \left\lbrace
                \begin{array}[]{l}
                    n\equiv 1[4]\\
                    \text{ou}\\
                    n\equiv 2[4]
                \end{array}
                \right.\\
            \end{array}
        \right.
        }
    \end{equation}
\end{proof}

\begin{proof}
    \phantom{}
    \begin{enumerate}
        \item Si $A\geqslant0$, soit $X\geqslant0$, on a 
        \begin{equation}
            [AX]_{i}=\sum_{j=1}^{n}a_{i,j}x_{j}\geqslant 0
        \end{equation}
        donc $AX\geqslant0$.

        Réciproquement, soit $j\in\llbracket1,n\rrbracket$, on prend 
        \begin{equation}
            X_{j}=
            \begin{pmatrix}
                0\\
                \vdots\\
                0\\
                1\\
                0\\
                \vdots\\
                0    
            \end{pmatrix}
        \end{equation}
        où le 1 est en $j$-ième position. $X_{j}\geqslant0$ et 
        \begin{equation}
            AX=
            \begin{pmatrix}
                a_{1,j}\\
                \vdots\\
                a_{n,j}
            \end{pmatrix}
            \geqslant0
        \end{equation}
        donc $A\geqslant0$.

        \item Soit $A\in GL_{n}(\R)$ avec $A=(A_{i,j})_{1\leqslant i,j\leqslant n}\geqslant0,A^{-1}=(A^{-1})_{1\leqslant i,j\leqslant n}\geqslant0$. Soit $(i,j)\in\llbracket1,n\rrbracket^{2}$ avec $i\neq j$. On a 
        \begin{equation}
            \sum_{k=1}^{n}A_{i,k}A^{-1}_{k,j}=0
        \end{equation}
        donc pour tout $k\in\llbracket1,n\rrbracket$ on a $A_{i,j}=0$ ou $A^{-1}_{k,j}=0$.

        $i$ étant fixé, comme $A\in Gl_{n}(\R)$, il existe $k_{0}\in\llbracket1,n\rrbracket$ tel que $A_{i,k_{0}}>0$. Alors pour tout $j\in\llbracket1,n\rrbracket\setminus\lbrace i\rbrace$, on a $A^{-1}_{k_{0},j}=0$ et $A^{-1}_{k_{0},i}>0$ (car $A^{-1}$ est inversible). Supposons qu'il existe $k_{1}\neq k_{0}$ tel que $A_{i,k_{1}}>0$. Alors pour tout $j\neq i$, on a $A^{-1}_{k,j}=0$ et $A^{-1}_{k_{1},i}>0$, mais alors les lignes $k_{0}$ et $k_{1}$ sont liées, ce qui est impossible. Donc il existe un unique $k_{i}\in\llbracket1,n\rrbracket$, $A_{i,k_{i}}>0$.

        Comme $A$ est inversible, pour $i\neq i'$, on a $k_{i}\neq k_{i'}$, sinon on aurait deux lignes proportionnelles. Donc \function{\Delta}{\llbracket1,n\rrbracket}{\llbracket1,n\rrbracket}{i}{k_{i}}
        Ainsi il existe une unique permutation $\sigma\in\Sigma_{n}$ telle que pour tout $i\in\llbracket1,n\rrbracket$, $A_{i,\sigma(i)}>0$ et pour tout $j\neq\sigma(i)$, $A_{ij}=0$. Donc 
        \begin{equation}
            \boxed{A=\diag(a_{1},\dots,a_{n})P_{\sigma}}
        \end{equation}
        avec $P_{\sigma}=(\delta_{i,\sigma(j)})_{i,j}$ et $a_{i}>0$.

        Réciproquement, si $A$ est de cette forme, on a $A\geqslant0$ et 
        \begin{equation}
            A^{-1}=P^{-1}_{\sigma}\diag\left(\frac{1}{a_{1}},\dots,\frac{1}{a_{n}}\right)=P_{\sigma^{-1}}\diag\left(\frac{1}{a_{1}},\dots,\frac{1}{a_{n}}\right)
        \end{equation}
        donc $A^{-1}\geqslant0$.
    \end{enumerate}
\end{proof}

\begin{remark}
    Soit 
    \begin{equation}
        A=
        \begin{pmatrix}
            2   & -1    & 0   &\dots &0\\
            -1  & \ddots&\ddots     &\ddots&\vdots\\
            0   & \ddots&\ddots     &\ddots&0\\
            \vdots&\ddots&\ddots&\ddots&-1\\
            0&\dots&0&-1&2
        \end{pmatrix}
    \end{equation}

    Si $AX\geqslant0$, en définissant $x_{0}=x_{n+1}=0$, on a pour tout $k\in\llbracket1,n\rrbracket$,
    \begin{equation}
        -x_{k-1}+2x_{k}-x_{k+1}\geqslant0
    \end{equation}
    Si $x_{i_{0}}=\min(x_{0},\dots,x_{n+1})$, on a 
    \begin{equation}
        2x_{i_{0}}\geqslant x_{i_{0}-1}+x_{i_{0}+1}\geqslant 2x_{i_{0}}
    \end{equation}
    donc $x_{i_{0}-1}=x_{i_{0}+1}=x_{i_{0}}$. De proche en proche, on a $x_{i_{0}}=x_{0}=0$. Donc $X\geqslant0$.

    Si $AX=0$, on a $AX\geqslant0$ et $A(-X)=0$ donc $X\geqslant0$ et $-X\geqslant0$ donc $X=0$ et $A\in GL_{n}(\R)$ et pour tout $Y=AX\geqslant0$, on a $A^{-1}Y=X\geqslant0$ donc $A^{-1}\geqslant0$.
\end{remark}

\begin{proof}
    Soit \function{u}{\R_{n-1}[X]}{\R_{n-1}[X]}{P}{P(X+1)}
    Pour tout $j\in\llbracket1,n\rrbracket$,
    \begin{equation}
        (X+1)^{j-1}=\sum_{i=0}^{j-1}\binom{j-1}{i}X^{i}=\sum_{i=1}^{j}\binom{j-1}{i-1}X^{i-1}
    \end{equation}
    On note $P_{i}=X^{i-1}$ et $\mathcal{B}=(P_{1},\dots,P_{n})$ la base canonique de $\R_{n-1}[X]$. On note $A=\mat_{\mathcal{B}}(u)$. $u^{-1}\colon P\mapsto P(X-1)$ donc $A$ est inversible et pour tout $k\in\llbracket1,n\rrbracket$,
    \begin{equation}
        (X-1)^{j-1}=\sum_{i=0}^{j-1}\binom{j-1}{i}X^{i}(-1)^{j-i-1}=\sum_{i=1}^{j}\binom{j-1}{i-1}X^{i-1}(-1)^{j-i}
    \end{equation}
    donc 
    \begin{equation}
        \boxed{A^{-1}=\left(\binom{j-1}{i-1}(-1)^{j-i}\right)_{1\leqslant i,j\leqslant n}}
    \end{equation}

    Pour tout $k\in\N$, on a $u^{k}\colon P\mapsto P(X+k)$ et pour tout $j\in\llbracket1,n\rrbracket$,
    \begin{equation}
        (X+k)^{j-1}=\sum_{i=0}^{j-1}\binom{j-1}{i}X^{i}k^{j-i-1}=\sum_{i=1}^{j}\binom{j-1}{i-1}X^{i-1}k^{j-i}
    \end{equation}
    donc 
    \begin{equation}
        \boxed{A^{k}=\left(\binom{j-1}{i-1}k^{j-i}\right)_{1\leqslant i,j\leqslant n}}
    \end{equation}
\end{proof}

\begin{proof}
    \label{sol:4.4}
    \phantom{}
    \begin{enumerate}
        \item Pour $n\in\N^{*}$, on note $H(n)$:`si $\dim(E)=n$ et si $u\in\L(E)$ vérifie $\Tr(u)=0$, alors il existe une base $\mathcal{B}$ de $E$ telle que 
        \begin{equation}
            \mat_{\mathcal{B}}(u)=
            \begin{pmatrix}
                0 & \star & \dots & \star\\
                \star & \ddots & \ddots & \vdots\\
                \vdots & \ddots & \ddots & \star\\
                \star & \dots & \star &0
            \end{pmatrix}
        \end{equation}'

        Pour $n=1$, on a $u=0$ si $\Tr(u)=0$. Pour $n\geqslant1$, on suppose $H(n)$, soit $E$ de dimension $n+1$ et $u\in\L(E)$ tel que $\Tr(u)=0$. S'il existe $\lambda\in\K$ tel que $u=\lambda id_{E}$, on a $\Tr(u)=(n+1)\lambda=0$ donc $\lambda=0$ donc $u=0$. 

        Sinon, il existe $e_{1}\neq0$ tel que $(e_{1},u(e_{1}))$ est libre (résultat classique, redémontré en remarque ci-dessous). On pose $e_{2}=u(e_{1})$ et on complète $(e_{1},e_{2})$ en une base de $E$: $(e_{1},e_{2},\dots,e_{n+1})=\mathcal{B}_{1}$. Alors $\mat_{\mathcal{B}_{1}}(u)$ est de la forme 
        \begin{equation}
            \begin{pmatrix}
                0 & 
                \begin{matrix}
                    \star & \dots & \dots & \star    
                \end{matrix}\\
                \begin{matrix}
                    1\\
                    0\\
                    \vdots\\
                    0
                \end{matrix}
                & \mbox{\Huge A'}
            \end{pmatrix}
        \end{equation}
        avec $\Tr(u)=\Tr(A')=0$. Posons $F=\Vect(e_{2},\dots,e_{n+1})$. On note $\Pi$ la projection sur $F$ parallèlement à $\Vect(e_{1})$. Alors si \function{u'}{F}{F}{x}{\Pi(u(x))}
        et $A'=\mat_{(e_{2},\dots,e_{n+1})}(u')$ donc $\Tr(u')=0$. D'après $H(n)$, il existe $(f_{2},\dots,f_{n+1})$ une base de $F$ telle que 
        \begin{equation}
            \mat_{(f_{2},\dots,f_{n+1})}(u')=
            \begin{pmatrix}
                0 & \star & \dots & \star\\
                \star & \ddots & \ddots & \vdots\\
                \vdots & \ddots & \ddots & \star\\
                \star & \dots & \star &0
            \end{pmatrix}
        \end{equation}

        Soit donc $\mathcal{B}_{2}=(e_{1},f_{2},\dots,f_{n+1})$ base de $E$. On a $u(e_{1})\in F$ donc 
        \begin{equation}
            \boxed{
            \mat_{\mathcal{B}_{2}}(u)=
            \begin{pmatrix}
                0 & \star & \dots & \star\\
                \star & \ddots & \ddots & \vdots\\
                \vdots & \ddots & \ddots & \star\\
                \star & \dots & \star &0
            \end{pmatrix}
            }
        \end{equation}

        \item Soit $M=(a_{i,j})_{1\leqslant i,j\leqslant n}$ et $D=(i\delta_{i,j})_{1\leqslant i,j\leqslant n}$. On a 
        \begin{equation}
            [DM]_{i,j}=\sum_{k=1}^{n}i\delta_{i,k}a_{k,j}=ia_{i,j}
        \end{equation}
        et 
        \begin{equation}
            [MD]_{i,j}=\sum_{k=1}^{n}a_{i,k}k\delta_{k,j}=ja_{i,j}
        \end{equation}

        On a $M\in\ker(\varphi)$ si et seulement si pour tout $i\neq j$, $a_{i,j}=0$ si et seulement si $M\in D_{n}(\K)$ (ensemble des matrices diagonales). Donc $\dim(\ker(\varphi))=n$ et $\dim(\im(\varphi))=n^{n}-n$. Or pour tout $M\in \M_{n}(\K), [MD-DM]_{i,i}=0$. Notons $\Delta_{n}$ l'ensemble des matrices de diagonale nulle. On a $\im\varphi\subset\Delta_{n}$ et $\dim(\Delta_{n})=n^{2}-n$ (une base de $\Delta_{n}$ est $(E_{i,j})_{i\neq j}$, matrices élémentaires) donc $\im(\varphi)=\Delta_{n}$.

        Soit alors $A\in\M_{n}(\K)$ telle que $\Tr(A)=0$. D'après 1. il existe $P\in GL_{n}(\K)$ telle que $P^{-1}AP\in\Delta_{n}=\im(\varphi)$ donc il existe $M\in\M_{n}(\K)$ telle que $P^{-1}AP=MD-DM$ donc 
        \begin{align}
            A
            &=P(MD-DM)P^{-1}\\
            &=PM DP^{-1}-PD MP^{-1}\\
            &=\boxed{XY-YX}
        \end{align}
        avec $X=PMP^{-1}$ et $Y=PDP^{-1}$.
    \end{enumerate}
\end{proof}

\begin{remark}
    Soit $E$ un $\K$-espace vectoriel et $u\in\L(E)$ tel que pour tout $x\in E\setminus\lbrace0\rbrace$, $(x,u(x))$ est liée i.e.~pour tout $x\in E\setminus\lbrace0\rbrace$, $u(x)=\lambda_{x}x$. Alors $u$ est une homothétie.

    En effet, soit $(x,y)\in\left(E\setminus\lbrace0\rbrace\right)^{2}$, si $(x,y)$ est liée, il existe $\mu\in\K^{*}$ tel que $y=\mu x$. On a alors 
    \begin{equation}
        u(y)=\lambda_{y}y=\mu u(x)=\mu\lambda_{x}x=\lambda_{x}y
    \end{equation}
    On a $y\neq0$ donc $\lambda_{x}=\lambda_{y}$.

    Si $(x,y)$ est libre, on a
    \begin{equation}
        u(x+y)=\lambda_{x+y}(x+y)=\lambda_{x}x+\lambda_{y}y
    \end{equation}
    Par liberté de $(x,y)$, on a $\lambda_{x+y}=\lambda_{x}=\lambda_{y}$. 
    
    Ainsi, $\lambda_{x}$ ne dépend pas de $x$: il existe $\lambda\in\K$ tel que pour tout $x\in E$, $u(x)=\lambda x$, i.e.~$u=\lambda id_{E}$.
\end{remark}

\begin{proof}
    \phantom{}
    \begin{enumerate}
        \item Si
        $X=\begin{pmatrix}
            x_{1} & \dots &x_{n}
        \end{pmatrix}^{\mathsf{T}}$ et $Y=\begin{pmatrix}
            x_{1} & \dots &x_{n}
        \end{pmatrix}^{\mathsf{T}}$, on a 
        \begin{equation}
            XY^{\mathsf{T}}=
            \begin{pmatrix}
                x_{1}y_{1}&\dots&\dots&x_{1}y_{n}\\
                \vdots & & & \vdots\\
                \vdots & & & \vdots\\
                x_{n}y_{1} & \dots & \dots & x_{n}y_{n}
            \end{pmatrix}
        \end{equation}
        est de rang 1. On a 
        \begin{equation}
            (XY^{\mathsf{T}})^{2}=X(Y^{\mathsf{T}}X)Y^{\mathsf{T}}=\left(\sum_{i=1}^{n}x_{i}y_{i}\right)XY^{\mathsf{T}}
        \end{equation}

        Si $\lambda=0$, c'est évident.

        Si $\lambda\neq0$ et $B=I_{n}+\lambda XY^{\mathsf{T}}$, on a
        \begin{equation}
            XY^{\mathsf{T}}=\frac{B-I_{n}}{\lambda}
        \end{equation}
        et 
        \begin{equation}
            (XY^{\mathsf{T}})^{2}=\frac{\left(B-I_{n}\right)^{2}}{\lambda^{2}}
        \end{equation}
        soit 
        \begin{equation}
            (XY^{\mathsf{T}})^{2}=\frac{B^{2}-2B+I_{n}}{\lambda^{2}}=\left(\sum_{i=1}^{n}x_{i}y_{i}\right)\left(\frac{B-I_{n}}{\lambda}\right)
        \end{equation}
        d'où 
        \begin{equation}
            \lambda\left(Y^{\mathsf{T}}X\right)\left(B-I_{n}\right)=B^{2}-2B+I_{n}
        \end{equation}
        d'où
        \begin{equation}
            B^{2}+\left(-2-\lambda\left(Y^{\mathsf{T}}X\right)\right)B+I_{n}\left(1+\lambda\left(Y^{\mathsf{T}}X\right)\right)=0
        \end{equation}

        Si $1+\lambda Y^{\mathsf{T}}X\neq0$, alors $B$ est inversible et 
        \begin{equation}
            \boxed{B^{-1}=-\frac{1}{1+\lambda Y^{\mathsf{T}}X}\left(B-\left(2+\lambda Y^{\mathsf{T}}X\right)I_{n}\right)}
        \end{equation}

        Si $1+\lambda Y^{\mathsf{T}}X=0$, on a 
        \begin{equation}
            B\left(B-I_{n}\right)=0
        \end{equation}
        Si $B$ est inversible, on aura $B=I_{n}$ et $\lambda XY^{\mathsf{T}}=O_{\M_{n}(\K)}$. Or $\lambda\neq0$ donc $X=Y=0$ et $1=0$: absurde. Donc $B\notin GL_{n}(\K)$.

        \item On a 
        \begin{equation}
            M=A+\lambda XY^{Y}=A\left(I_{n}+\lambda A^{-1}XY^{\mathsf{T}}\right)
        \end{equation}
        donc $M\in GL_{n}(\R)$ si et seulement si $\left(I_{n}+\lambda A^{-1}XY^{\mathsf{T}}\right)$ est inversible si et seulement si $1+\lambda Y^{\mathsf{T}}A^{-1}X$ est inversible d'après 1. Alors 
        \begin{equation}
            \boxed{M^{-1}=\left(I_{n}-\frac{\lambda A^{-1}XY^{\mathsf{T}}}{1+\lambda Y^{\mathsf{T}}A^{-1}X}\right)A^{-1}}
        \end{equation}
    \end{enumerate}
\end{proof}

\begin{proof}
    On a $\dim(\R_{n}[X])=n+1$ donc il faut montrer que $(S_{0},\dots, S_{n})$ est libre. Soit donc $\alpha=\left(\alpha_{0},\dots,\alpha_{n}\right)\in\R^{n+1}$ tel que 
    \begin{equation}
        \alpha_{0}S_{0}+\dots+\alpha_{n}S_{n}=0
    \end{equation}

    Si $\alpha\neq0$, on pose $k_{0}=\max\left(k\in\llbracket0,n\rrbracket\middle|\alpha_{k}\neq0\right)$. On a 
    \begin{equation}
        \alpha_{0}(1-X)^{n}+\dots+\alpha_{k_{0}}X^{k_{0}}(1-X)^{n-k_{0}}=0
    \end{equation}
    soit 
    \begin{equation}
        \alpha_{0}(1-X)^{k_{0}}+\dots+\alpha_{k_{0}}X^{k_{0}}=0
    \end{equation}
    En évaluant en 1, on a $\alpha_{k_{0}}=0$ ce qui est absurde. Donc $(S_{0},\dots,S_{n})$ est une base de $\R_{n}[X]=n+1$.

    Pour tout $k\in\llbracket0,n\rrbracket$, on a 
    \begin{align}
        S_{j}
        &=X^{j}(1-X)^{n-j}\\
        &=X^{j}\left(\sum_{k=0}^{n-j}\binom{n-j}{k}(-1)^{k}X^{k}\right)\\
        &=\sum_{k=0}^{n-j}\binom{n-j}{k}(-1)^{k}X^{k+j}\\
        &=\sum_{k=j}^{n}\binom{n-j}{k-j}(-1)^{k-j}X^{k}
    \end{align}
    donc 
    \begin{equation}
        \boxed{
            A=P_{(1,\dots,X^{n})\to(S_{0},\dots, S_{n})}=\left(\binom{n-j}{k-j}(-1)^{k-j}\right)_{0\leqslant k,j\leqslant n}
        }
    \end{equation}

    On considère $u\in\L\left(\R[X]\right)$ tel que $u(X^{j})=S_{j}$ pour tout $j\in\llbracket0,n\rrbracket$. On a $u(X^{j})=\left(\frac{X}{1-X}\right)^{j}(1-X)^{n}$. Pour tout $P\in\R_{n}[X]$, on a $u(P)=P\left(\frac{X}{1-X}\right)(1-X)^{n}$. Soit $(P,Q)\in\R_{n}[X]^{2}$, on a $u(P)=Q$ si et seulement si $P\left(\frac{X}{1-X}\right)(1-X)^{n}=Q(X)$ si et seulement si $P(Y)\left(\frac{1}{1+Y}\right)^{n}=Q\left(\frac{Y}{1+Y}\right)$ soit $u(P)=Q$ si et seulement si $P(Y)=Q\left(\frac{Y}{1+Y}\right)(1+Y)^{n}$. Ainsi $u^{-1}(X^{j})=X^{j}(1+X)^{n-j}$, donc 
    \begin{equation}
        \boxed{
            A^{-1}=\mat_{(1,\dots,X^{n})}(u^{-1})=\left(\binom{n-j}{k-j}\right)_{0\leqslant k,j\leqslant n}
        }
    \end{equation}
\end{proof}

\begin{proof}
    Si on a $H\cap GL_{n}(\K)=\emptyset$, on a $I_{n}\notin H$. On écrit donc 
    \begin{equation}
        \M_{n}(\K)=H\oplus \K I_{n}
    \end{equation}
    Soit $i\neq j$, on prend $E_{i,j}=M+\lambda I_{n}$ (décomposition précédente) avec $\lambda\in\K$. Si $\lambda\neq0$, on a
    \begin{equation}
        M=E_{i,j}-\lambda I_{n}\in GL_{n}(\K)
    \end{equation}
    donc $M\in GL_{n}(\K)\cap H$: absurde. Donc $\lambda=0$ et $E_{i,j}\in H$, d'où $\Vect(E_{i,j})_{,\neq i}\subset H$. Or
    \begin{equation}
        \begin{pmatrix}
            0 & \dots & \dots & 0 & 1\\
            1 & \ddots & & & 0\\
            0 & \ddots & \ddots& & \vdots\\
            \vdots & \ddots & \ddots&\ddots & \vdots\\
            0 & \dots& 0 & 1 & 0
        \end{pmatrix}\in \left(GL_{n}(\K)\cap\Vect(E_{i,j})_{i\neq j}\right)\subset \left(GL_{n}(\K)\cap H\right)
    \end{equation}
    donc $H\cap GL_{n}(\K)\neq\emptyset$: absurde.
\end{proof}

\begin{remark}
    Il existe une forme linéaire non nulle $\varphi\colon\M_{n}(\K)\to \K$ telle que $H=\ker(\varphi)$. 

    En effet, pour toute forme linéaire $\varphi$ sur $\M_{n}(\K)$, il existe une unique matrice $A\in\M_{n}(\K)$ telle que 
    \begin{equation}
        \varphi(M)=\Tr(AM)
    \end{equation}

    Pour le montrer: si $A$ existe, pour tout $(i,j)\in\llbracket1,n\rrbracket^{2}$, $\varphi(E_{i,j})=\Tr(AE_{i,j})=a_{j,i}$. Réciproquement, soit $A=\left(\varphi(E_{j,i})\right)_{1\leqslant i,j\leqslant n}$. On a pour tout $M\in\M_{n}(\K)$, $\varphi(M)=\Tr(AM)$ car cex deux formes linéaires coïncident sur les $(E_{i,j})_{1\leqslant i,j\leqslant n}$.

    Il existe donc $A\in \M_{n}(\K)\setminus\lbrace0\rbrace$,
    \begin{equation}
        H=\left\lbrace M\in\M_{n}(\K)\middle|\Tr(AM)=0\right\rbrace
    \end{equation}

    Si $r=\rg(A)$, il existe $(P,Q)\in GL_{n}(\K)^{2}$ telles que $A=Q^{-1}J_{n,n,r}P$ ($J_{n,n,r}$: matrice de taille $n\times n$ avec les $r$ premiers coefficients diagonaux valant 1). Alors pour tout $M\in \M_{n}(\K)$, on a 
    \begin{equation}
        \Tr(AM)=\Tr(J_{n,n,r}\underbrace{MPQ^{-1}}_{=~M'})
    \end{equation}
    et il suffit de prendre 
    \begin{equation}
        M'=
        \begin{pmatrix}
            0 & \dots & \dots & 0 & 1\\
            1 & \ddots & & & 0\\
            0 & \ddots & \ddots& & \vdots\\
            \vdots & \ddots & \ddots&\ddots & \vdots\\
            0 & \dots& 0 & 1 & 0
        \end{pmatrix}
    \end{equation}
\end{remark}

\begin{remark}
    Si $F$ est un sous-espace vectoriel de $\M_{n}(\K)$ vérifie $\dim(F)\geqslant n^{2}-n+1$ alors 
    \begin{equation}
        G\cap GL_{n}(\K)\neq\emptyset
    \end{equation}
\end{remark}

\begin{proof}
    \phantom{}
    \begin{enumerate}
        \item On prend $\lambda=0$ et $N(0\times 0)=0\times N(0)=0$ donc 
        \begin{equation}
            \boxed{N(0)=0}
        \end{equation}

        \item On a pour $j\neq i,$ $E_{i,j}\times E_{j,j}=E_{i,j}$ et $E_{j,j}E_{i,j}=0$ donc $N(E_{i,j})=N(E_{i,j}E_{j,j})=N(E_{j,j}E_{i,j})$ d'où 
        \begin{equation}
            \boxed{N(E_{i,j})=0}
        \end{equation}

        \item Déjà traité à l'Exercice~4.
        
        \item Si $\Tr(A)=0$, alors il existe $P\in GL_{n}(\C)$ telle que 
        \begin{equation}
            P^{-1}AP=\sum_{i\neq j}\alpha_{i,j}E_{i,j}
        \end{equation}
        donc 
        \begin{equation}
            \boxed{N(A)=N(P^{-1}AP)\leqslant\sum_{i\neq j}\alpha_{i,j}N(E_{i,j})=0}
        \end{equation}

        \item Soit $A'=A-\frac{\Tr(A)}{n}I_{n}$. On a $N(A')=0$ d'après ce qui précède. Montrons que pour tout $(A,B)\in\M_{n}(\C)^{2}$,
        \begin{equation}
            \left\lvert N(A)-N(B)\right\rvert\leqslant N(A-B)
        \end{equation}
        On écrit $A=A-B+B$ et $N(A)\leqslant N(A-B)+N(B)$ d'où $N(A)-N(B)\leqslant N(A-B)$ et on a le résultat par symétrie de $A$ et $B$.

        On a donc 
        \begin{equation}
            \left\lvert N(A)-N\left(\frac{\Tr(A)}{n}I_{n}\right)\right\rvert\leqslant N\left(A-\frac{\Tr(A)}{n}I_{n}\right)=0
        \end{equation}
        d'où 
        \begin{equation}
            \boxed{N(A)=N\left(\frac{\Tr(A)}{n}I_{n}\right)=\left\lvert\Tr(A)\right\rvert\times \underbrace{N\left(\frac{I_{n}}{n}\right)}_{=~a\geqslant0}}
        \end{equation}
    \end{enumerate}
\end{proof}

\begin{proof}
    On écrit 
    \begin{equation}
        f+g=f\circ\left(id+f^{-1}\circ g\right)
    \end{equation}
    avec $f^{-1}\circ g$ de rang 1. Il existe une base $\mathcal{B}
    $ de $E$ telle que 
    \begin{equation}
        \mat_{\mathcal{B}}(f^{-1}\circ g)=
        \begin{pmatrix}
            0 & \dots & \dots & 0 & \star\\
            \vdots & & &\vdots &\vdots\\
            \vdots & & &\vdots & \vdots\\
            0 & \dots & \dots & 0 & \alpha
        \end{pmatrix}
    \end{equation}
    avec $\alpha=\Tr(f^{-1}\circ g)$ et donc $\mat_{\mathcal{B}}(id+g^{-1}\circ g)$ est inversible si et seulement si $1+\alpha\neq0$ si et seulement si $\Tr(f^{-1}\circ g)\neq 1$.
\end{proof}

\begin{proof}
    Par symétrie du problème, il suffit de déterminer les
    \begin{equation}
        a_{n,j}=\left\lvert\left\lbrace\text{chemins de longueur }n\text{ de }1\text{ vers }j\in\left\lbrace2,3,4\right\rbrace\right\rbrace\right\rvert
    \end{equation}

    On pose 
    \begin{equation}
        X_{n}=
        \begin{pmatrix}
            a(n,1)\\
            a(n,2)\\
            a(n,3)\\
            a(n,4)
        \end{pmatrix}
    \end{equation}
    On a alors 
    \begin{equation}
        X_{n+1}=
        \underbrace{
        \begin{pmatrix}
            0 &1 &0 &1\\
            1 &0 &1 &0\\
            0 &1 &0 &1\\
            1 &0 &1 &0
        \end{pmatrix}}_{=~A}
        X_{n}
    \end{equation}
    car (en raisonnant modulo 4) il y a autant de chemins de longueur n+1 reliant 1 à j que de chemins de longueur n reliant 1 à j-1 + chemins de longueur n reliant 1 à j+1.
    d'où $X_{n}=A^{n}X_{0}$ avec 
    \begin{equation}
        X_{0}=
        \begin{pmatrix}
            1\\0\\0\\0
        \end{pmatrix}
    \end{equation}
    On a
    \begin{equation}
        A=
        \begin{pmatrix}
            B&B\\B&B
        \end{pmatrix}
    \end{equation}
    avec
    \begin{equation}
        B=
        \begin{pmatrix}
            0&1\\1&0
        \end{pmatrix}
    \end{equation}
    On a $B^{2}=I_{2}$ et on montre par récurrence
    \begin{equation}
        \left\lbrace
            \begin{array}[]{ll}
                A^{2p}=2^{2p-1}
                \begin{pmatrix}
                    I_{2} &I_{2}\\ I_{2}&I_{2}
                \end{pmatrix} &p\geqslant1\\
                A^{2p+1}=2^{2p}
                \begin{pmatrix}
                    B & B\\B &B
                \end{pmatrix}&p\geqslant0
            \end{array}
        \right.
    \end{equation}

    Ainsi,
    \begin{equation}
        \boxed{
            \begin{array}[]{l}
                a(2p,1)=2^{2p-1}=a(2p,3)\\
                a(2p,2)=0=a(2p,4)\\
                a(2p+1,1)=0=a(2p+1,3)\\
                a(2p+1,4)=2^{2p}=a(2p+1,4)\\
            \end{array}
        }
    \end{equation}

    \item Ici, on a 
    \begin{equation}
        A=
        \begin{pmatrix}
            0 &1 &0 &1 &0 &1 &0 &0\\
            1 &0 &1 &0 &0 &0 &1 &0\\
            0 &1 &0 &1 &0 &0 &0 &1\\
            1 &0 &1 &0 &1 &0 &0 &0\\
            0 &0 &0 &1 &0 &1 &0 &1\\
            1 &0 &0 &0 &1 &0 &1 &0\\
            0 &1 &0 &0 &0 &1 &0 &1\\
            0 &0 &1 &0 &1 &0 &1 &0
        \end{pmatrix}
    \end{equation}

    En deux itérations, il y a chaque fois deux possibilités pour relier deux sommets différents de même partié, et 3 pour revenir au même sommet. On a donc 
    \begin{equation}
        A^{2}=
        \begin{pmatrix}
            3 &0 &2 &0 &2 &0 &2 &0\\
            0 &3 &0 &2 &0 &2 &0 &2\\
            2 &0 &3 &0 &2 &0 &2 &0\\
            0 &2 &0 &3 &0 &2 &0 &2\\
            2 &0 &2 &0 &3 &0 &2 &0\\
            0 &2 &0 &2 &0 &3 &0 &2\\
            2 &0 &2 &0 &2 &0 &3 &0\\
            0 &2 &0 &2 &0 &2 &0 &3
        \end{pmatrix}
        =I_{8}+2
        \begin{pmatrix}
            B &B &B &B\\
            B &B &B &B\\
            B &B &B &B\\
            B &B &B &B
        \end{pmatrix}
    \end{equation}

    On applique le binôme de Newton pour calculer les puissances paires de $A$, puis on déduit les puissances impaires en multipliant par $A$.
\end{proof}

\begin{proof}
    Soit $X=\begin{pmatrix}
        x_{1}&\dots&x_{n}
    \end{pmatrix}^{\mathsf{T}}\in\M_{n,1}(\C)$. Supposons $AX=0$. Alors pour tout $i\in\llbracket1,n\rrbracket$,
    \begin{equation}
        \sum_{j=1}^{n}a_{i,j}x_{j}=0\Rightarrow -a_{i,i}x_{i}=\sum_{\substack{j=1\\j\neq i}}^{n}a_{i,j}x_{j}
    \end{equation}
    donc 
    \begin{equation}
        \left\lvert\sum_{\substack{j=1\\j\neq i}}^{n}a_{i,j}x_{j}\right\rvert=\left\lvert a_{i,i}x_{i}\right\rvert\leqslant\sum_{\substack{j=1\\j\neq i}}^{n}\left\lvert a_{i,j}x_{j}\right\rvert
    \end{equation}
    Soit $i_{0}\in\llbracket1,n\rrbracket$ tel que 
    \begin{equation}
        x_{i_{0}}=\max\left\lbrace\left\lvert x_{i}\right\rvert,i\in\llbracket1,n\rrbracket\right\rbrace
    \end{equation}

    On a alors 
    \begin{equation}
        \left\lvert a_{j,i_{0}}\right\rvert\left\lvert x_{i_{0}}\right\rvert\leqslant \left\lvert x_{i_{0}}\right\rvert\sum_{\substack{j=1\\j\neq i}}^{n}\left\lvert a_{i_{0},j}\right\rvert
    \end{equation}
    D'après l'hypothèse, on a $\left\lvert x_{i_{0}}\right\rvert=0$ donc $X=0$ et $A$ est inversible.

    Il faut l'inégalité stricte, un contre-exemple est donnée par une ligne nulle.
\end{proof}

\begin{remark}
    Si pour tout $j\in\llbracket1,n\rrbracket$, $\left\lvert a_{j,j}\right\rvert>\sum_{i\neq j}\left\lvert a_{i,j}\right\rvert$ alors $A^{\mathsf{T}}\in GL_{n}(\C)$ et donc $A\in GL_{n}(\C)$.
\end{remark}

\begin{proof}
    On écrit, pour tout $(i,j)\in\llbracket1,n\rrbracket^{2}$,
    \begin{align}
        i\wedge j
        &=\sum_{k\mid i\wedge j}\varphi(k)\\
        &=\sum_{\substack{k\mid i\\ k\mid j}}\varphi(k)\\
        &=\sum_{k=1}^{n}b_{k,i}b_{k,j}\varphi(k)
    \end{align}
    avec $b_{k,i}=1$ si $k\mid i$ et 0 sinon. On a alors, si $A=\left(i\wedge j\right)_{1\leqslant i,j\leqslant n}$, $A=B^{\mathsf{T}}C$ avec $B=(b_{k,i})_{1\leqslant i,k\leqslant n}$ (triangulaire supérieure) et $C=\left(\varphi(k)b_{k,j}\right)_{1\leqslant k,j\leqslant n}$ (triangulaire supérieure). Donc 
    \begin{equation}
        \boxed{\det(A)=\prod_{i=1}^{n}\varphi(i)}
    \end{equation}
\end{proof}

\begin{proof}
    Pour l'unicité, si $A=L_{1}U_{1}=L_{2}U_{2}$ telles que proposées. Comme $A$ est inversible, on a $\det(A)=\det(L_{i})\det(U_{i})\neq0$ pour $i\lbrace1,2\rbrace$ et donc $L_{i}$ et $U_{i}$ sont inversibles. Ainsi,
    \begin{equation}
        L_{2}^{-1}L_{1}=U_{2}U_{1}^{-1}\in \mathcal{T}_{n}^{-}(\C)\cap\mathcal{T}_{n}^{+}(\C)
    \end{equation}
    avec des 1 sur la diagonale, c'est donc $I_{n}$, d'où l'unicité.

    Pour l'existence, on travaille par récurrence sur $n\in\N$: pour $n=1$ on a $A=(1)\times(a_{1,1})$. Soit $A_{n+1}\in \M_{n+1}(\C)$ vérifiant l'hypothèse, alors $A_{n}$ vérifie l'hypothèse $A_{n}=L_{n}U_{n}$ avec 
    \begin{equation}
        A_{n+1}=
        \begin{pmatrix}
            A_{n} & Y\\
            X^{\mathsf{T}} & a_{n+1,n+1}
        \end{pmatrix}
    \end{equation}

    On veut 
    \begin{equation}
        A_{n+1}=
        \begin{pmatrix}
            L' &\begin{matrix}
                0\\\vdots\\0
            \end{matrix}\\
            X_{1}^{\mathsf{T}} &1
        \end{pmatrix}\times\begin{pmatrix}
            U' & Y_{1}\\
            \begin{matrix}
                0&\dots&0
            \end{matrix}&u_{n+1,n+1}
        \end{pmatrix}
    \end{equation}
    On a $(X,Y)\in\M_{n+1}(\C)$, par produits par blocs, on a $A_{n}=L'U'=L_{n}U_{n}$ et par unicité, $L'=L_{n}$ et $U'=U_{n}$. On a $X^{\mathsf{T}}=X_{1}^{\mathsf{T}}U'$ et donc $X_{1}^{\mathsf{T}}=X^{\mathsf{T}}U_{n}^{-1}$ et $Y=L_{n}Y_{1}$ donc $Y_{1}=L_{n}^{-1}Y$.

    Enfin, $a_{n+1,n+1}=X_{1}^{\mathsf{T}}Y_{1}+u_{n+1,n+1}$ et donc 
    \begin{equation}
        u_{n+1,n+1}=a_{n+1,n+1}-X_{1}^{\mathsf{T}}Y_{1}=a_{n+1,n+1}-X^{\mathsf{T}}U_{n}^{-1}L_{n}^{-1}Y
    \end{equation}

    Réciproquement, en définissant ainsi $U$ et $L$, on a bien $A=Lu$ en remontant les calculs.
\end{proof}

\begin{proof}
    On a $\sum_{k\in A_{i}}a_{k}-\sum_{k\in B_{i}}a_{k}=0$ (combinaison linéaire des $a_{k}$ avec des coefficients $\pm1$), donc 
    \begin{equation}
        \underbrace{
            \begin{pmatrix}
                0 & \pm 1&\dots&\dots&\pm1\\
                \pm 1&\ddots&\ddots&&\vdots\\
                \vdots&\ddots&\ddots&\ddots&\vdots\\
                \vdots&&\ddots&\ddots&\pm1\\
                \pm1&\dots&\dots&\pm1&0
            \end{pmatrix}
        }_{=~A}
        \underbrace{
            \begin{pmatrix}
                a_{1}\\
                \vdots\\
                a_{2n+1}
            \end{pmatrix}
        }_{=~X}=0
    \end{equation}

    Sur chaque ligne, il y a $n$ fois 1 et $n$ fois -1 (car les $A_{i}$ et $B_{i}$ sont disjoints). On veut montrer que $X=\alpha\bm{1}$. On a $X\in\ker(A)$ et $\bm{1}\in\ker(A)$ (car il y a $n$ 1 et $n$ -1 par ligne). On veut donc montrer que $\dim(\ker(A))=1$, soit $\rg(A)=2n$.

    On doit donc montrer qu'il existe une sous-matrice de taille $2n$ inversible car $\dim(\ker(A))\geqslant1$. Comme on est bloqué par les $\pm1$, on se place dans $\Z/2\Z$. Soit donc 
    \begin{equation}
        \overline{B_{n}}=
        \begin{pmatrix}
            \overline{0} & \overline{1}&\dots&\dots&\overline{1}1\\
            \overline{1}&\ddots&\ddots&&\vdots\\
            \vdots&\ddots&\ddots&\ddots&\vdots\\
            \vdots&&\ddots&\ddots&\overline{1}1\\
            \overline{1}1&\dots&\dots&\overline{1}1&\overline{0}
        \end{pmatrix}\in\M_{n}\left(\Z/2\Z\right)
    \end{equation}
    Si $\det(\overline{B_{n}})\neq0$, on a $\det(B_{n})\neq 2k$ pour tout $k\in\N$ où $B_{n}$ est obtenue en enlevant à $A$ sa dernière ligne et sa dernière colonne, et donc $\det(A)\neq0$.

    On cherche un polynôme annulateur de $\overline{B_{n}}$. On a 
    \begin{equation}
        \left(\overline{B_{n}}+\overline{I_{2n}}\right)^{2}=\overline{B_{n}}^{2}+2\overline{B_{n}}+I_{2n}=\begin{pmatrix}
            \overline{1}&\dots&\overline{1}\\
            \vdots & & \vdots\\
            \overline{1}&\dots&\overline{1}
        \end{pmatrix}^{2}=2n
        \begin{pmatrix}
            \overline{1}&\dots&\overline{1}\\
            \vdots & & \vdots\\
            \overline{1}&\dots&\overline{1}
        \end{pmatrix}=\begin{pmatrix}
            \overline{0}
        \end{pmatrix}
    \end{equation}

    Ainsi,
    \begin{equation}
        \overline{B_{n}}\left(\overline{B_{n}}+2\overline{I_{2n}}\right)=-\overline{I_{2n}}=\overline{I_{2n}}
    \end{equation}
    donc $\overline{B_{n}}\in GL_{n}\left(\Z/2\Z\right)$ et donc $B_{n}\in GL_{2n}(\R)$, ce qui démontre bien que $\rg(A)=2n$ et $\ker(A)=\Vect(\bm{1})$, d'où
    \begin{equation}
        \boxed{a_{1}=\dots=a_{2n+1}}
    \end{equation}
\end{proof}

\begin{proof}
    On note $T_{i,j}(\lambda)=I_{n}+\lambda E_{i,j}$ pour $i<j$. On rappelle que la multiplication à gauche par $T_{i,j}(\lambda)$ remplace la $i$-ième ligne de la matrice $L_{i}$ par $L_{i}+\lambda L_{j}$: on ajoute à une ligne $\lambda$ fois une ligne d'indice supérieur.
    La multiplication à droite par $T_{i,j}(\lambda)$ remplace la $j$-ième colonne de la matrice $C_{j}$ par $C_{j}+\lambda C_{i}$: on ajoute à une colonne $\lambda$ foi une colonne d'indice inférieur. Ces matrices sont des matrices de transvection.

    On note aussi $D_{i}(\lambda)$ la matrice de dilatation qui contient des 1 sur la diagonale sauf en $i$ position où il y a un $\lambda$. On rappelle que la multiplication à gauche par $D_{i}(\lambda)$ revient à multiplier $L_{i}$ par $\lambda$ et la multiplication à droite revient à multiplier $C_{i}$ par $\lambda$.

    Sur la première colonne de $M$, il y a au moins un coefficient non nul car $M\in GL_{n}(\C)$. Soit $i_{1}=\max\lbrace i\in\llbracket,n\rrbracket,m_{i,1}\neq0\rbrace$. On effectue alors 
    \begin{equation}
        D_{i_{1}}\left(\frac{1}{m_{i_{1},1}}\right)M=
        \begin{pmatrix}
            \star & \star & \dots & \dots & \star\\
            \vdots & \vdots & & &\vdots\\
            \star & \vdots & & & \vdots\\
            1 & \vdots & & & \vdots\\
            0 & \vdots & & &\vdots\\
            \vdots & \vdots & & & \vdots\\
            0 & \star & \dots & \dots & \star
        \end{pmatrix}
    \end{equation}

    Par produite de transvections (qui sont des matrices triangulaires supérieures, i.e.~dans $\mathcal{T}_{n}^{+}$) à gauche, on obtient 
    \begin{equation}
        \begin{pmatrix}
            0 & \star & \dots & \dots & \star\\
            \vdots & \vdots & & &\vdots\\
            0 & \vdots & & & \vdots\\
            1 & \vdots & & & \vdots\\
            0 & \vdots & & &\vdots\\
            \vdots & \vdots & & & \vdots\\
            0 & \star & \dots & \dots & \star
        \end{pmatrix}
    \end{equation}

    Par produite de transvections $\in\mathcal{T}_{n}^{+}$ à droite, on obtient
    \begin{equation}
        \begin{pmatrix}
            0 & \star & \dots & \dots & \star\\
            \vdots & \vdots & & &\vdots\\
            0 & \star &\dots &\dots & \star\\
            1 & 0 &\dots & \dots& 0\\
            0 & \star & \dots& \dots&\star\\
            \vdots & \vdots & & & \vdots\\
            0 & \star & \dots & \dots & \star
        \end{pmatrix}
    \end{equation}

    Soit $M'\in GL_{n}(\C)$ la matrice extraite de $M$ en ôtant la première colonne et la $i_{1}$-ième ligne. On procède par récurrence avec $M'$. Donc il existe $\sigma\in\Sigma_{n},(T,T')\in\left(\mathcal{T}_{n}^{+}\right)^{2}$ telle que 
    \begin{equation}
        \boxed{M=TP_{\sigma}T'}
    \end{equation}

    Montrons que tout matrice de $\mathcal{T}_{n}^{+}$ inversible est produit de matrices de transvections dans $\mathcal{T}_{n}^{+}$ et de dilatations.

    Soit $T\in\mathcal{T}_{n}^{+}\cap GL_{n}(\C)$ avec 
    \begin{equation}
        T=
        \begin{pmatrix}
            t_{1,1}&\star &\dots & \dots& \star\\
            0 & \ddots & \ddots & & \vdots\\
            \vdots & \ddots & \ddots & \ddots & \vdots\\
            \vdots & & \ddots & \ddots & \star\\
            0 &\dots & \dots & 0 & t_{n,n}
        \end{pmatrix}
    \end{equation}

    On a $t_{1,1}\neq0$ car sinon la colonne 1 est nulle. On a donc 
    \begin{equation}
        TD_{1}\left(\frac{1}{t_{1,1}}\right)=
        \begin{pmatrix}
            1&\star &\dots & \dots& \star\\
            0 & \star & \ddots & & \vdots\\
            \vdots & \ddots & \ddots & \ddots & \vdots\\
            \vdots & & \ddots & \ddots & \star\\
            0 &\dots & \dots & 0 & \star
        \end{pmatrix}
    \end{equation}

    Puis, par produit de transvections à droite, on a 
    \begin{equation}
        \begin{pmatrix}
            1&0 &\dots & \dots& 0\\
            0 & \star & \dots &\dots & \star\\
            \vdots & \ddots & \ddots &  & \vdots\\
            \vdots & & \ddots & \ddots & \star\\
            0 &\dots & \dots & 0 & \star
        \end{pmatrix}
    \end{equation}

    On procède ensuite par récurrence sur $n$, et on a 
    \begin{equation}
        T\times B_{1}\times\dots\times B_{l}=I_{n}
    \end{equation}
    donc 
    \begin{equation}
        T=B_{l}^{-1}\times\dots\times B_{1}^{-1}
    \end{equation}
    où $B_{i}\mathcal{T}_{n}^{+}$ transvection ou dilatation.

    Soit donc $(T,T',P_{\sigma})$ vérifiant les hypothèses telles que $M=TP_{\sigma}T'$, alors on a 
    \begin{equation}
        T^{-1}MT'^{-1}=P_{\sigma}=\underbrace{B_{l}^{-1}\times\dots\times B_{1}^{-1}}_{\text{transvections ou dilatations}}\times M\times\underbrace{B_{l}'^{-1}\times\dots\times B_{1}'^{-1}}_{\text{transvections ou dilatations}}
    \end{equation}

    Nécessairement, on a  $\sigma(1)=i$ défini plus haut. Donc de proche en proche, $\sigma$ est univoquement déterminée.

    Cependant, on peut écrire
    \begin{equation}
        I_{2}=
        \begin{pmatrix}
        \frac{1}{2}&0\\
        0&\frac{1}{2}    
        \end{pmatrix}\times I_{2}\times
        \begin{pmatrix}
            2&0\\
            0&2    
        \end{pmatrix}
        =
        I_{2}=
        \begin{pmatrix}
        \frac{1}{3}&0\\
        0&\frac{1}{3}    
        \end{pmatrix}\times I_{2}\times
        \begin{pmatrix}
            3&0\\
            0&3    
        \end{pmatrix}
    \end{equation}
    donc il n'y a pas unicité de $T$ et $T'$.
\end{proof}

\begin{proof}
    \phantom{}
    \begin{enumerate}
        \item Soit $A\in J\cap GL_{n}(\K)$, on a pour tout $M\in\M_{n}(\K)$, on a 
        \begin{equation}
            M=\underbrace{\underbrace{M\times A}_{\in J}\times A^{-1}}_{\in J}\in J
        \end{equation}

        \item Soit $A_{0}\in J\setminus\lbrace0\rbrace$ de rang $r\neq0$. Il existe $(P,Q)\in GL_{n}(\K)$ telle que $Q^{-1}A_{0}P=J_{r}\in J$, on a alors 
        \begin{equation}
            \boxed{J_{r}\times J_{1}=J_{1}\in J}
        \end{equation}

        \item Deux matrices de rang 1 sont équivalentes donc toutes les matrices de rang 1 son dans $J$. Or si $A=\left(a_{i,j}\right)_{1\leqslant i,j\leqslant n}\in\M_{n}(\K)$ s'écrit 
        \begin{equation}
            \boxed{A=\sum_{1\leqslant i,j\leqslant n}\underbrace{a_{i,j}E_{i,j}}_{\text{de rang 1 ou 0}}\in J}
        \end{equation}
    \end{enumerate}
\end{proof}

\begin{proof}
    On a 
    \begin{equation}
        (\lambda A+I_{n})(\lambda B+I_{n})=\lambda^{2} AB+\lambda A+\lambda B+I_{n}=I_{n}
    \end{equation}
    donc $\lambda B+I_{n}$ est inversible. De plus, $A(\lambda B+I_{n})=-B$ donc 
    \begin{equation}
        A=-\left(\lambda B+I_{n}\right)^{-1}B
    \end{equation}

    Or $(\lambda B+I_{n})^{-1}$ et $B$ commutent. En effet, comme $\lambda\neq0$, on a 
    \begin{equation}
        B(\lambda B+I_{n})^{-1}=\left(\left[B+\frac{1}{\lambda I_{n}}\right]-\frac{1}{\lambda}I_{n}\right)(\lambda B+I_{n})^{-1}=\frac{1}{\lambda}I_{n}-\frac{1}{\lambda}(\lambda B+I_{n})^{-1}
    \end{equation}
    et on montre de même que 
    \begin{equation}
        (\lambda B+I_{n})^{-1}B=\frac{1}{\lambda}I_{n}-\frac{1}{\lambda}(\lambda B+I_{n})^{-1}
    \end{equation}

    Ainsi, 
    \begin{equation}
        \boxed{BA=-B(\lambda B+I_{n})^{-1}B=-(\lambda B+I_{n})^{-1}BB=AB}
    \end{equation}
\end{proof}

\begin{proof}
    Soit $X=\begin{pmatrix}
        x_{1} &\dots&x_{n}
    \end{pmatrix}^{\mathsf{T}}$ et $Y=\begin{pmatrix}
        y_{1}&\dots& y_{n}
    \end{pmatrix}^{\mathsf{T}}$

    On a $AX=Y$ si et seulement si 
    \begin{equation}
        \left\lbrace
        \begin{array}[]{lcll}
            x_{1}-a_{2}x_{2}-\dots- a_{n}x_{n} &= &y_{1} &[1]\\
            a_{2}x_{1}+x_{2} &= &y_{2} &[2]\\
            \vdots\\
            a_{n}x_{1}+x_{n}&=&y_{n}&[n]
        \end{array}
        \right.
    \end{equation}
    si et seulement si ($L_{1}\leftarrow L_{1}+\sum_{i=2}^{n}a_{i}L_{i}$)
    \begin{equation}
        \left\lbrace
        \begin{array}[]{lcll}
            \left(1+\sum_{i=2}^{n}a_{i}^{2}\right)x_{1} &= &y_{1}+\sum_{i=2}^{n}a_{i}y_{i} &[1]\\
            a_{2}x_{1}+x_{2} &= &y_{2} &[2]\\
            \vdots\\
            a_{n}x_{1}+x_{n}&=&y_{n}&[n]
        \end{array}
        \right.
    \end{equation}
    si et seulement si 
    \begin{equation}
        \left\lbrace
        \begin{array}[]{lcll}
            x_{1} &= &\dfrac{y_{1}+a_{2}y_{2}+\dots+a_{n}y_{n}}{1+\sum_{i=2}^{n}a_{i}^{2}}\\
            x_{j}&=&y_{j}-a_{j}x_{1} &\forall j\in\llbracket2,n\rrbracket
        \end{array}
        \right.
    \end{equation}
    En posant 
    \begin{equation}
        \lambda=\frac{1}{1+\sum_{i=2}^{n}a_{i}^{2}}
    \end{equation}
    cela équivaut à (en posant $a_{1}=1$)
    \begin{equation}
        \left\lbrace
        \begin{array}[]{lcll}
            x_{1} &= &\lambda(y_{1}+a_{2}y_{2}+\dots+a_{n}y_{n})\\
            x_{j}&=&\lambda\left[\sum_{\substack{i=1\\i\neq j}}a_{i}y_{i}-\left(1+\sum_{\substack{i=1\\i\neq j}}a_{i}^{2}\right)y_{j}\right] &\forall j\in\llbracket2,n\rrbracket
        \end{array}
        \right.
    \end{equation}

    Donc $A\in GL_{n}(\R)$.
\end{proof}

\begin{remark}
    On pourrait se poser la question si $A\in GL_{n}(\C)$ ? Si $1+a_{2}^{2}+\dots+a_{n}^{2}\neq0$, on sait que $A\in GL_{n}(\C)$. Cependant, on vérifie que si $X=\begin{pmatrix}
        1 & -a_{2}&\dots&-a_{n}
    \end{pmatrix}^{\mathsf{T}}\neq0$, on a $AX=0$ et donc $A\notin GL_{n}(\C)$.
\end{remark}

\begin{proof}
    \phantom{}
    \begin{enumerate}
        \item Soit $X=\begin{pmatrix}
            x_{1}&\dots &x_{n}
        \end{pmatrix}\in \ker(A)\cap H$, on a $\sum_{i=1}^{n}x_{i}=0$ et $AX=0$. Notons que l'on a 
        \begin{equation}
            A+A^{\mathsf{T}}=
            \begin{pmatrix}
                0&1 &\dots & \dots& 1\\
                1 & \ddots & \ddots & & \vdots\\
                \vdots & \ddots & \ddots & \ddots & \vdots\\
                \vdots & & \ddots & \ddots & 1\\
                1 &\dots & \dots & 1 & 0
            \end{pmatrix}=N
        \end{equation}
        On a 
        \begin{equation}
            NX=-X
        \end{equation}
        et $N^{\mathsf{T}}=N$.

        On a alors 
        \begin{equation}
            X^{\mathsf{T}}AX+X^{\mathsf{T}}A^{\mathsf{T}}X=X^{\mathsf{T}}NX=-X^{\mathsf{T}}X=-\sum_{i=1}^{n}x_{i}^{2}
        \end{equation}
        Comme $AX=0$, on a aussi $X^{\mathsf{T}}AX=0$ et $X^{\mathsf{T}}A^{\mathsf{T}}X=(AX)^{\mathsf{T}}X=0$ donc on a $\sum_{i=1}^{n}x_{i}^{2}=0$ d'où $x_{i}=0$ et $X=0$. Donc 
        \begin{equation}
            \boxed{\ker(u)\cap H}=\lbrace 0\rbrace
        \end{equation}

        Donc $\dim(\ker(u))\in\lbrace0,1\rbrace$ et le théorème du rang assure alors que $\rg(A)\in\lbrace n-1,n\rbrace$.

        \item Comme $A+A^{\mathsf{T}}=N$, on a $A=\frac{1}{2}N+S$ avec $S\in\mathcal{A}_{n}(\R)$. Or, pour $S=0$, on a 
        \begin{equation}
            N=\begin{pmatrix}
                1&\dots&1\\
                \vdots & &1\\
                1 & \dots &1
            \end{pmatrix}
            -I_{n}
        \end{equation}
        et $(N+I_{n})^{2}=n(M+I_{n})$ donc $N\in GL_{n}(\R)$. De même, pour 
        \begin{equation}
            S=\frac{1}{2}
            \begin{pmatrix}
                0&-1 &\dots & \dots& -1\\
                1 & \ddots & \ddots & & \vdots\\
                \vdots & \ddots & \ddots & \ddots & \vdots\\
                \vdots & & \ddots & \ddots & -1\\
                1 &\dots & \dots & 1 & 0
            \end{pmatrix}
        \end{equation}
        on a
        \begin{equation}
            A=
            \begin{pmatrix}
                0&0 &\dots & \dots& 0\\
                1 & \ddots & \ddots & & \vdots\\
                \vdots & \ddots & \ddots & \ddots & \vdots\\
                \vdots & & \ddots & \ddots & 0\\
                1 &\dots & \dots & 1 & 0
            \end{pmatrix}
        \end{equation}
        et $\rg(A)=n-1$.

        Donc on peut avoir les deux possibilités.
    \end{enumerate}
\end{proof}

\begin{proof}
    Soit $u\in\L(\C^{n})$ de range 1 telle que $\Tr(u)=\lambda$. On a $\dim(\ker(u))=n-1$

    En prenant une base de $\ker(u)$ $(e_{1},\dots,e_{n-1})$ que l'on complète en $\mathcal{B}=(e_{1},\dots,e_{n-1},e_{n})$ une base de $\C^{n}$, on a 
    \begin{equation}
        \mat_{\mathcal{B}}(u)
        =
        \begin{pmatrix}
            0 & \dots & 0 & \alpha_{1}\\
            \vdots & & \vdots &\vdots\\
            \vdots & & \vdots &\alpha_{n-1}\\
            0 & \dots & 0& \lambda
        \end{pmatrix}
    \end{equation}

    Si $\lambda\neq0$, posons $f_{n}=\beta_{1}e_{1}+\dots+\beta_{n-1}e_{n}+e_{n}$, on a 
    \begin{equation}
        u(f_{n})=\lambda f_{n}
    \end{equation}
    si et seulement si
    \begin{equation}
        \alpha_{1}e_{1}+\dots+\alpha_{n-1}e_{n-1}+\lambda e_{n}=\lambda f_{n}
    \end{equation}
    On pose $\beta_{1}=\frac{\alpha}{\lambda},\dots,\beta_{n-1}=\frac{\alpha_{n-1}}{\lambda}$ et si $\mathcal{B}'=(e_{1},\dots,e_{n-1},f_{n})$, on a 
    \begin{equation}
        \mat_{\mathcal{B}'}(u)=
        \begin{pmatrix}
            0 & \dots & 0 & 0\\
            \vdots & & \vdots &\vdots\\
            \vdots & & \vdots &0\\
            0 & \dots & 0& \lambda
        \end{pmatrix}
    \end{equation}

    Si $\lambda=0$, il existe $i_{0}\in\llbracket 1,n-1\rrbracket$ tel que $\alpha_{i_{0}}\neq0$ (sinon $\rg(u)=0$). On pose $f_{n}=e_{n}$ et $f_{1}=\alpha_{1}e_{1}+\dots+\alpha_{n-1}e_{n-1}\in\ker(u)\setminus\lbrace0\rbrace$ et on complète $(f_{1},\dots,f_{n-1})$ en une base de $\ker(u)$. On pose $\mathcal{B}'=(f_{1},\dots,f_{n})$ base de $\C^{n}$ et on a alors 
    \begin{equation}
        \mat_{\mathcal{B}'}(u)=
        \begin{pmatrix}
            0 & \dots & 0 & 1\\
            \vdots & & 0 &0\\
            \vdots & & \vdots &\vdots\\
            0 & \dots & 0& 0
        \end{pmatrix}
    \end{equation}

    Ainsi, dans les deux cas, deux matrices sont de rang 1 et de même trace si et seulement si elles sont semblables.
\end{proof}

\begin{proof}
    \phantom{}
    \begin{enumerate}
        \item Soit $M\in F$, telle que $\rg(M)=r$. $M$ est équivalente à $J_{r}$, donc il existe $(P_{0},Q_{0})\in GL_{n}(\R)^{2}$ telle que $P_{0}^{-1}J_{r}Q_{0}=M\in F$.
        
        \item Soit \function{\varphi}{F}{F_0}{M}{P_0 MQ_{0}^{-1}}
        est linéaire surjective par définition de $F_{0}$ de réciproque $\varphi^{-1}\colon M_{0}\to P_{0}^{-1}M_{0}Q_{0}$ donc $F$ et $F_{0}$ sont isomorphes.

        Pour tout $M\in F$, $\rg(M)=\rg(\varphi(M))$: $\varphi$ étant bijective, on a $r=\max\left\lbrace\rg(M_{0})\middle| M_{0}\in F_{0}\right\rbrace$

        \item Il suffit de choisir les coefficients de $B$ et $C$ donc 
        \begin{equation}
            \boxed{\dim(G_{0})=n(n-r)}
        \end{equation}

        \item On écrit 
        \begin{equation}
            \begin{pmatrix}
                \lambda I_{r} & B^{\mathsf{T}}\\
                B & C
            \end{pmatrix}
            =\lambda J_{r}+M_{0}\in F_{0}
        \end{equation}

        Si on avait 
        \begin{equation}
            \det\left(
				\begin{array}{@{}c|c@{}}
				\lambda
				I_{r} &
				\begin{matrix}
				b_{j,1}\\
						\vdots\\
						b_{j,r}
						\end{matrix}
						\\
					\hline
					\begin{matrix}
						b_{i,1} &
						\dots
						& b_{i,r}
						\end{matrix}
						& c_{i,j}
				\end{array}
			\right)\neq0
        \end{equation}
        (déterminant d'une sous-matrice de taille $r+1$ de la matrice précédente), on aurait 
        \begin{equation}
            \rg\begin{pmatrix}
                \lambda I_{r} & B^{\mathsf{T}}\\
                B & C
            \end{pmatrix}\geqslant r+1>r
        \end{equation}
        ce qui est exclu d'après 2.

        \item En effectuant $L_{r+1}\leftarrow L_{r+1}-\frac{b_{i,1}}{\lambda}L_{1}-\dots-\frac{b_{i,r}}{\lambda}L_{r}$, en notant $f(\lambda)=c_{i,j}-\sum_{k=1}^{r}\frac{b_{i,k}}{\lambda}b_{j,k}$, on obtient 
        \begin{equation}
            \det\left(
				\begin{array}{@{}c|c@{}}
				\lambda
				I_{r} &
				\begin{matrix}
				b_{j,1}\\
						\vdots\\
						b_{j,r}
						\end{matrix}
						\\
					\hline
					\begin{matrix}
						0 &
						\dots
						& 0
						\end{matrix}
						& f(\lambda)
				\end{array}
			\right)=0
        \end{equation}

        D'où $f(\lambda)=0$ et comme $\lambda\neq0$, on a
        \begin{equation}
            \lambda f(\lambda)=0=\lambda c_{i,j}-\sum_{k=1}^{r}b_{i,k}b_{j,k}
        \end{equation}
        qui est nulle sur $\R^{*}$ donc $c_{i,j}=0$ et $\sum_{k=1}^{r}b_{i,k}b_{j,k}=0$. Ceci implique $C=0$ et pour $i=j$, on a $\sum_{k=1}^{r}b_{j,k}^{2}=0$ donc $B=0$.

        \item On a donc $G_{0}\cap F_{0}=\lbrace0\rbrace$ ($\dim(G_{0})=n(n-r)$). $G_{0}$ et $F_{0}$ sont en somme directe, donc 
        \begin{equation}
            \dim(G_{0}\oplus F_{0})=\dim(G_{0})+\dim(F_{0})\leqslant n^{2}
        \end{equation}
        donc 
        \begin{equation}
            \boxed{\dim(F)=\dim(F_{0})\leqslant n^{2}-n(n-r)=nr}
        \end{equation}

        \item Si $F\cap GL_{n}(\R)=\emptyset$, on a $r\leqslant n-1$ et $\dim(F)\leqslant n(n-1)$. Par contraposée, si $\dim(F)\geqslant n^{2}-n+1$, on a $F\cap GL_{n}(\R)\neq\emptyset$.
        
        \item Soit 
        \begin{equation}
            G_{1}=\left\lbrace\begin{pmatrix}
                0 & B^{\mathsf{T}}\\
                B & C
            \end{pmatrix}\middle| B\in\M_{n-r,r}(\C),C\in\M_{r}(\C)\right\rbrace
        \end{equation}
        sous-$\R$-espace-vectoriel de $\M_{n}(\C)$. Par les mêmes arguments que précédemment, on a $G_{1}\cap F_{0}=\lbrace0\rbrace$ et $\dim_{\R}(G_{1})=2n(n-r)$ et $\dim_{\R}\M_{n}(\C)=2n^{2}$ donc 
        \begin{equation}
            \boxed{\dim_{\R}F_{0}=2\dim_{\C}F_{0}\leqslant 2nr}
        \end{equation}

        Le résultat est donc encore valable.
    \end{enumerate}
\end{proof}

\begin{proof}
    On a $f(I_{n})=f(I_{n})^{2}$ donc $f(I_{n}\in\lbrace0,1\rbrace$. Si $f(I_{n})=0$, alors $f=0$ ce qui est exclu.

    Si $M$ est inversible, on a 
    \begin{equation}
        f(M\times M^{-1})=f(M)\times f(M^{-1})=1
    \end{equation}
    donc $f(M)\neq0$.

    Si $M$ n'est pas inversible, posons $r=\rg(M)\leqslant n-1$. $M$ est équivalente la matrice nilpotente
    \begin{equation}
        M'=
        \begin{pmatrix}
            0 & 1 & 0 &\dots & \dots&\dots & 0\\
            \vdots & \ddots & \ddots & \ddots& & &\vdots\\
            \vdots & &\ddots & 1 & \ddots &&\vdots\\
            \vdots & && \ddots & 0 &\ddots &\vdots\\
            \vdots & & & &\ddots & \ddots & 0\\
            0 &\dots & \dots & \dots & \dots &0&0
        \end{pmatrix}
    \end{equation}

    Donc il existe $(P,Q)\in (GL_{n}(\C))^{2}$ telles que $M=P^{-1}M'Q$. On a
    \begin{equation}
        f(M'^{n})=\left(f(M')\right)^{n}=f(0)
    \end{equation}
    Comme $f(0)=f(0)^{2}$, on a aussi $f(0)\in\lbrace0,1\rbrace$. Si $f(0)=1$, pour tout $A\in\M_{n}(\C)$, on a $f(A\times 0)=f(A)\times f(0)=1$ ce qui est impossible car $f$ n'est pas constante. Donc $f(0)=0$. Ainsi, $f(M')=0$ et donc $f(M)=0$.
\end{proof}

\begin{remark}
    $f$ induit donc un morphisme de $\left(GL_{n}(\C),\times\right)\to(\C^{*},\times)$.
\end{remark}

\begin{remark}
    On peut montrer que pour $n\geqslant 2$, pour tout $i\neq j\in\lbrace1,n\rbrace^{2}$, $\forall \lambda \in \C$, il existe $(A,B)\in GL_{n}(\C)^{2}$,
    \begin{equation}
        T_{i,j}(\lambda)=ABA^{-1}B^{-1}
    \end{equation}
    en écrivant 
    \begin{align}
        T_{i,k}(\alpha)T_{k,j}(\beta)T_{i,k}(-\alpha)T_{k,j}(-\beta)
        =&\left(I_{n}+\alpha E_{i,k}+\beta E_{k,j}+\alpha \beta E_{i,j}\right)\notag\\
        ~~~~~~&\times \left(I_{n}-\alpha E_{i,k}-\beta E_{k,j}+\alpha\beta E_{i,j}\right)\\
        =&I_{n}+\alpha\beta E_{i,j}
    \end{align}

    Il vient 
    \begin{equation}
        f(T_{i,j}(\lambda))=f(A)f(B)f(A)^{-1}f(B)^{-1}=1
    \end{equation}
    Si $M\in GL_{n}(\C)$ s'écrit comme produit de transvections $T_{i,j}(\lambda)$ et de dilatations $D_{n}(\det(M))$.

    Il vient $f(M)=f(D_{n}(\det(M)))$. Or 
    \function{\varphi}{(\C^{*},\times)}{(\C^{*},\times)}{\alpha}{f(D_{n}(\alpha))}
    est un morphisme de groupe (car $D_{n}(\alpha\beta)=D_{n}(\alpha)D_{n}(\beta)$).

    Finalement, $f(M)=\varphi(\det(M))$.

    Si de plus $f$ est continue, $\varphi$ aussi et on peut montrer qu'il existe $k\in\R$ tel que pour tout $z\in\C^{*}$, $\varphi(z)=z^{k}$.
\end{remark}

\end{document}